{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tWP4R-BZmaZi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\pyimagesearch')\n",
    "from timeseires.CNN1D.GoogLeNet1D import PC\n",
    "from timeseires.utils.to_split import to_split\n",
    "#from timeseires.callbacks.EpochCheckpoint import EpochCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeseires.callbacks.TrainingMonitor import TrainingMonitor\n",
    "from timeseires.utils.load_only_multi_step import load_only_multi_step\n",
    "from timeseires.utils.multivariate_multi_step import multivariate_multi_step\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD  \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,MaxPooling1D,Concatenate,AveragePooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "import pandas as pd\n",
    "import time, pickle\n",
    "import pydot\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, Reshape, Dense, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten, LeakyReLU, concatenate\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12130, 21), (24259, 21))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tr = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_train.csv'\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values \n",
    "\n",
    "path_v = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_validation.csv'\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "\n",
    "path_te = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_test.csv'\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "#.......................................................................................................\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_scaler.pkl\", 'rb'))\n",
    "df_te.shape,df_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nmYgpwTPnsf4"
   },
   "outputs": [],
   "source": [
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIYjXnV81ZHC",
    "outputId": "4520a0c1-81f1-4cf6-9b17-d15f5ae9c7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.35156679153442383 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X, train_y = multivariate_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24,ahead=0)\n",
    "validation_X, validation_y = multivariate_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24,ahead=0)\n",
    "test_X, test_y = multivariate_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24,ahead=0)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.24755477905273438 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X_load, train_y_load = load_only_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24)\n",
    "validation_X_load, validation_y_load = load_only_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24)\n",
    "test_X_load, test_y_load = load_only_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PbWIXWwSJN3k"
   },
   "outputs": [],
   "source": [
    "#EXP 1 STEPS 24 LOOKBACK 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import the necessary packages\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None, kernel_initializer='lecun_normal'):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"selu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = PC.conv_module(inputs, 128, 5, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(1)(x)\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_goog():   \n",
    "    inputs_combine = Input(shape=(time_steps, num_features))\n",
    "    lstm1   = LSTM(20, return_sequences = True)(inputs_combine)\n",
    "    lstm2   = LSTM(20)(lstm1)\n",
    "    #--------------------------------------------------------\n",
    "    inputs_load = Input(shape=(time_steps, 1))\n",
    "    l = Bidirectional(LSTM(32, return_sequences=True))(inputs_load)\n",
    "    l = Bidirectional(LSTM(32, return_sequences=True))(l)\n",
    "    \n",
    "    c11 = PC.inception_module(inputs_load, 32, 32, 64, 8, 32, \"1a\", 0.01)\n",
    "    c12 = Concatenate(axis=-1)([inputs_load, c11])\n",
    "    \n",
    "    c21 = PC.inception_module(c12, 64, 64, 128, 16, 64, \"2a\", 0.01)\n",
    "    c22 = Concatenate(axis=-1)([ c11, c21])\n",
    "    \n",
    "    c31 = PC.inception_module(c22, 64, 64, 128, 16, 64, \"3a\", 0.01)\n",
    "    #---------------------------------------------------------\n",
    "    y = Concatenate(axis=-1)([inputs_load, l, c21, c31])\n",
    "\n",
    "    #y= PC.conv_module(y, 128, 3, 1)\n",
    "    x = Flatten()(y)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([x, lstm2])\n",
    "    \n",
    "    x = Dense(24, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inputs_load,inputs_combine], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = review_goog()\n",
    "tensorflow.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 24, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_conv (Conv1D)        (None, 24, 32)       64          input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_conv (Conv1D)         (None, 24, 8)        16          input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_bn (BatchNormalizati (None, 24, 32)       128         1a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_bn (BatchNormalizatio (None, 24, 8)        32          1a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_act (Activation)     (None, 24, 32)       0           1a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_act (Activation)      (None, 24, 8)        0           1a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_conv (Conv1D)          (None, 24, 32)       64          input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_conv (Conv1D)        (None, 24, 64)       6208        1a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_conv (Conv1D)         (None, 24, 32)       1312        1a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_bn (BatchNormalization (None, 24, 32)       128         1a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_bn (BatchNormalizati (None, 24, 64)       256         1a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_bn (BatchNormalizatio (None, 24, 32)       128         1a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_act (Activation)       (None, 24, 32)       0           1a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_act (Activation)     (None, 24, 64)       0           1a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_act (Activation)      (None, 24, 32)       0           1a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_mixed (Concatenate)          (None, 24, 128)      0           1a_first_act[0][0]               \n",
      "                                                                 1a_second2_act[0][0]             \n",
      "                                                                 1a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 24, 129)      0           input_12[0][0]                   \n",
      "                                                                 1a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_conv (Conv1D)        (None, 24, 64)       8320        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_conv (Conv1D)         (None, 24, 16)       2080        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_bn (BatchNormalizati (None, 24, 64)       256         2a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_bn (BatchNormalizatio (None, 24, 16)       64          2a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_act (Activation)     (None, 24, 64)       0           2a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_act (Activation)      (None, 24, 16)       0           2a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_conv (Conv1D)          (None, 24, 64)       8320        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_conv (Conv1D)        (None, 24, 128)      24704       2a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_conv (Conv1D)         (None, 24, 64)       5184        2a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_bn (BatchNormalization (None, 24, 64)       256         2a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_bn (BatchNormalizati (None, 24, 128)      512         2a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_bn (BatchNormalizatio (None, 24, 64)       256         2a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_act (Activation)       (None, 24, 64)       0           2a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_act (Activation)     (None, 24, 128)      0           2a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_act (Activation)      (None, 24, 64)       0           2a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_mixed (Concatenate)          (None, 24, 256)      0           2a_first_act[0][0]               \n",
      "                                                                 2a_second2_act[0][0]             \n",
      "                                                                 2a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 24, 384)      0           1a_mixed[0][0]                   \n",
      "                                                                 2a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_conv (Conv1D)        (None, 24, 64)       24640       concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_conv (Conv1D)         (None, 24, 16)       6160        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_bn (BatchNormalizati (None, 24, 64)       256         3a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_bn (BatchNormalizatio (None, 24, 16)       64          3a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_act (Activation)     (None, 24, 64)       0           3a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_act (Activation)      (None, 24, 16)       0           3a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_conv (Conv1D)          (None, 24, 64)       24640       concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_conv (Conv1D)        (None, 24, 128)      24704       3a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_conv (Conv1D)         (None, 24, 64)       5184        3a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_bn (BatchNormalization (None, 24, 64)       256         3a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_bn (BatchNormalizati (None, 24, 128)      512         3a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_bn (BatchNormalizatio (None, 24, 64)       256         3a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 24, 64)       8704        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_act (Activation)       (None, 24, 64)       0           3a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_act (Activation)     (None, 24, 128)      0           3a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_act (Activation)      (None, 24, 64)       0           3a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 24, 64)       24832       bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "3a_mixed (Concatenate)          (None, 24, 256)      0           3a_first_act[0][0]               \n",
      "                                                                 3a_second2_act[0][0]             \n",
      "                                                                 3a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 24, 21)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 24, 577)      0           input_12[0][0]                   \n",
      "                                                                 bidirectional_11[0][0]           \n",
      "                                                                 2a_mixed[0][0]                   \n",
      "                                                                 3a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  (None, 24, 20)       3360        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 13848)        0           concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 20)           3280        lstm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 13868)        0           flatten_5[0][0]                  \n",
      "                                                                 lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 24)           332856      concatenate_23[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 517,992\n",
      "Trainable params: 516,312\n",
      "Non-trainable params: 1,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9rQrz9B0JWvR"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\5\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\5'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "h6NbczAlOpSQ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NkF7jg_XO346"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-rIEZj0PCPF",
    "outputId": "95edda02-84de-4f8a-83a2-b4d53387049d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = review_goog()\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-o0qgceiPFdI",
    "outputId": "ebb2c588-e73f-4182-baad-2f4c28f3db95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.1769 - mae: 0.0551 - mape: 2110.1882\n",
      "Epoch 00001: val_loss improved from inf to 0.04133, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0001-loss0.04.h5\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.1769 - mae: 0.0551 - mape: 2110.1882 - val_loss: 0.0413 - val_mae: 0.0406 - val_mape: 18.0955\n",
      "Epoch 2/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0349 - mae: 0.0343 - mape: 2275.4241\n",
      "Epoch 00002: val_loss improved from 0.04133 to 0.03918, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0002-loss0.04.h5\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0349 - mae: 0.0343 - mape: 2273.8494 - val_loss: 0.0392 - val_mae: 0.0387 - val_mape: 17.6912\n",
      "Epoch 3/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0338 - mae: 0.0334 - mape: 2336.2554\n",
      "Epoch 00003: val_loss did not improve from 0.03918\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0338 - mae: 0.0334 - mape: 2336.2554 - val_loss: 0.0420 - val_mae: 0.0416 - val_mape: 16.4381\n",
      "Epoch 4/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0331 - mae: 0.0328 - mape: 2312.5745\n",
      "Epoch 00004: val_loss improved from 0.03918 to 0.03508, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0004-loss0.04.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0331 - mae: 0.0328 - mape: 2310.9731 - val_loss: 0.0351 - val_mae: 0.0348 - val_mape: 16.0472\n",
      "Epoch 5/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0327 - mae: 0.0324 - mape: 2306.5696\n",
      "Epoch 00005: val_loss did not improve from 0.03508\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0327 - mae: 0.0324 - mape: 2305.8386 - val_loss: 0.0365 - val_mae: 0.0362 - val_mape: 16.5958\n",
      "Epoch 6/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0322 - mae: 0.0319 - mape: 2344.4248\n",
      "Epoch 00006: val_loss did not improve from 0.03508\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0322 - mae: 0.0319 - mape: 2342.8008 - val_loss: 0.0368 - val_mae: 0.0365 - val_mape: 14.7997\n",
      "Epoch 7/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0318 - mae: 0.0316 - mape: 2410.6318\n",
      "Epoch 00007: val_loss did not improve from 0.03508\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0318 - mae: 0.0316 - mape: 2410.6318 - val_loss: 0.0579 - val_mae: 0.0577 - val_mape: 20.1601\n",
      "Epoch 8/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0313 - mae: 0.0311 - mape: 2394.7139\n",
      "Epoch 00008: val_loss improved from 0.03508 to 0.03428, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0008-loss0.03.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0313 - mae: 0.0311 - mape: 2394.7139 - val_loss: 0.0343 - val_mae: 0.0341 - val_mape: 15.7074\n",
      "Epoch 9/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0309 - mae: 0.0308 - mape: 2359.3862\n",
      "Epoch 00009: val_loss did not improve from 0.03428\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0309 - mae: 0.0308 - mape: 2357.7524 - val_loss: 0.0345 - val_mae: 0.0345 - val_mape: 14.1941\n",
      "Epoch 10/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0304 - mae: 0.0304 - mape: 2422.7935\n",
      "Epoch 00010: val_loss improved from 0.03428 to 0.03237, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0010-loss0.03.h5\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0304 - mae: 0.0304 - mape: 2421.1152 - val_loss: 0.0324 - val_mae: 0.0323 - val_mape: 14.1933\n",
      "Epoch 11/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0301 - mae: 0.0301 - mape: 2364.4558\n",
      "Epoch 00011: val_loss did not improve from 0.03237\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0301 - mae: 0.0301 - mape: 2362.8164 - val_loss: 0.0329 - val_mae: 0.0329 - val_mape: 14.7422\n",
      "Epoch 12/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0299 - mae: 0.0299 - mape: 2374.9211\n",
      "Epoch 00012: val_loss did not improve from 0.03237\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0299 - mae: 0.0299 - mape: 2373.2771 - val_loss: 0.0332 - val_mae: 0.0332 - val_mape: 14.9493\n",
      "Epoch 13/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0296 - mae: 0.0296 - mape: 2364.8398\n",
      "Epoch 00013: val_loss did not improve from 0.03237\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0296 - mae: 0.0296 - mape: 2364.0896 - val_loss: 0.0324 - val_mae: 0.0324 - val_mape: 14.1466\n",
      "Epoch 14/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0293 - mae: 0.0293 - mape: 2287.1084- ETA: 1s - loss: 0.0293 - mae: 0.\n",
      "Epoch 00014: val_loss did not improve from 0.03237\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0293 - mae: 0.0293 - mape: 2286.3833 - val_loss: 0.0324 - val_mae: 0.0324 - val_mape: 13.9777\n",
      "Epoch 15/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0291 - mae: 0.0291 - mape: 2250.0344\n",
      "Epoch 00015: val_loss did not improve from 0.03237\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0291 - mae: 0.0291 - mape: 2249.3218 - val_loss: 0.0327 - val_mae: 0.0327 - val_mape: 14.4193\n",
      "Epoch 16/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0289 - mae: 0.0289 - mape: 2242.7671\n",
      "Epoch 00016: val_loss improved from 0.03237 to 0.03124, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0016-loss0.03.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0289 - mae: 0.0289 - mape: 2241.2134 - val_loss: 0.0312 - val_mae: 0.0312 - val_mape: 13.1535\n",
      "Epoch 17/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0287 - mae: 0.0287 - mape: 2187.2039\n",
      "Epoch 00017: val_loss did not improve from 0.03124\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0287 - mae: 0.0287 - mape: 2185.6892 - val_loss: 0.0324 - val_mae: 0.0324 - val_mape: 14.2175\n",
      "Epoch 18/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0286 - mae: 0.0286 - mape: 2155.2102\n",
      "Epoch 00018: val_loss did not improve from 0.03124\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0286 - mae: 0.0286 - mape: 2155.2102 - val_loss: 0.0316 - val_mae: 0.0316 - val_mape: 13.6146\n",
      "Epoch 19/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0284 - mae: 0.0284 - mape: 2133.7146\n",
      "Epoch 00019: val_loss did not improve from 0.03124\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0284 - mae: 0.0284 - mape: 2133.7146 - val_loss: 0.0313 - val_mae: 0.0313 - val_mape: 13.9712\n",
      "Epoch 20/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0282 - mae: 0.0282 - mape: 2091.3650\n",
      "Epoch 00020: val_loss did not improve from 0.03124\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0282 - mae: 0.0282 - mape: 2089.9165 - val_loss: 0.0322 - val_mae: 0.0322 - val_mape: 14.3681\n",
      "Epoch 21/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0280 - mae: 0.0280 - mape: 2032.6659\n",
      "Epoch 00021: val_loss improved from 0.03124 to 0.03080, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0021-loss0.03.h5\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0280 - mae: 0.0280 - mape: 2032.6659 - val_loss: 0.0308 - val_mae: 0.0308 - val_mape: 13.4364\n",
      "Epoch 22/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0279 - mape: 2080.6848\n",
      "Epoch 00022: val_loss did not improve from 0.03080\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0279 - mae: 0.0279 - mape: 2080.0256 - val_loss: 0.0321 - val_mae: 0.0321 - val_mape: 14.5812\n",
      "Epoch 23/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0278 - mae: 0.0278 - mape: 2031.8508- ETA: 2s - l\n",
      "Epoch 00023: val_loss improved from 0.03080 to 0.03073, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0023-loss0.03.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0278 - mae: 0.0278 - mape: 2031.2064 - val_loss: 0.0307 - val_mae: 0.0307 - val_mape: 12.9312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0277 - mae: 0.0277 - mape: 2031.6588\n",
      "Epoch 00024: val_loss did not improve from 0.03073\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0277 - mae: 0.0277 - mape: 2030.2531 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.4279\n",
      "Epoch 25/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0275 - mae: 0.0275 - mape: 2007.6039\n",
      "Epoch 00025: val_loss did not improve from 0.03073\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0275 - mae: 0.0275 - mape: 2006.9677 - val_loss: 0.0313 - val_mae: 0.0313 - val_mape: 13.6957\n",
      "Epoch 26/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0274 - mae: 0.0274 - mape: 2037.0947\n",
      "Epoch 00026: val_loss did not improve from 0.03073\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0274 - mae: 0.0274 - mape: 2036.4492 - val_loss: 0.0313 - val_mae: 0.0313 - val_mape: 13.7942\n",
      "Epoch 27/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0273 - mape: 2014.6956- ETA: 1s - loss: 0.0273 \n",
      "Epoch 00027: val_loss did not improve from 0.03073\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0273 - mae: 0.0273 - mape: 2014.0580 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 13.8274\n",
      "Epoch 28/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0272 - mape: 2055.9026\n",
      "Epoch 00028: val_loss did not improve from 0.03073\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0272 - mae: 0.0272 - mape: 2055.2507 - val_loss: 0.0312 - val_mae: 0.0312 - val_mape: 13.8150\n",
      "Epoch 29/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0271 - mae: 0.0271 - mape: 2009.5399\n",
      "Epoch 00029: val_loss improved from 0.03073 to 0.03036, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0029-loss0.03.h5\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0271 - mae: 0.0271 - mape: 2009.5399 - val_loss: 0.0304 - val_mae: 0.0304 - val_mape: 13.0730\n",
      "Epoch 30/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0270 - mae: 0.0270 - mape: 1956.8110\n",
      "Epoch 00030: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0270 - mae: 0.0270 - mape: 1956.8110 - val_loss: 0.0307 - val_mae: 0.0307 - val_mape: 13.3230\n",
      "Epoch 31/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0269 - mape: 1957.8000\n",
      "Epoch 00031: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0269 - mae: 0.0269 - mape: 1957.1803 - val_loss: 0.0307 - val_mae: 0.0307 - val_mape: 13.7411\n",
      "Epoch 32/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0268 - mae: 0.0268 - mape: 1967.2214\n",
      "Epoch 00032: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0268 - mae: 0.0268 - mape: 1966.5975 - val_loss: 0.0322 - val_mae: 0.0322 - val_mape: 14.5242\n",
      "Epoch 33/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0268 - mae: 0.0268 - mape: 1957.7191\n",
      "Epoch 00033: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0268 - mae: 0.0268 - mape: 1956.3638 - val_loss: 0.0304 - val_mae: 0.0304 - val_mape: 12.9894\n",
      "Epoch 34/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0267 - mae: 0.0267 - mape: 1968.3022\n",
      "Epoch 00034: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0267 - mae: 0.0267 - mape: 1966.9396 - val_loss: 0.0306 - val_mae: 0.0306 - val_mape: 13.1819\n",
      "Epoch 35/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0266 - mape: 1967.8219\n",
      "Epoch 00035: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0266 - mae: 0.0266 - mape: 1967.1985 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 13.8195\n",
      "Epoch 36/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.0265 - mape: 1937.3785\n",
      "Epoch 00036: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0265 - mae: 0.0265 - mape: 1936.0391 - val_loss: 0.0306 - val_mae: 0.0306 - val_mape: 13.6714\n",
      "Epoch 37/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0265 - mae: 0.0265 - mape: 1957.8059\n",
      "Epoch 00037: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0265 - mae: 0.0265 - mape: 1957.8059 - val_loss: 0.0308 - val_mae: 0.0308 - val_mape: 13.3783\n",
      "Epoch 38/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0264 - mae: 0.0264 - mape: 1910.5824\n",
      "Epoch 00038: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0264 - mae: 0.0264 - mape: 1910.5824 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.8380\n",
      "Epoch 39/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0263 - mae: 0.0263 - mape: 1964.8206\n",
      "Epoch 00039: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0263 - mae: 0.0263 - mape: 1964.8206 - val_loss: 0.0305 - val_mae: 0.0305 - val_mape: 13.1435\n",
      "Epoch 40/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0263 - mape: 1945.0571- ETA: 3\n",
      "Epoch 00040: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0263 - mae: 0.0263 - mape: 1943.7097 - val_loss: 0.0318 - val_mae: 0.0318 - val_mape: 13.6300\n",
      "Epoch 41/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0262 - mape: 1978.7371\n",
      "Epoch 00041: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0262 - mae: 0.0262 - mape: 1978.1104 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.6557\n",
      "Epoch 42/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0261 - mape: 1918.3773\n",
      "Epoch 00042: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0261 - mae: 0.0261 - mape: 1917.0487 - val_loss: 0.0304 - val_mae: 0.0304 - val_mape: 13.1033\n",
      "Epoch 43/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0261 - mape: 1897.5328\n",
      "Epoch 00043: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0261 - mae: 0.0261 - mape: 1896.2188 - val_loss: 0.0305 - val_mae: 0.0305 - val_mape: 13.0522\n",
      "Epoch 44/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0260 - mae: 0.0260 - mape: 1967.7892\n",
      "Epoch 00044: val_loss did not improve from 0.03036\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0260 - mae: 0.0260 - mape: 1966.4277 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.7099\n",
      "Epoch 45/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0259 - mae: 0.0259 - mape: 1927.9528\n",
      "Epoch 00045: val_loss improved from 0.03036 to 0.03007, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0045-loss0.03.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0259 - mae: 0.0259 - mape: 1927.9528 - val_loss: 0.0301 - val_mae: 0.0301 - val_mape: 12.8168\n",
      "Epoch 46/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0259 - mape: 1912.0977\n",
      "Epoch 00046: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0259 - mae: 0.0259 - mape: 1911.4919 - val_loss: 0.0308 - val_mae: 0.0308 - val_mape: 13.5648\n",
      "Epoch 47/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0258 - mape: 1943.1573\n",
      "Epoch 00047: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0258 - mae: 0.0258 - mape: 1941.8116 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.0456\n",
      "Epoch 48/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0257 - mae: 0.0257 - mape: 1842.7394\n",
      "Epoch 00048: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0257 - mae: 0.0257 - mape: 1842.7394 - val_loss: 0.0313 - val_mae: 0.0313 - val_mape: 13.7142\n",
      "Epoch 49/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0257 - mape: 1855.4802\n",
      "Epoch 00049: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0257 - mae: 0.0257 - mape: 1854.8925 - val_loss: 0.0320 - val_mae: 0.0320 - val_mape: 13.7150\n",
      "Epoch 50/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0257 - mape: 1934.9281\n",
      "Epoch 00050: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0257 - mae: 0.0257 - mape: 1933.5883 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 13.0775\n",
      "Epoch 51/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0256 - mae: 0.0256 - mape: 1907.2518\n",
      "Epoch 00051: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0256 - mae: 0.0256 - mape: 1907.2518 - val_loss: 0.0313 - val_mae: 0.0313 - val_mape: 13.7454\n",
      "Epoch 52/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0256 - mape: 1892.2009\n",
      "Epoch 00052: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0256 - mae: 0.0256 - mape: 1890.8909 - val_loss: 0.0304 - val_mae: 0.0304 - val_mape: 12.7116\n",
      "Epoch 53/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0255 - mape: 1849.8993\n",
      "Epoch 00053: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0255 - mae: 0.0255 - mape: 1849.3134 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.1961\n",
      "Epoch 54/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0254 - mape: 1843.4021\n",
      "Epoch 00054: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0254 - mae: 0.0254 - mape: 1842.1255 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 13.3682\n",
      "Epoch 55/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0254 - mape: 1861.4059- ETA: 2s - \n",
      "Epoch 00055: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0254 - mae: 0.0254 - mape: 1860.1166 - val_loss: 0.0305 - val_mae: 0.0305 - val_mape: 13.2104\n",
      "Epoch 56/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0254 - mape: 1813.6742\n",
      "Epoch 00056: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0254 - mae: 0.0254 - mape: 1812.4181 - val_loss: 0.0303 - val_mae: 0.0303 - val_mape: 13.0067\n",
      "Epoch 57/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0253 - mape: 1823.0083\n",
      "Epoch 00057: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0253 - mae: 0.0253 - mape: 1822.4307 - val_loss: 0.0307 - val_mae: 0.0307 - val_mape: 12.8399\n",
      "Epoch 58/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0253 - mae: 0.0253 - mape: 1798.3763- ETA: - ETA: 1s - loss: 0.0253 - mae: 0.\n",
      "Epoch 00058: val_loss did not improve from 0.03007\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0253 - mae: 0.0253 - mape: 1798.3763 - val_loss: 0.0312 - val_mae: 0.0312 - val_mape: 13.2141\n",
      "Epoch 59/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0252 - mape: 1839.8241\n",
      "Epoch 00059: val_loss improved from 0.03007 to 0.03000, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0059-loss0.03.h5\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0252 - mae: 0.0252 - mape: 1838.5514 - val_loss: 0.0300 - val_mae: 0.0300 - val_mape: 12.7243\n",
      "Epoch 60/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0251 - mape: 1834.1978\n",
      "Epoch 00060: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0251 - mae: 0.0251 - mape: 1832.9281 - val_loss: 0.0306 - val_mae: 0.0306 - val_mape: 13.1767\n",
      "Epoch 61/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0251 - mape: 1817.0457\n",
      "Epoch 00061: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0251 - mae: 0.0251 - mape: 1816.4697 - val_loss: 0.0303 - val_mae: 0.0303 - val_mape: 12.7756\n",
      "Epoch 62/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0251 - mape: 1809.0404\n",
      "Epoch 00062: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0251 - mae: 0.0251 - mape: 1808.4670 - val_loss: 0.0313 - val_mae: 0.0313 - val_mape: 13.5122\n",
      "Epoch 63/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0250 - mape: 1811.0699\n",
      "Epoch 00063: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0250 - mae: 0.0250 - mape: 1809.8157 - val_loss: 0.0304 - val_mae: 0.0304 - val_mape: 13.1650\n",
      "Epoch 64/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0250 - mae: 0.0250 - mape: 1831.2506\n",
      "Epoch 00064: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0250 - mae: 0.0250 - mape: 1831.2506 - val_loss: 0.0305 - val_mae: 0.0305 - val_mape: 13.0502\n",
      "Epoch 65/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0250 - mape: 1800.2910\n",
      "Epoch 00065: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0250 - mae: 0.0250 - mape: 1799.7212 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.3422\n",
      "Epoch 66/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0249 - mae: 0.0249 - mape: 1801.6941\n",
      "Epoch 00066: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0249 - mae: 0.0249 - mape: 1800.4468 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.3043\n",
      "Epoch 67/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0248 - mape: 1803.9584\n",
      "Epoch 00067: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0248 - mae: 0.0248 - mape: 1803.3868 - val_loss: 0.0307 - val_mae: 0.0307 - val_mape: 13.2100\n",
      "Epoch 68/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0248 - mape: 1784.6796\n",
      "Epoch 00068: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0248 - mae: 0.0248 - mape: 1783.4438 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 13.4358\n",
      "Epoch 69/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0248 - mape: 1829.4823\n",
      "Epoch 00069: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0248 - mae: 0.0248 - mape: 1828.9025 - val_loss: 0.0316 - val_mae: 0.0316 - val_mape: 13.4027\n",
      "Epoch 70/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0247 - mae: 0.0247 - mape: 1811.5359\n",
      "Epoch 00070: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0247 - mae: 0.0247 - mape: 1810.2814 - val_loss: 0.0303 - val_mae: 0.0303 - val_mape: 12.6653\n",
      "Epoch 71/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0247 - mae: 0.0247 - mape: 1794.0448\n",
      "Epoch 00071: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0247 - mae: 0.0247 - mape: 1794.0448 - val_loss: 0.0304 - val_mae: 0.0304 - val_mape: 12.8775\n",
      "Epoch 72/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0246 - mape: 1762.2949\n",
      "Epoch 00072: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0246 - mae: 0.0246 - mape: 1761.0743 - val_loss: 0.0312 - val_mae: 0.0312 - val_mape: 13.4118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0246 - mape: 1773.9572\n",
      "Epoch 00073: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0246 - mae: 0.0246 - mape: 1773.3950 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.1567\n",
      "Epoch 74/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0245 - mape: 1770.4775\n",
      "Epoch 00074: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0245 - mae: 0.0245 - mape: 1769.2521 - val_loss: 0.0310 - val_mae: 0.0310 - val_mape: 13.3552\n",
      "Epoch 75/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0245 - mae: 0.0245 - mape: 1752.6229\n",
      "Epoch 00075: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0245 - mae: 0.0245 - mape: 1752.6229 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.1115\n",
      "Epoch 76/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0245 - mape: 1753.8877\n",
      "Epoch 00076: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0245 - mae: 0.0245 - mape: 1752.6738 - val_loss: 0.0313 - val_mae: 0.0313 - val_mape: 13.3094\n",
      "Epoch 77/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0244 - mae: 0.0244 - mape: 1765.1567\n",
      "Epoch 00077: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0244 - mae: 0.0244 - mape: 1765.1567 - val_loss: 0.0313 - val_mae: 0.0313 - val_mape: 13.2208\n",
      "Epoch 78/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mae: 0.0244 - mape: 1731.0267\n",
      "Epoch 00078: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0244 - mae: 0.0244 - mape: 1730.4779 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 13.4205\n",
      "Epoch 79/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0243 - mae: 0.0243 - mape: 1752.3063\n",
      "Epoch 00079: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0243 - mae: 0.0243 - mape: 1752.3063 - val_loss: 0.0319 - val_mae: 0.0319 - val_mape: 13.6749\n",
      "Epoch 80/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0243 - mape: 1725.5522\n",
      "Epoch 00080: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0243 - mae: 0.0243 - mape: 1724.3573 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 12.7663\n",
      "Epoch 81/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0243 - mape: 1734.9948- ETA: 3s - loss: 0.02 - ETA: 1s - loss: 0.0243 - mae: 0.024 - ETA: 0s - loss: 0.0243 - mae: 0.0243 - mape: 174\n",
      "Epoch 00081: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0243 - mae: 0.0243 - mape: 1733.7939 - val_loss: 0.0314 - val_mae: 0.0314 - val_mape: 13.6543\n",
      "Epoch 82/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0242 - mae: 0.0242 - mape: 1724.3956\n",
      "Epoch 00082: val_loss did not improve from 0.03000\n",
      "2652/2652 [==============================] - 63s 24ms/step - loss: 0.0242 - mae: 0.0242 - mape: 1723.2010 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 13.2596\n",
      "Epoch 83/100\n",
      " 225/2652 [=>............................] - ETA: 50s - loss: 0.0243 - mae: 0.0243 - mape: 1433.7983"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10576\\2109174755.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalidation_X_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_X\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oS6zzoUoPL-A",
    "outputId": "ce1a0f3b-6b76-4980-d717-d3542c18a37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 485.44\n",
      "Median Absolute Error (MedAE): 330.47\n",
      "Mean Squared Error (MSE): 474793.66\n",
      "Root Mean Squared Error (RMSE): 689.05\n",
      "Mean Absolute Percentage Error (MAPE): 3.22 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.3 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\5\\\\E1-cp-0045-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7y39sVKwLMm"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AVN1WhyzuWFl"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\5\\\\E2-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\5\\\\E1-cp-0045-loss0.03.h5'\n",
    "start_epoch= 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WBeDTA9lu5Qm"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2j6fmkOvA2O",
    "outputId": "7fc1b23b-4d93-4b54-a4df-921c557b9a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0045-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rTCJW6IzvLU4",
    "outputId": "f810e1ca-98cc-4de7-a4a3-465e83705c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0248 - mean_absolute_error: 0.0248 - mean_absolute_percentage_error: 1898.2002\n",
      "Epoch 00001: val_loss improved from inf to 0.03001, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E2-cp-0001-loss0.03.h5\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0248 - mean_absolute_error: 0.0248 - mean_absolute_percentage_error: 1896.8859 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300 - val_mean_absolute_percentage_error: 12.9522\n",
      "Epoch 2/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0247 - mean_absolute_error: 0.0247 - mean_absolute_percentage_error: 1878.4425\n",
      "Epoch 00002: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0247 - mean_absolute_error: 0.0247 - mean_absolute_percentage_error: 1877.8473 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300 - val_mean_absolute_percentage_error: 12.9457\n",
      "Epoch 3/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1863.4044\n",
      "Epoch 00003: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1862.8136 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1510\n",
      "Epoch 4/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1870.9830\n",
      "Epoch 00004: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1870.3900 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1811\n",
      "Epoch 5/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1866.6724\n",
      "Epoch 00005: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1866.6724 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304 - val_mean_absolute_percentage_error: 13.2401\n",
      "Epoch 6/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1867.6958\n",
      "Epoch 00006: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1867.1040 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0163\n",
      "Epoch 7/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1849.6239\n",
      "Epoch 00007: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1849.0377 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9824\n",
      "Epoch 8/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1852.4467\n",
      "Epoch 00008: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1852.4467 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304 - val_mean_absolute_percentage_error: 13.1703\n",
      "Epoch 9/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1853.4146\n",
      "Epoch 00009: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1852.1316 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0106\n",
      "Epoch 10/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1847.4153- ETA: 1s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percent\n",
      "Epoch 00010: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1846.1372 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0279\n",
      "Epoch 11/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1839.2570- ETA: 1s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute\n",
      "Epoch 00011: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1839.2570 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1317\n",
      "Epoch 12/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1832.5651\n",
      "Epoch 00012: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1831.2968 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9474\n",
      "Epoch 13/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1835.7986\n",
      "Epoch 00013: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1835.2163 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300 - val_mean_absolute_percentage_error: 12.8533\n",
      "Epoch 14/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1827.0063\n",
      "Epoch 00014: val_loss improved from 0.03001 to 0.02984, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E2-cp-0014-loss0.03.h5\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1827.0063 - val_loss: 0.0298 - val_mean_absolute_error: 0.0298 - val_mean_absolute_percentage_error: 12.7396\n",
      "Epoch 15/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1840.8979\n",
      "Epoch 00015: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1839.6233 - val_loss: 0.0301 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 12.9270\n",
      "Epoch 16/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1830.7869\n",
      "Epoch 00016: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1830.2061 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.0077\n",
      "Epoch 17/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1823.9042\n",
      "Epoch 00017: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1823.3262 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0050\n",
      "Epoch 18/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1821.5284- ETA: 3s - loss: 0.0244 - mean_absolut\n",
      "Epoch 00018: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 62s 24ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1820.9518 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.0704\n",
      "Epoch 19/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1823.4219- ETA: \n",
      "Epoch 00019: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 63s 24ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1822.1588 - val_loss: 0.0301 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 12.8642\n",
      "Epoch 20/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1816.8361\n",
      "Epoch 00020: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1816.8361 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9048\n",
      "Epoch 21/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1814.0159\n",
      "Epoch 00021: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1814.0159 - val_loss: 0.0301 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 12.7949\n",
      "Epoch 22/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1820.1229\n",
      "Epoch 00022: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1820.1229 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.0084\n",
      "Epoch 23/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1807.3566\n",
      "Epoch 00023: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1806.1049 - val_loss: 0.0305 - val_mean_absolute_error: 0.0305 - val_mean_absolute_percentage_error: 13.1951\n",
      "Epoch 24/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1804.7067\n",
      "Epoch 00024: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1804.1354 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300 - val_mean_absolute_percentage_error: 12.7502\n",
      "Epoch 25/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1797.2174- ETA: 9s - loss: 0.0243 - mean_absolute_error\n",
      "Epoch 00025: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1795.9727 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9298\n",
      "Epoch 26/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1806.2993\n",
      "Epoch 00026: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1805.7277 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304 - val_mean_absolute_percentage_error: 13.1903\n",
      "Epoch 27/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1800.3931- ETA: 4s - loss: 0.0243 - m\n",
      "Epoch 00027: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1800.3931 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300 - val_mean_absolute_percentage_error: 12.7915\n",
      "Epoch 28/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1803.5500\n",
      "Epoch 00028: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1802.3015 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304 - val_mean_absolute_percentage_error: 13.1215\n",
      "Epoch 29/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1805.6102\n",
      "Epoch 00029: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1804.3599 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9977\n",
      "Epoch 30/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1792.7385\n",
      "Epoch 00030: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1791.4976 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9227\n",
      "Epoch 31/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1790.1373\n",
      "Epoch 00031: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1789.5696 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9248\n",
      "Epoch 32/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1791.7062- ETA: 0s - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1797.7\n",
      "Epoch 00032: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1791.1382 - val_loss: 0.0301 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 12.8711\n",
      "Epoch 33/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1785.5836\n",
      "Epoch 00033: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1784.3464 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.0029\n",
      "Epoch 34/60\n",
      "1179/2652 [============>.................] - ETA: 30s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1438.4391"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10576\\4001373639.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalidation_X_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_X\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y_load\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y_load),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qj8HRsBXvf0M",
    "outputId": "084e6fbf-b9cc-46e8-e4d7-808e9031da87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 485.07\n",
      "Median Absolute Error (MedAE): 325.85\n",
      "Mean Squared Error (MSE): 475778.52\n",
      "Root Mean Squared Error (RMSE): 689.77\n",
      "Mean Absolute Percentage Error (MAPE): 3.22 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.28 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\5\\\\E2-cp-0014-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAuKvjoOG-fA"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QZI2LOvEw6Qq"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E2-cp-0006-loss0.04.h5'\n",
    "start_epoch= 59+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qPy-9EGRw6eR"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beh0i6Vww6pP",
    "outputId": "41024e1d-abfa-4ec6-a3ed-cf5e5fdef9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E2-cp-0006-loss0.04.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y-9y1xTeYAaB",
    "outputId": "3e59db7d-d1fa-41c7-a91e-cf2063e855da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0231 - mape: 8601.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03575, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0001-loss0.04.h5\n",
      "32614/32614 [==============================] - 7s 210us/sample - loss: 0.0235 - mae: 0.0231 - mape: 8729.3066 - val_loss: 0.0358 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0230 - mape: 8563.2227\n",
      "Epoch 00002: val_loss improved from 0.03575 to 0.03574, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0002-loss0.04.h5\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0235 - mae: 0.0230 - mape: 8710.7080 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1938\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8942.2402\n",
      "Epoch 00003: val_loss improved from 0.03574 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0003-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 115us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8703.8408 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8589.9561\n",
      "Epoch 00004: val_loss improved from 0.03573 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 114us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8711.1074 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1949\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8953.2998\n",
      "Epoch 00005: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8714.5996 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8595.0498\n",
      "Epoch 00006: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8716.8604 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8175.1738\n",
      "Epoch 00007: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8717.2090 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8577.0596\n",
      "Epoch 00008: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8716.0957 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1954\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2109\n",
      "Epoch 00009: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.7314 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2744\n",
      "Epoch 00010: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.3086 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.7139\n",
      "Epoch 00011: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.5303 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.2705\n",
      "Epoch 00012: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8717.8525 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1962\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2910\n",
      "Epoch 00013: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.2773 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8968.6553\n",
      "Epoch 00014: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.5400 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1970\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8580.6602\n",
      "Epoch 00015: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8719.8223 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1967\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8623\n",
      "Epoch 00016: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.7832 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1963\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8877\n",
      "Epoch 00017: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.8203 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8606.8154\n",
      "Epoch 00018: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0547 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2119\n",
      "Epoch 00019: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.2393 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8973.6885\n",
      "Epoch 00020: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8734.4512 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8217.7236  ETA: 2s - loss: 0.0233 -\n",
      "Epoch 00021: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8730.0449 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8967.8350\n",
      "Epoch 00022: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8728.7363 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8965.8945\n",
      "Epoch 00023: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.8516 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1969\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8607.8672\n",
      "Epoch 00024: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0010 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1964\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8588.1279\n",
      "Epoch 00025: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8735.5576 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8230.3066\n",
      "Epoch 00026: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8738.8379 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 27/200\n",
      "13312/32614 [===========>..................] - ETA: 1s - loss: 0.0233 - mae: 0.0229 - mape: 7589.0488"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-05065ff65640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y, \n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9fPE-jZuQ0r",
    "outputId": "aca09565-e9df-4067-d473-32dd8826a738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 541.71\n",
      "Median Absolute Error (MedAE): 449.47\n",
      "Mean Squared Error (MSE): 473684.02\n",
      "Root Mean Squared Error (RMSE): 688.25\n",
      "Mean Absolute Percentage Error (MAPE): 5.4 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.39 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-0004-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7x0O-u0GbLy"
   },
   "source": [
    "# E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rdBroUOqIeW6"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(200, activation='relu')(x)\n",
    " #       x = Dense(10, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AQaM-IuPIXqN"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QwUKEvSfIW09"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NHgO7mJ4IQVp"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TIsLEDoIN0C",
    "outputId": "3ef77929-b334-4953-faab-b938f376c306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpLkyqBmIKIw",
    "outputId": "3d701f4a-270d-4efc-b52a-cd26d12d4dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.2038 - mae: 0.0831 - mape: 10284.1670\n",
      "Epoch 00001: val_loss improved from inf to 0.10131, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0001-loss0.10.h5\n",
      "32614/32614 [==============================] - 19s 581us/sample - loss: 0.2036 - mae: 0.0830 - mape: 10262.1201 - val_loss: 0.1013 - val_mae: 0.0675 - val_mape: 11.9466\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0654 - mae: 0.0492 - mape: 9617.4668\n",
      "Epoch 00002: val_loss improved from 0.10131 to 0.07428, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0002-loss0.07.h5\n",
      "32614/32614 [==============================] - 14s 420us/sample - loss: 0.0654 - mae: 0.0492 - mape: 9596.8438 - val_loss: 0.0743 - val_mae: 0.0671 - val_mape: 11.8373\n",
      "Epoch 3/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0511 - mae: 0.0461 - mape: 9812.7764\n",
      "Epoch 00003: val_loss improved from 0.07428 to 0.06341, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0003-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0510 - mae: 0.0460 - mape: 9762.8730 - val_loss: 0.0634 - val_mae: 0.0599 - val_mape: 10.4427\n",
      "Epoch 4/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0450 - mae: 0.0419 - mape: 9616.9199\n",
      "Epoch 00004: val_loss did not improve from 0.06341\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0450 - mae: 0.0419 - mape: 9596.3027 - val_loss: 0.0706 - val_mae: 0.0680 - val_mape: 11.4199\n",
      "Epoch 5/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0441 - mae: 0.0414 - mape: 9613.5029\n",
      "Epoch 00005: val_loss improved from 0.06341 to 0.05454, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0005-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0441 - mae: 0.0414 - mape: 9611.7354 - val_loss: 0.0545 - val_mae: 0.0519 - val_mape: 9.2312\n",
      "Epoch 6/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0418 - mae: 0.0397 - mape: 9602.1924\n",
      "Epoch 00006: val_loss did not improve from 0.05454\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0418 - mae: 0.0397 - mape: 9572.1885 - val_loss: 0.0587 - val_mae: 0.0570 - val_mape: 9.8649\n",
      "Epoch 7/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0406 - mae: 0.0389 - mape: 9195.3799\n",
      "Epoch 00007: val_loss improved from 0.05454 to 0.04682, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0007-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0406 - mae: 0.0389 - mape: 9554.0049 - val_loss: 0.0468 - val_mae: 0.0450 - val_mape: 8.7847\n",
      "Epoch 8/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0399 - mae: 0.0384 - mape: 9416.9941\n",
      "Epoch 00008: val_loss did not improve from 0.04682\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0399 - mae: 0.0384 - mape: 9396.7998 - val_loss: 0.0475 - val_mae: 0.0462 - val_mape: 8.9737\n",
      "Epoch 9/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0397 - mae: 0.0383 - mape: 9665.3730\n",
      "Epoch 00009: val_loss improved from 0.04682 to 0.04522, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0009-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0397 - mae: 0.0383 - mape: 9644.6445 - val_loss: 0.0452 - val_mae: 0.0439 - val_mape: 8.5116\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0377 - mape: 9573.1260\n",
      "Epoch 00010: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0389 - mae: 0.0377 - mape: 9533.8271 - val_loss: 0.0940 - val_mae: 0.0928 - val_mape: 15.8112\n",
      "Epoch 11/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0390 - mae: 0.0378 - mape: 9537.1748\n",
      "Epoch 00011: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0390 - mae: 0.0378 - mape: 9535.4209 - val_loss: 0.0501 - val_mae: 0.0490 - val_mape: 8.8788\n",
      "Epoch 12/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0381 - mae: 0.0371 - mape: 9512.4170\n",
      "Epoch 00012: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0381 - mae: 0.0371 - mape: 9492.0146 - val_loss: 0.0511 - val_mae: 0.0502 - val_mape: 9.2328\n",
      "Epoch 13/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0377 - mae: 0.0368 - mape: 9480.7461\n",
      "Epoch 00013: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0377 - mae: 0.0368 - mape: 9479.0029 - val_loss: 0.0494 - val_mae: 0.0485 - val_mape: 9.0546\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.0370 - mape: 9640.1309\n",
      "Epoch 00014: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0379 - mae: 0.0370 - mape: 9600.5547 - val_loss: 0.0475 - val_mae: 0.0466 - val_mape: 8.6730\n",
      "Epoch 15/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0375 - mae: 0.0367 - mape: 9567.5752\n",
      "Epoch 00015: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0375 - mae: 0.0367 - mape: 9565.8154 - val_loss: 0.0497 - val_mae: 0.0489 - val_mape: 9.3880\n",
      "Epoch 16/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0374 - mae: 0.0366 - mape: 9551.2051\n",
      "Epoch 00016: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0374 - mae: 0.0366 - mape: 9540.0859 - val_loss: 0.0464 - val_mae: 0.0456 - val_mape: 8.5940\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0362 - mape: 9563.9082\n",
      "Epoch 00017: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0369 - mae: 0.0362 - mape: 9562.1514 - val_loss: 0.0475 - val_mae: 0.0468 - val_mape: 8.6608\n",
      "Epoch 18/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0361 - mape: 9577.6992\n",
      "Epoch 00018: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0368 - mae: 0.0361 - mape: 9566.5488 - val_loss: 0.0627 - val_mae: 0.0620 - val_mape: 10.5263\n",
      "Epoch 19/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0366 - mae: 0.0359 - mape: 8800.0615\n",
      "Epoch 00019: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0366 - mae: 0.0359 - mape: 9530.1943 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 8.7106\n",
      "Epoch 20/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9487.9727\n",
      "Epoch 00020: val_loss improved from 0.04522 to 0.04432, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0020-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9476.9248 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.3836\n",
      "Epoch 21/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9175.1748\n",
      "Epoch 00021: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9548.0811 - val_loss: 0.0444 - val_mae: 0.0438 - val_mape: 8.4635\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0356 - mape: 9438.7158\n",
      "Epoch 00022: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0362 - mae: 0.0356 - mape: 9427.7275 - val_loss: 0.0463 - val_mae: 0.0456 - val_mape: 8.8022\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0363 - mae: 0.0356 - mape: 9494.2676\n",
      "Epoch 00023: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0363 - mae: 0.0356 - mape: 9492.5234 - val_loss: 0.0502 - val_mae: 0.0496 - val_mape: 8.8956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0360 - mae: 0.0354 - mape: 9481.0664\n",
      "Epoch 00024: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0360 - mae: 0.0354 - mape: 9479.3242 - val_loss: 0.0482 - val_mae: 0.0476 - val_mape: 8.8535\n",
      "Epoch 25/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9452.0479\n",
      "Epoch 00025: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9422.5107 - val_loss: 0.0450 - val_mae: 0.0444 - val_mape: 8.3675\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9389.7109\n",
      "Epoch 00026: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9360.3701 - val_loss: 0.0505 - val_mae: 0.0499 - val_mape: 8.9818\n",
      "Epoch 27/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9449.0146\n",
      "Epoch 00027: val_loss improved from 0.04432 to 0.04404, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0027-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9419.4844 - val_loss: 0.0440 - val_mae: 0.0434 - val_mape: 8.4639\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9357.8389\n",
      "Epoch 00028: val_loss improved from 0.04404 to 0.04385, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0028-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9346.9443 - val_loss: 0.0438 - val_mae: 0.0433 - val_mape: 8.3336\n",
      "Epoch 29/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0350 - mape: 9348.0967- ETA: 2s - loss: 0.\n",
      "Epoch 00029: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0356 - mae: 0.0350 - mape: 9337.2129 - val_loss: 0.0453 - val_mae: 0.0447 - val_mape: 8.8223\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0355 - mae: 0.0349 - mape: 9328.2656\n",
      "Epoch 00030: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0355 - mae: 0.0349 - mape: 9299.1152 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.4327\n",
      "Epoch 31/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0348 - mape: 9367.9561\n",
      "Epoch 00031: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0354 - mae: 0.0348 - mape: 9329.5146 - val_loss: 0.0556 - val_mae: 0.0550 - val_mape: 9.5869\n",
      "Epoch 32/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0346 - mae: 0.0341 - mape: 9331.7041\n",
      "Epoch 00032: val_loss improved from 0.04385 to 0.03952, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0032-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0346 - mae: 0.0341 - mape: 9293.3955 - val_loss: 0.0395 - val_mae: 0.0389 - val_mape: 7.3175\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0313 - mae: 0.0307 - mape: 8705.7988\n",
      "Epoch 00033: val_loss did not improve from 0.03952\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0313 - mae: 0.0307 - mape: 9014.9424 - val_loss: 0.0402 - val_mae: 0.0396 - val_mape: 6.9964\n",
      "Epoch 34/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0310 - mae: 0.0304 - mape: 8956.5234\n",
      "Epoch 00034: val_loss improved from 0.03952 to 0.03736, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0034-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 427us/sample - loss: 0.0310 - mae: 0.0304 - mape: 8937.3135 - val_loss: 0.0374 - val_mae: 0.0368 - val_mape: 6.7796\n",
      "Epoch 35/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0308 - mae: 0.0302 - mape: 9005.4199\n",
      "Epoch 00035: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0308 - mae: 0.0302 - mape: 9003.7646 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.5172\n",
      "Epoch 36/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 8986.6465\n",
      "Epoch 00036: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0307 - mae: 0.0301 - mape: 8984.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.7190\n",
      "Epoch 37/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 9036.8193\n",
      "Epoch 00037: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0307 - mae: 0.0301 - mape: 9017.4414 - val_loss: 0.0397 - val_mae: 0.0391 - val_mape: 6.9239\n",
      "Epoch 38/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0304 - mae: 0.0298 - mape: 8871.0625\n",
      "Epoch 00038: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0305 - mae: 0.0299 - mape: 8852.0391 - val_loss: 0.0418 - val_mae: 0.0413 - val_mape: 7.1686\n",
      "Epoch 39/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8882.9082\n",
      "Epoch 00039: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8881.2744 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.0621\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8929.6758\n",
      "Epoch 00040: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8910.5225 - val_loss: 0.0390 - val_mae: 0.0384 - val_mape: 6.7820\n",
      "Epoch 41/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8876.4180\n",
      "Epoch 00041: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8857.3779 - val_loss: 0.0391 - val_mae: 0.0385 - val_mape: 6.9458\n",
      "Epoch 42/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8984.8389\n",
      "Epoch 00042: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8956.7549 - val_loss: 0.0413 - val_mae: 0.0407 - val_mape: 7.6354\n",
      "Epoch 43/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0301 - mae: 0.0295 - mape: 8793.3711\n",
      "Epoch 00043: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0301 - mae: 0.0295 - mape: 8783.1338 - val_loss: 0.0435 - val_mae: 0.0429 - val_mape: 7.4288\n",
      "Epoch 44/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8822.2676- ETA: 6s - loss: 0.0301 - mae: 0.0 - ETA: 1s - loss: 0.0302 - mae:\n",
      "Epoch 00044: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8811.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.9922\n",
      "Epoch 45/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0298 - mae: 0.0292 - mape: 8836.8652\n",
      "Epoch 00045: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0298 - mae: 0.0292 - mape: 8800.5830 - val_loss: 0.0593 - val_mae: 0.0587 - val_mape: 11.2009\n",
      "Epoch 46/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0287 - mae: 0.0281 - mape: 8818.6738\n",
      "Epoch 00046: val_loss improved from 0.03736 to 0.03728, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0046-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0287 - mae: 0.0281 - mape: 8791.1104 - val_loss: 0.0373 - val_mae: 0.0367 - val_mape: 6.3479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0269 - mape: 8903.1064\n",
      "Epoch 00047: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0276 - mae: 0.0269 - mape: 8866.5479 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 7.3343\n",
      "Epoch 48/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0264 - mape: 8938.7715\n",
      "Epoch 00048: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0270 - mae: 0.0264 - mape: 8919.5977 - val_loss: 0.0419 - val_mae: 0.0413 - val_mape: 6.8349\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0265 - mape: 8954.8320\n",
      "Epoch 00049: val_loss improved from 0.03728 to 0.03701, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0049-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0272 - mae: 0.0265 - mape: 8926.8408 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2637\n",
      "Epoch 50/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0263 - mape: 8925.1689\n",
      "Epoch 00050: val_loss did not improve from 0.03701\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0269 - mae: 0.0263 - mape: 8914.7754 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2835\n",
      "Epoch 51/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0262 - mape: 8973.1758\n",
      "Epoch 00051: val_loss improved from 0.03701 to 0.03644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0051-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0269 - mae: 0.0262 - mape: 8936.3379 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3367\n",
      "Epoch 52/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0267 - mae: 0.0260 - mape: 8984.7070\n",
      "Epoch 00052: val_loss improved from 0.03644 to 0.03603, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0267 - mae: 0.0260 - mape: 8947.8154 - val_loss: 0.0360 - val_mae: 0.0354 - val_mape: 6.2870\n",
      "Epoch 53/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.0259 - mape: 8895.4316\n",
      "Epoch 00053: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0265 - mae: 0.0259 - mape: 8893.7969 - val_loss: 0.0433 - val_mae: 0.0426 - val_mape: 6.9243\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0258 - mape: 8529.0977\n",
      "Epoch 00054: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0264 - mae: 0.0258 - mape: 8883.7363 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.3396\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8902.2764\n",
      "Epoch 00055: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8883.1807 - val_loss: 0.0377 - val_mae: 0.0371 - val_mape: 6.5213\n",
      "Epoch 56/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8834.2520\n",
      "Epoch 00056: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8806.6387 - val_loss: 0.0378 - val_mae: 0.0372 - val_mape: 6.4096\n",
      "Epoch 57/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0255 - mape: 8899.8281\n",
      "Epoch 00057: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0261 - mae: 0.0255 - mape: 8889.4648 - val_loss: 0.0365 - val_mae: 0.0358 - val_mape: 6.4844\n",
      "Epoch 58/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0257 - mape: 8935.8350\n",
      "Epoch 00058: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0263 - mae: 0.0257 - mape: 8899.1426 - val_loss: 0.0389 - val_mae: 0.0382 - val_mape: 6.9754\n",
      "Epoch 59/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0255 - mape: 8985.3438\n",
      "Epoch 00059: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0262 - mae: 0.0255 - mape: 8966.0693 - val_loss: 0.0448 - val_mae: 0.0442 - val_mape: 7.1970\n",
      "Epoch 60/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0254 - mape: 8698.2939\n",
      "Epoch 00060: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0261 - mae: 0.0254 - mape: 9026.8086 - val_loss: 0.0371 - val_mae: 0.0365 - val_mape: 6.4728\n",
      "Epoch 61/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0256 - mape: 8944.7266\n",
      "Epoch 00061: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0262 - mae: 0.0256 - mape: 8943.0820 - val_loss: 0.0376 - val_mae: 0.0369 - val_mape: 6.4816\n",
      "Epoch 62/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0253 - mape: 8861.0674\n",
      "Epoch 00062: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0259 - mae: 0.0253 - mape: 8833.3691 - val_loss: 0.0376 - val_mae: 0.0370 - val_mape: 6.7046\n",
      "Epoch 63/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0252 - mape: 8937.2539\n",
      "Epoch 00063: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0259 - mae: 0.0252 - mape: 8909.3174 - val_loss: 0.0441 - val_mae: 0.0434 - val_mape: 7.0393\n",
      "Epoch 64/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0251 - mape: 8836.8535\n",
      "Epoch 00064: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0257 - mae: 0.0251 - mape: 8826.5635 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.1966\n",
      "Epoch 65/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8864.3887\n",
      "Epoch 00065: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8845.3740 - val_loss: 0.0403 - val_mae: 0.0396 - val_mape: 6.6836\n",
      "Epoch 66/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0251 - mape: 8864.2441\n",
      "Epoch 00066: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0258 - mae: 0.0251 - mape: 8853.9238 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.2001\n",
      "Epoch 67/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8919.5137- ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8391\n",
      "Epoch 00067: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0256 - mae: 0.0249 - mape: 8900.3799 - val_loss: 0.0435 - val_mae: 0.0428 - val_mape: 6.9965\n",
      "Epoch 68/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0249 - mape: 8921.0537\n",
      "Epoch 00068: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0255 - mae: 0.0249 - mape: 8910.6650 - val_loss: 0.0368 - val_mae: 0.0361 - val_mape: 6.4016\n",
      "Epoch 69/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8904.1221\n",
      "Epoch 00069: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8885.0215 - val_loss: 0.0366 - val_mae: 0.0359 - val_mape: 6.4782\n",
      "Epoch 70/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0248 - mape: 8509.3799\n",
      "Epoch 00070: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0255 - mae: 0.0248 - mape: 8848.4863 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0247 - mape: 8894.0742\n",
      "Epoch 00071: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0254 - mae: 0.0247 - mape: 8874.9961 - val_loss: 0.0447 - val_mae: 0.0440 - val_mape: 7.2170\n",
      "Epoch 72/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0246 - mape: 8940.8789\n",
      "Epoch 00072: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0253 - mae: 0.0246 - mape: 8921.6992 - val_loss: 0.0399 - val_mae: 0.0393 - val_mape: 6.6394\n",
      "Epoch 73/80\n",
      "31840/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0246 - mape: 8697.8066"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d15c63a445f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BNTzwnub2Eh-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 577.97\n",
      "Median Absolute Error (MedAE): 474.07\n",
      "Mean Squared Error (MSE): 554533.54\n",
      "Root Mean Squared Error (RMSE): 744.67\n",
      "Mean Absolute Percentage Error (MAPE): 5.86 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.58 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tYU2gaBI4Yl"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fz9kU3b6IvyO"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5'\n",
    "start_epoch= 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2bMbPx8RJKL4"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5T4Ho3YJJKTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "14t41QwoJKZ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0241 - mae: 0.0235 - mape: 8787.5186\n",
      "Epoch 00001: val_loss improved from inf to 0.03371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 57s 2ms/sample - loss: 0.0241 - mae: 0.0235 - mape: 8785.9023 - val_loss: 0.0337 - val_mae: 0.0331 - val_mape: 5.6219\n",
      "Epoch 2/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8818.7939\n",
      "Epoch 00002: val_loss improved from 0.03371 to 0.03268, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 8808.5244 - val_loss: 0.0327 - val_mae: 0.0321 - val_mape: 5.5019\n",
      "Epoch 3/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 8832.4248\n",
      "Epoch 00003: val_loss improved from 0.03268 to 0.03238, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 8830.8018 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4341\n",
      "Epoch 4/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 8807.1309\n",
      "Epoch 00004: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 8805.5117 - val_loss: 0.0330 - val_mae: 0.0324 - val_mape: 5.4931\n",
      "Epoch 5/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0212 - mape: 8834.0068\n",
      "Epoch 00005: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0212 - mape: 8832.3828 - val_loss: 0.0333 - val_mae: 0.0328 - val_mape: 5.5382\n",
      "Epoch 6/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8771.0771\n",
      "Epoch 00006: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8769.4648 - val_loss: 0.0326 - val_mae: 0.0321 - val_mape: 5.4661\n",
      "Epoch 7/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8793.5381\n",
      "Epoch 00007: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8783.2979 - val_loss: 0.0325 - val_mae: 0.0320 - val_mape: 5.4550\n",
      "Epoch 8/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8746.4922\n",
      "Epoch 00008: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8744.8848 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.4740\n",
      "Epoch 9/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8799.6211\n",
      "Epoch 00009: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8789.3721 - val_loss: 0.0332 - val_mae: 0.0327 - val_mape: 5.5137\n",
      "Epoch 10/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8783.9893\n",
      "Epoch 00010: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8782.3740 - val_loss: 0.0327 - val_mae: 0.0322 - val_mape: 5.4856\n",
      "Epoch 11/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8814.5137\n",
      "Epoch 00011: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8804.2490 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.4683\n",
      "Epoch 12/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0208 - mape: 8829.3320\n",
      "Epoch 00012: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0213 - mae: 0.0208 - mape: 8819.0498 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4762\n",
      "Epoch 13/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8766.2891\n",
      "Epoch 00013: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8764.6777 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.6098\n",
      "Epoch 14/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0208 - mape: 8807.6914\n",
      "Epoch 00014: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0208 - mape: 8797.4355 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4454\n",
      "Epoch 15/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8763.3545\n",
      "Epoch 00015: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8761.7432 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4502\n",
      "Epoch 16/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0207 - mape: 8800.9727\n",
      "Epoch 00016: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0207 - mape: 8790.7227 - val_loss: 0.0336 - val_mae: 0.0331 - val_mape: 5.5508\n",
      "Epoch 17/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8805.7637\n",
      "Epoch 00017: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 8804.1465 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5136\n",
      "Epoch 18/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8785.8975\n",
      "Epoch 00018: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8775.6660 - val_loss: 0.0324 - val_mae: 0.0320 - val_mape: 5.4366\n",
      "Epoch 19/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8766.3740\n",
      "Epoch 00019: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8756.1650 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5053\n",
      "Epoch 20/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8869.6514- ETA: 1s - loss: 0.0209 - mae: 0.0205\n",
      "Epoch 00020: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8859.3301 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5394\n",
      "Epoch 21/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8719.9990\n",
      "Epoch 00021: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8718.3955 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.4912\n",
      "Epoch 22/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8816.5059\n",
      "Epoch 00022: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8814.8848 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5017\n",
      "Epoch 23/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8851.2988\n",
      "Epoch 00023: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8840.9902 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5257\n",
      "Epoch 24/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8849.9473\n",
      "Epoch 00024: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8839.6475 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8417.9062\n",
      "Epoch 00025: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8762.0635 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5165\n",
      "Epoch 26/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8781.5459\n",
      "Epoch 00026: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8771.3203 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5305\n",
      "Epoch 27/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8805.2725\n",
      "Epoch 00027: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8795.0176 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5373\n",
      "Epoch 28/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8814.1201\n",
      "Epoch 00028: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8803.8555 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5812\n",
      "Epoch 29/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8835.2373\n",
      "Epoch 00029: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8833.6133 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.5439\n",
      "Epoch 30/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8756.9062\n",
      "Epoch 00030: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8746.7100 - val_loss: 0.0335 - val_mae: 0.0331 - val_mape: 5.5782\n",
      "Epoch 31/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8848.8691\n",
      "Epoch 00031: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8847.2422 - val_loss: 0.0345 - val_mae: 0.0342 - val_mape: 5.6903\n",
      "Epoch 32/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8786.9775\n",
      "Epoch 00032: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8785.3623 - val_loss: 0.0348 - val_mae: 0.0344 - val_mape: 5.7179\n",
      "Epoch 33/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8790.1523\n",
      "Epoch 00033: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8779.9150 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5215\n",
      "Epoch 34/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8812.7725\n",
      "Epoch 00034: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8811.1523 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5479\n",
      "Epoch 35/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8778.3477-\n",
      "Epoch 00035: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8776.7334 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5697\n",
      "Epoch 36/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8766.7051\n",
      "Epoch 00036: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8756.4961 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.6073\n",
      "Epoch 37/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0200 - mape: 8769.1572\n",
      "Epoch 00037: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0200 - mape: 8767.5449 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5441\n",
      "Epoch 38/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0201 - mape: 8747.3877\n",
      "Epoch 00038: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0201 - mape: 8745.7793 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5797\n",
      "Epoch 39/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0200 - mape: 8836.6064\n",
      "Epoch 00039: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0200 - mape: 8834.9814 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5334\n",
      "Epoch 40/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8814.8398\n",
      "Epoch 00040: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8804.5742 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5985\n",
      "Epoch 41/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8839.9746\n",
      "Epoch 00041: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8829.6797 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5734\n",
      "Epoch 42/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0198 - mape: 8831.3848\n",
      "Epoch 00042: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0202 - mae: 0.0198 - mape: 8829.7607 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5997\n",
      "Epoch 43/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0201 - mae: 0.0198 - mape: 8783.8525\n",
      "Epoch 00043: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0201 - mae: 0.0198 - mape: 8782.2383 - val_loss: 0.0337 - val_mae: 0.0334 - val_mape: 5.6261\n",
      "Epoch 44/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0197 - mape: 8822.6426\n",
      "Epoch 00044: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0197 - mape: 8821.0205 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.6350\n",
      "Epoch 45/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8898.9219\n",
      "Epoch 00045: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8888.5576 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5640\n",
      "Epoch 46/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8848.5547\n",
      "Epoch 00046: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8846.9277 - val_loss: 0.0326 - val_mae: 0.0323 - val_mape: 5.5130\n",
      "Epoch 47/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8915.1211\n",
      "Epoch 00047: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8904.7383 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5152\n",
      "Epoch 48/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8884.6523\n",
      "Epoch 00048: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8874.3057 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5568\n",
      "Epoch 49/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0195 - mape: 8901.6914\n",
      "Epoch 00049: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0195 - mape: 8891.3252 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8912.0732\n",
      "Epoch 00050: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8901.6934 - val_loss: 0.0326 - val_mae: 0.0322 - val_mape: 5.5117\n",
      "Epoch 51/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0194 - mape: 8897.9141\n",
      "Epoch 00051: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0194 - mape: 8896.2773 - val_loss: 0.0339 - val_mae: 0.0336 - val_mape: 5.6620\n",
      "Epoch 52/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8820.4697\n",
      "Epoch 00052: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8810.1973 - val_loss: 0.0336 - val_mae: 0.0333 - val_mape: 5.6221\n",
      "Epoch 53/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8908.1738\n",
      "Epoch 00053: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8906.5361 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5511\n",
      "Epoch 54/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8871.0322\n",
      "Epoch 00054: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8869.4014 - val_loss: 0.0333 - val_mae: 0.0330 - val_mape: 5.5471\n",
      "Epoch 55/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8876.8662\n",
      "Epoch 00055: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8866.5283 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5071\n",
      "Epoch 56/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8917.7432- ETA: 2s - loss: 0.0196 - ma\n",
      "Epoch 00056: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8916.1035 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.5076\n",
      "Epoch 57/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8886.0176\n",
      "Epoch 00057: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8884.3838 - val_loss: 0.0330 - val_mae: 0.0327 - val_mape: 5.6687\n",
      "Epoch 58/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8843.5547\n",
      "Epoch 00058: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0195 - mae: 0.0192 - mape: 8833.2559 - val_loss: 0.0340 - val_mae: 0.0337 - val_mape: 5.6366\n",
      "Epoch 59/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8897.1895\n",
      "Epoch 00059: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8886.8271 - val_loss: 0.0344 - val_mae: 0.0340 - val_mape: 5.7448\n",
      "Epoch 60/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8878.5566\n",
      "Epoch 00060: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0192 - mape: 8868.2227 - val_loss: 0.0338 - val_mae: 0.0334 - val_mape: 5.6100\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.92\n",
      "Median Absolute Error (MedAE): 341.57\n",
      "Mean Squared Error (MSE): 329961.29\n",
      "Root Mean Squared Error (RMSE): 574.42\n",
      "Mean Absolute Percentage Error (MAPE): 4.31 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.33 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPZ0OMjDJhnO"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UZgZB020JqhL"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5'\n",
    "start_epoch= 53+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ssm3plQeJn6F"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "u-3UhfYuJlge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "9b_pJN2ZJKff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9133.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03257, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 9s 283us/sample - loss: 0.0214 - mae: 0.0208 - mape: 8890.3867 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4449\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8733.1416\n",
      "Epoch 00002: val_loss improved from 0.03257 to 0.03256, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8869.8643 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4437\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9107.9521\n",
      "Epoch 00003: val_loss improved from 0.03256 to 0.03242, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8865.1064 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8707.8369\n",
      "Epoch 00004: val_loss improved from 0.03242 to 0.03233, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8851.4385 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4285\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9089.2246\n",
      "Epoch 00005: val_loss improved from 0.03233 to 0.03228, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8846.9043 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8326.9922\n",
      "Epoch 00006: val_loss improved from 0.03228 to 0.03226, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8841.4912 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8705.7549\n",
      "Epoch 00007: val_loss improved from 0.03226 to 0.03223, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8839.4424 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4216\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9068.8096\n",
      "Epoch 00008: val_loss improved from 0.03223 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8827.0107 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4208\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8693.2275  ETA: 1s - loss: 0.0213 - mae: 0.020\n",
      "Epoch 00009: val_loss improved from 0.03222 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8828.1250 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8320.9189\n",
      "Epoch 00010: val_loss improved from 0.03222 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8823.5527 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8678.0664\n",
      "Epoch 00011: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8818.6221 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9058.5664\n",
      "Epoch 00012: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8817.0547 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.8018\n",
      "Epoch 00013: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8819.4121 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9055.2490\n",
      "Epoch 00014: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8813.7979 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9056.6045\n",
      "Epoch 00015: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8815.1211 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4180\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9046.7842\n",
      "Epoch 00016: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8805.6162 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.0303\n",
      "Epoch 00017: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8811.1191 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4184\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8288.7080\n",
      "Epoch 00018: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8806.9727 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9051.8076\n",
      "Epoch 00019: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.4658 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8302.6875\n",
      "Epoch 00020: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.8389 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4196\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.1914\n",
      "Epoch 00021: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8803.5537 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8289.9629\n",
      "Epoch 00022: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8808.2617 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.5645\n",
      "Epoch 00023: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8807.9199 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8657.6855\n",
      "Epoch 00024: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.0996 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8660.9229- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00025: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4287 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4194\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.9697\n",
      "Epoch 00026: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.0215 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4190\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.5674- ETA: 1s - loss: 0.0211 - mae: 0.0205\n",
      "Epoch 00027: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.4775 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.4648\n",
      "Epoch 00028: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4198\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8674.4756\n",
      "Epoch 00029: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.5635 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7906.3037\n",
      "Epoch 00030: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.4629 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.0000\n",
      "Epoch 00031: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.7725 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4202\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9045.3105\n",
      "Epoch 00032: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.1299 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.4668  ETA: 2s - loss: 0.0211 - m\n",
      "Epoch 00033: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.4404 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8285.9170\n",
      "Epoch 00034: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.4795 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4204\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.9082\n",
      "Epoch 00035: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6709 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8681.3047- ETA: 2s - loss: 0.0210 - mae: 0.\n",
      "Epoch 00036: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.3027 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1055- ETA: 2s - loss: 0.0211 - mae: \n",
      "Epoch 00037: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.6387 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.3789\n",
      "Epoch 00038: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4212\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7911.5845\n",
      "Epoch 00039: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.6006 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4207\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7988\n",
      "Epoch 00040: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.7549 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4209\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.4160\n",
      "Epoch 00041: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8797.7119 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4218\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.8086\n",
      "Epoch 00042: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.4561 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9040.8076- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8763.4\n",
      "Epoch 00043: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.7510 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8671.4062\n",
      "Epoch 00044: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9043.8984\n",
      "Epoch 00045: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.7754 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.5801\n",
      "Epoch 00046: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4297 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4199\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.4580\n",
      "Epoch 00047: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.2539 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4226\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8656.3926\n",
      "Epoch 00048: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8795.1895 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4227\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.8350\n",
      "Epoch 00049: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6064 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9053.7900\n",
      "Epoch 00050: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8812.3799 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4222\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.2178\n",
      "Epoch 00051: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.1719 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4229\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.2744\n",
      "Epoch 00052: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.0586 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4240\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7900\n",
      "Epoch 00053: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.5859 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8664.6738\n",
      "Epoch 00054: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.6367 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4230\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8658.2246\n",
      "Epoch 00055: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.2617 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4231\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9039.0703\n",
      "Epoch 00056: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.0605 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.2510\n",
      "Epoch 00057: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.1816 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4238\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.6846\n",
      "Epoch 00058: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.4893 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4235\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.6758\n",
      "Epoch 00059: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4250\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.4639\n",
      "Epoch 00060: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.2031 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.5479\n",
      "Epoch 00061: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8801.4482 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.7363\n",
      "Epoch 00062: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.5029 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1113- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7955. - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8069\n",
      "Epoch 00063: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8803.5488 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.6602\n",
      "Epoch 00064: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8809.3477 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8292.0371\n",
      "Epoch 00065: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8800.8613 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4243\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8661.9170\n",
      "Epoch 00066: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.5088 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4233\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.0117\n",
      "Epoch 00067: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.7666 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.6230- ETA: 2s - loss: 0.0212 - mae:\n",
      "Epoch 00068: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.3867 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4254\n",
      "Epoch 69/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8285.9189\n",
      "Epoch 00069: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.3457 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4237\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.5889\n",
      "Epoch 00070: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.3418 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.3438\n",
      "Epoch 00071: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8803.1846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4249\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.1455\n",
      "Epoch 00072: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.9678 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8665.8174\n",
      "Epoch 00073: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.1689 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.3008\n",
      "Epoch 00074: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.6846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.2236\n",
      "Epoch 00075: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.9883 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4261\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.5234  ETA: 2s - loss: 0.0209 - \n",
      "Epoch 00076: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4239\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9045.4512\n",
      "Epoch 00077: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.2715 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4265\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.2568\n",
      "Epoch 00078: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0527 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4256\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9055.4580\n",
      "Epoch 00079: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.0088 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4271\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9054.0859\n",
      "Epoch 00080: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8812.6738 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9052.4775\n",
      "Epoch 00081: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.1074 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4263\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.4385\n",
      "Epoch 00082: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.1035 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.2695\n",
      "Epoch 00083: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.9912 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8680.7773\n",
      "Epoch 00084: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8681.9316\n",
      "Epoch 00085: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.9023 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4258\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.9326\n",
      "Epoch 00086: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.5898 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.4629\n",
      "Epoch 00087: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.1172 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4259\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8666.3701\n",
      "Epoch 00088: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2188 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8664.1152\n",
      "Epoch 00089: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.5791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4258\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.7578\n",
      "Epoch 00090: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4267\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8674.5449\n",
      "Epoch 00091: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.6641 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4264\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.3330\n",
      "Epoch 00092: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.8057 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8680.4805\n",
      "Epoch 00093: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8807.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4268\n",
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8669.4492\n",
      "Epoch 00094: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0020 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8670.0195\n",
      "Epoch 00095: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.6592 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.8623\n",
      "Epoch 00096: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8808.5645 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4274\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8297.3535\n",
      "Epoch 00097: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8814.3789 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.8828\n",
      "Epoch 00098: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8803.7236 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8692.5449\n",
      "Epoch 00099: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8821.6729 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4253\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.0117  ETA: 2s - loss: 0.0212 - mae: 0.0207 - mape - ETA: 1s - loss: 0.0211 - mae: 0.0206 - mape: 13501.83 - ETA: 1s - loss: 0.0211 - mae: 0.0\n",
      "Epoch 00100: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.5918 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8291.2773\n",
      "Epoch 00101: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.7295 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4274\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.1973\n",
      "Epoch 00102: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.7637 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4269\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.2109\n",
      "Epoch 00103: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.1260 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.0146\n",
      "Epoch 00104: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.8604 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8309.9893\n",
      "Epoch 00105: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.9658 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8300.4561\n",
      "Epoch 00106: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.9160 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8687.0166\n",
      "Epoch 00107: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8820.6562 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4257\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.9922  ETA: 1s - loss: 0.021\n",
      "Epoch 00108: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8818.7070 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7918.0552\n",
      "Epoch 00109: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8806.1182 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4266\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9054.2461\n",
      "Epoch 00110: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8812.8340 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 111/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.2012\n",
      "Epoch 00111: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8809.0146 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 112/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.0439\n",
      "Epoch 00112: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.5566 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 113/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8305.8838\n",
      "Epoch 00113: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.7695 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4286\n",
      "Epoch 114/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9051.7646\n",
      "Epoch 00114: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.4189 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4268\n",
      "Epoch 115/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8671.9346\n",
      "Epoch 00115: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.5635 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4283\n",
      "Epoch 116/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.4092  ETA: 2s - loss: \n",
      "Epoch 00116: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.9219 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 117/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8685.8936- ETA: 1s - loss: 0.0209 - mae: 0.02\n",
      "Epoch 00117: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8817.3330 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 118/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8302.0400\n",
      "Epoch 00118: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8812.6797 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4707\n",
      "Epoch 00119: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.0967 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 120/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8304.8125\n",
      "Epoch 00120: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8818.3047 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4265\n",
      "Epoch 121/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9058.0273\n",
      "Epoch 00121: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5293 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4270\n",
      "Epoch 122/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8682.1084  ETA: 1s - loss: 0.0208 - mae:\n",
      "Epoch 00122: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.7100 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 123/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9050.8926\n",
      "Epoch 00123: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.5742 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4287\n",
      "Epoch 124/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.2852- ETA: 2s - loss: 0.0209 - mae:\n",
      "Epoch 00124: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.8037 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4277\n",
      "Epoch 125/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.9121\n",
      "Epoch 00125: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.4541 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 126/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7553.2466\n",
      "Epoch 00126: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.8301 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 127/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.8857\n",
      "Epoch 00127: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.4443 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 128/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.3145\n",
      "Epoch 00128: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.7705 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 129/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9062.5234\n",
      "Epoch 00129: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8820.9062 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 130/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8310.6270\n",
      "Epoch 00130: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.6572 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4276\n",
      "Epoch 131/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8676.7832\n",
      "Epoch 00131: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8813.8525 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 132/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.6250\n",
      "Epoch 00132: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1289 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 133/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.2627\n",
      "Epoch 00133: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.7959 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 134/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9385\n",
      "Epoch 00134: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.0283 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 135/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9990\n",
      "Epoch 00135: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.8438 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 136/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9050.7119- ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8224.\n",
      "Epoch 00136: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.3926 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 137/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9065.7979\n",
      "Epoch 00137: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.0713 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 138/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.0449\n",
      "Epoch 00138: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.1436 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4327\n",
      "Epoch 139/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.4678\n",
      "Epoch 00139: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.9834 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 140/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.8877\n",
      "Epoch 00140: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.4102 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 141/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.1895\n",
      "Epoch 00141: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.6465 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4293\n",
      "Epoch 142/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9068.6006\n",
      "Epoch 00142: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8826.8496 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4311\n",
      "Epoch 143/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.5254\n",
      "Epoch 00143: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.0791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8303.7139\n",
      "Epoch 00144: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.9434 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 145/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9057.6035\n",
      "Epoch 00145: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1113 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 146/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8295.9824\n",
      "Epoch 00146: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 147/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8299.4932- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00147: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.4980 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 148/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8677.7822\n",
      "Epoch 00148: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.8496 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 149/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4521\n",
      "Epoch 00149: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8807.3672 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4317\n",
      "Epoch 150/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8687.6270\n",
      "Epoch 00150: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.2520 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 151/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7550.1221\n",
      "Epoch 00151: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5537 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4318\n",
      "Epoch 152/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8665.8770\n",
      "Epoch 00152: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.2012 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4307\n",
      "Epoch 153/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8305.3477\n",
      "Epoch 00153: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.7168 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 154/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8664.9102\n",
      "Epoch 00154: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8803.5449 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 155/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.1807\n",
      "Epoch 00155: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.1172 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4323\n",
      "Epoch 156/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9061.2051\n",
      "Epoch 00156: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8819.6104 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 157/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9058.3799\n",
      "Epoch 00157: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.8613 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4316\n",
      "Epoch 158/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9053.1445\n",
      "Epoch 00158: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8811.7646 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 159/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.0205\n",
      "Epoch 00159: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8813.5781 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 160/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8673.6445\n",
      "Epoch 00160: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8812.8994 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4306\n",
      "Epoch 161/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.0654\n",
      "Epoch 00161: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8804.1348 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 162/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8678.7412\n",
      "Epoch 00162: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8817.2510 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 163/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9058.5049  ETA: 1s - loss: 0.0209 - mae: 0.0204 - map\n",
      "Epoch 00163: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.9990 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4288\n",
      "Epoch 164/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8671.3730\n",
      "Epoch 00164: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8807.5947 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 165/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8304.8350\n",
      "Epoch 00165: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8825.1436 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4294\n",
      "Epoch 166/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7930.7822\n",
      "Epoch 00166: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.1318 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 167/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8676.1992  ETA: 1s - loss: 0.0209 - mae: 0.0203 - mape:  - ETA: 1s - loss: 0.0208 - mae: 0.0202 - mape: - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9\n",
      "Epoch 00167: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.6123 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8682.7363\n",
      "Epoch 00168: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.8896 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 169/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9065.5938\n",
      "Epoch 00169: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8823.9004 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 170/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9051.8340\n",
      "Epoch 00170: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.4756 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 171/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8678.6602- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8967.800\n",
      "Epoch 00171: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8815.0049 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 172/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.9189\n",
      "Epoch 00172: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.3682 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 173/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9055.6904\n",
      "Epoch 00173: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2402 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 174/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.9043- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8588.017\n",
      "Epoch 00174: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.3857 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 175/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.7666\n",
      "Epoch 00175: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3809 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 176/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8673.9277\n",
      "Epoch 00176: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.3027 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4343\n",
      "Epoch 177/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.8848\n",
      "Epoch 00177: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2305 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4321\n",
      "Epoch 178/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9054.9736  ETA: 1s - loss: 0.0208 - mae: 0.0203 \n",
      "Epoch 00178: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.5439 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4330\n",
      "Epoch 179/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8675.4736\n",
      "Epoch 00179: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8809.1797 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4327\n",
      "Epoch 180/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9061.8086\n",
      "Epoch 00180: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2021 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4329\n",
      "Epoch 181/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9060.0869\n",
      "Epoch 00181: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5107 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4319\n",
      "Epoch 182/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.6191\n",
      "Epoch 00182: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5000 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 183/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.7178\n",
      "Epoch 00183: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2656 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4324\n",
      "Epoch 184/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8291.2705\n",
      "Epoch 00184: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.1533 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4331\n",
      "Epoch 185/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.7324\n",
      "Epoch 00185: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.8145 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 186/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.2549\n",
      "Epoch 00186: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8817.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4305\n",
      "Epoch 187/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.2168\n",
      "Epoch 00187: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8819.0938 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4315\n",
      "Epoch 188/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.7939\n",
      "Epoch 00188: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.2949 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 189/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.9834\n",
      "Epoch 00189: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.4678 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 190/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.2012\n",
      "Epoch 00190: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7588 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4332\n",
      "Epoch 191/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8679.1182\n",
      "Epoch 00191: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.3721 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4325\n",
      "Epoch 192/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8303.0508\n",
      "Epoch 00192: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7334 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 193/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8677.2891\n",
      "Epoch 00193: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.0742 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4315\n",
      "Epoch 194/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9051.3916\n",
      "Epoch 00194: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.0596 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4353\n",
      "Epoch 195/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 7911.0254\n",
      "Epoch 00195: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.6299 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 196/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9053.1436\n",
      "Epoch 00196: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.7627 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4345\n",
      "Epoch 197/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9056.7646- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 73\n",
      "Epoch 00197: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.2822 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 198/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8293.9717\n",
      "Epoch 00198: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3867 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4346\n",
      "Epoch 199/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.1162\n",
      "Epoch 00199: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.7646 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4328\n",
      "Epoch 200/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.9668- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 796\n",
      "Epoch 00200: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8808.9336 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4344\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.57\n",
      "Median Absolute Error (MedAE): 338.68\n",
      "Mean Squared Error (MSE): 332645.96\n",
      "Root Mean Squared Error (RMSE): 576.75\n",
      "Mean Absolute Percentage Error (MAPE): 4.3 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.3 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-0013-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aW5pp6LD-1"
   },
   "source": [
    "# E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "j9RoOmzuLgHR"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B2H5iBwQLfhj"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "k06fUuK7LerZ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mB2kXVVCLRBA"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sizKJbygLNBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Q6uDSkmdLJlR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.1973 - mae: 0.0794 - mape: 9692.2637\n",
      "Epoch 00001: val_loss improved from inf to 0.10644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0001-loss0.11.h5\n",
      "32614/32614 [==============================] - 19s 584us/sample - loss: 0.1972 - mae: 0.0794 - mape: 9680.9932 - val_loss: 0.1064 - val_mae: 0.0766 - val_mape: 12.9892\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0557 - mae: 0.0421 - mape: 9249.9512\n",
      "Epoch 00002: val_loss improved from 0.10644 to 0.05840, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0002-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0556 - mae: 0.0420 - mape: 9230.1133 - val_loss: 0.0584 - val_mae: 0.0523 - val_mape: 8.7301\n",
      "Epoch 3/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0416 - mae: 0.0375 - mape: 9262.3770\n",
      "Epoch 00003: val_loss improved from 0.05840 to 0.05401, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0003-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0416 - mae: 0.0375 - mape: 9224.3594 - val_loss: 0.0540 - val_mae: 0.0503 - val_mape: 8.7703\n",
      "Epoch 4/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0398 - mae: 0.0367 - mape: 9258.6729\n",
      "Epoch 00004: val_loss improved from 0.05401 to 0.04371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 431us/sample - loss: 0.0398 - mae: 0.0367 - mape: 9247.8994 - val_loss: 0.0437 - val_mae: 0.0407 - val_mape: 6.7483\n",
      "Epoch 5/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0336 - mape: 9132.0303  ETA: 4s - \n",
      "Epoch 00005: val_loss did not improve from 0.04371\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0362 - mae: 0.0336 - mape: 9121.3975 - val_loss: 0.0456 - val_mae: 0.0433 - val_mape: 7.2883\n",
      "Epoch 6/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0332 - mape: 9247.4834\n",
      "Epoch 00006: val_loss improved from 0.04371 to 0.03429, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0358 - mae: 0.0332 - mape: 9227.6475 - val_loss: 0.0343 - val_mae: 0.0317 - val_mape: 5.5244\n",
      "Epoch 7/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0334 - mae: 0.0314 - mape: 8832.6689\n",
      "Epoch 00007: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0334 - mae: 0.0314 - mape: 9193.4805 - val_loss: 0.0770 - val_mae: 0.0752 - val_mape: 10.8461\n",
      "Epoch 8/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0309 - mape: 9279.7900\n",
      "Epoch 00008: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0326 - mae: 0.0309 - mape: 9241.6865 - val_loss: 0.0911 - val_mae: 0.0895 - val_mape: 12.6461\n",
      "Epoch 9/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0301 - mape: 9240.0088\n",
      "Epoch 00009: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0315 - mae: 0.0301 - mape: 9211.1299 - val_loss: 0.0497 - val_mae: 0.0483 - val_mape: 7.3868\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0289 - mape: 8889.9570\n",
      "Epoch 00010: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0303 - mae: 0.0289 - mape: 9243.3398 - val_loss: 0.0727 - val_mae: 0.0714 - val_mape: 10.6575\n",
      "Epoch 11/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0290 - mape: 9401.6436\n",
      "Epoch 00011: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0303 - mae: 0.0290 - mape: 9372.2676 - val_loss: 0.0601 - val_mae: 0.0589 - val_mape: 8.9566\n",
      "Epoch 12/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0292 - mae: 0.0280 - mape: 9176.0518\n",
      "Epoch 00012: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0292 - mae: 0.0280 - mape: 9147.3730 - val_loss: 0.0464 - val_mae: 0.0452 - val_mape: 7.1787\n",
      "Epoch 13/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0291 - mae: 0.0279 - mape: 9234.7246\n",
      "Epoch 00013: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0291 - mae: 0.0279 - mape: 9214.9170 - val_loss: 0.0398 - val_mae: 0.0387 - val_mape: 6.3188\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.0278 - mape: 9259.1611\n",
      "Epoch 00014: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0290 - mae: 0.0278 - mape: 9221.1396 - val_loss: 0.0613 - val_mae: 0.0602 - val_mape: 8.9035\n",
      "Epoch 15/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0285 - mae: 0.0274 - mape: 9332.2979\n",
      "Epoch 00015: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0285 - mae: 0.0274 - mape: 9293.9756 - val_loss: 0.0448 - val_mae: 0.0438 - val_mape: 6.8179\n",
      "Epoch 16/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9270.8516- ETA: 0s - loss: 0.0278 - mae: 0.0268 - ma\n",
      "Epoch 00016: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9269.1475 - val_loss: 0.0455 - val_mae: 0.0446 - val_mape: 7.0034\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9212.3809\n",
      "Epoch 00017: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9210.6865 - val_loss: 0.0670 - val_mae: 0.0661 - val_mape: 9.5463\n",
      "Epoch 18/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9337.9092\n",
      "Epoch 00018: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9308.7227 - val_loss: 0.0456 - val_mae: 0.0447 - val_mape: 7.0638\n",
      "Epoch 19/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9224.4463\n",
      "Epoch 00019: val_loss improved from 0.03429 to 0.03249, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0019-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9213.7041 - val_loss: 0.0325 - val_mae: 0.0316 - val_mape: 5.3572\n",
      "Epoch 20/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9302.4258\n",
      "Epoch 00020: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9264.2266 - val_loss: 0.0382 - val_mae: 0.0373 - val_mape: 6.0539\n",
      "Epoch 21/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9244.2197\n",
      "Epoch 00021: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 436us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9206.2666 - val_loss: 0.0409 - val_mae: 0.0400 - val_mape: 6.2748\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0260 - mape: 9348.6699\n",
      "Epoch 00022: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0269 - mae: 0.0260 - mape: 9337.7842 - val_loss: 0.0328 - val_mae: 0.0319 - val_mape: 5.4575\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9254.0684\n",
      "Epoch 00023: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0266 - mae: 0.0257 - mape: 9252.3672 - val_loss: 0.0552 - val_mae: 0.0543 - val_mape: 8.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9298.8604\n",
      "Epoch 00024: val_loss improved from 0.03249 to 0.03114, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0024-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0265 - mae: 0.0257 - mape: 9260.6738 - val_loss: 0.0311 - val_mae: 0.0303 - val_mape: 5.1846\n",
      "Epoch 25/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0254 - mape: 9225.4922\n",
      "Epoch 00025: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0262 - mae: 0.0254 - mape: 9223.7969 - val_loss: 0.0519 - val_mae: 0.0511 - val_mape: 7.9640\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0254 - mape: 9168.1641\n",
      "Epoch 00026: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0263 - mae: 0.0254 - mape: 9139.5078 - val_loss: 0.0324 - val_mae: 0.0315 - val_mape: 5.4029\n",
      "Epoch 27/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0252 - mape: 9292.8506\n",
      "Epoch 00027: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 435us/sample - loss: 0.0261 - mae: 0.0252 - mape: 9282.0293 - val_loss: 0.0368 - val_mae: 0.0359 - val_mape: 5.8316\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0250 - mape: 9257.2148\n",
      "Epoch 00028: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0259 - mae: 0.0250 - mape: 9246.4346 - val_loss: 0.0322 - val_mae: 0.0313 - val_mape: 5.3347\n",
      "Epoch 29/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0249 - mape: 9201.8809\n",
      "Epoch 00029: val_loss improved from 0.03114 to 0.03017, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0257 - mae: 0.0249 - mape: 9173.1152 - val_loss: 0.0302 - val_mae: 0.0293 - val_mape: 5.0982\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0246 - mape: 9224.1299\n",
      "Epoch 00030: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0255 - mae: 0.0246 - mape: 9195.2969 - val_loss: 0.0316 - val_mae: 0.0308 - val_mape: 5.3475\n",
      "Epoch 31/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0247 - mape: 9221.6504\n",
      "Epoch 00031: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0256 - mae: 0.0247 - mape: 9219.9551 - val_loss: 0.0383 - val_mae: 0.0375 - val_mape: 6.0447\n",
      "Epoch 32/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9185.3818\n",
      "Epoch 00032: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0253 - mae: 0.0244 - mape: 9156.6689 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.6061\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9151.5020\n",
      "Epoch 00033: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0252 - mae: 0.0243 - mape: 9104.9512 - val_loss: 0.0435 - val_mae: 0.0426 - val_mape: 6.6821\n",
      "Epoch 34/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0240 - mape: 9209.8779\n",
      "Epoch 00034: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0248 - mae: 0.0240 - mape: 9163.0439 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 5.9090\n",
      "Epoch 35/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0243 - mape: 9190.4863\n",
      "Epoch 00035: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0251 - mae: 0.0243 - mape: 9170.7715 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.4763\n",
      "Epoch 36/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0241 - mape: 9201.8359\n",
      "Epoch 00036: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0250 - mae: 0.0241 - mape: 9182.0967 - val_loss: 0.0393 - val_mae: 0.0384 - val_mape: 6.1283\n",
      "Epoch 37/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0247 - mae: 0.0238 - mape: 9064.8945\n",
      "Epoch 00037: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0247 - mae: 0.0238 - mape: 9063.2285 - val_loss: 0.0322 - val_mae: 0.0314 - val_mape: 5.4603\n",
      "Epoch 38/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0236 - mape: 9153.4014\n",
      "Epoch 00038: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0245 - mae: 0.0236 - mape: 9142.7412 - val_loss: 0.0396 - val_mae: 0.0387 - val_mape: 6.1510\n",
      "Epoch 39/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9212.5186\n",
      "Epoch 00039: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9183.7207 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5752\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0237 - mape: 9298.7363\n",
      "Epoch 00040: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0246 - mae: 0.0237 - mape: 9278.7891 - val_loss: 0.0358 - val_mae: 0.0349 - val_mape: 5.7883\n",
      "Epoch 41/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9290.7383\n",
      "Epoch 00041: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9252.5859 - val_loss: 0.0359 - val_mae: 0.0350 - val_mape: 6.1253\n",
      "Epoch 42/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0244 - mae: 0.0235 - mape: 9100.7422\n",
      "Epoch 00042: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0244 - mae: 0.0235 - mape: 9099.0762 - val_loss: 0.0365 - val_mae: 0.0356 - val_mape: 5.8232\n",
      "Epoch 43/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.0231 - mape: 9206.7373\n",
      "Epoch 00043: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0240 - mae: 0.0231 - mape: 9186.9854 - val_loss: 0.0428 - val_mae: 0.0419 - val_mape: 6.5949\n",
      "Epoch 44/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.0229 - mape: 9111.2773\n",
      "Epoch 00044: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0238 - mae: 0.0229 - mape: 9109.6025 - val_loss: 0.0335 - val_mae: 0.0326 - val_mape: 5.5624\n",
      "Epoch 45/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0229 - mape: 9251.8613\n",
      "Epoch 00045: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0229 - mape: 9250.1602 - val_loss: 0.0383 - val_mae: 0.0374 - val_mape: 6.0213\n",
      "Epoch 46/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0228 - mape: 9249.3652\n",
      "Epoch 00046: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9229.5244 - val_loss: 0.0470 - val_mae: 0.0461 - val_mape: 7.1372\n",
      "Epoch 47/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0228 - mape: 9240.2764\n",
      "Epoch 00047: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9220.4561 - val_loss: 0.0355 - val_mae: 0.0346 - val_mape: 5.8071\n",
      "Epoch 48/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0227 - mape: 9228.7842\n",
      "Epoch 00048: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0236 - mae: 0.0227 - mape: 9227.0869 - val_loss: 0.0335 - val_mae: 0.0327 - val_mape: 5.7051\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0225 - mape: 9308.8867\n",
      "Epoch 00049: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0234 - mae: 0.0225 - mape: 9279.7959 - val_loss: 0.0391 - val_mae: 0.0382 - val_mape: 6.2820\n",
      "Epoch 50/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0223 - mape: 8910.0205\n",
      "Epoch 00050: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0232 - mae: 0.0223 - mape: 9253.5254 - val_loss: 0.0347 - val_mae: 0.0338 - val_mape: 5.7545\n",
      "Epoch 51/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0224 - mape: 9279.0811\n",
      "Epoch 00051: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0233 - mae: 0.0224 - mape: 9277.3750 - val_loss: 0.0390 - val_mae: 0.0381 - val_mape: 6.1803\n",
      "Epoch 52/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0222 - mape: 9221.8008\n",
      "Epoch 00052: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0231 - mae: 0.0222 - mape: 9202.0244 - val_loss: 0.0355 - val_mae: 0.0347 - val_mape: 5.8992\n",
      "Epoch 53/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0218 - mape: 9313.6279\n",
      "Epoch 00053: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0227 - mae: 0.0219 - mape: 9284.5176 - val_loss: 0.0346 - val_mae: 0.0337 - val_mape: 5.8458\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0221 - mape: 9235.9512\n",
      "Epoch 00054: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0230 - mae: 0.0221 - mape: 9207.0791 - val_loss: 0.0399 - val_mae: 0.0391 - val_mape: 6.3575\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.0219 - mape: 9211.0430\n",
      "Epoch 00055: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0228 - mae: 0.0219 - mape: 9191.2910 - val_loss: 0.0397 - val_mae: 0.0388 - val_mape: 6.2600\n",
      "Epoch 56/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0217 - mape: 9251.5020\n",
      "Epoch 00056: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0226 - mae: 0.0217 - mape: 9240.7275 - val_loss: 0.0405 - val_mae: 0.0397 - val_mape: 6.3565\n",
      "Epoch 57/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 9213.3115\n",
      "Epoch 00057: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9166.4385 - val_loss: 0.0343 - val_mae: 0.0334 - val_mape: 5.7472\n",
      "Epoch 58/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0216 - mape: 9254.9170\n",
      "Epoch 00058: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9253.2207 - val_loss: 0.0376 - val_mae: 0.0367 - val_mape: 6.0893\n",
      "Epoch 59/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0220 - mape: 9354.7930\n",
      "Epoch 00059: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0230 - mae: 0.0220 - mape: 9325.5488 - val_loss: 0.0341 - val_mae: 0.0332 - val_mape: 5.7046\n",
      "Epoch 60/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0214 - mape: 9235.1680\n",
      "Epoch 00060: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0222 - mae: 0.0214 - mape: 9188.1836 - val_loss: 0.0366 - val_mae: 0.0358 - val_mape: 5.9825\n",
      "Epoch 61/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0215 - mape: 9196.2959\n",
      "Epoch 00061: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0223 - mae: 0.0215 - mape: 9167.5479 - val_loss: 0.0399 - val_mae: 0.0390 - val_mape: 6.2796\n",
      "Epoch 62/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0213 - mape: 9107.4619\n",
      "Epoch 00062: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0222 - mae: 0.0213 - mape: 9087.9248 - val_loss: 0.0385 - val_mae: 0.0376 - val_mape: 6.1628\n",
      "Epoch 63/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0213 - mape: 9210.2637\n",
      "Epoch 00063: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0221 - mae: 0.0213 - mape: 9172.4463 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5972\n",
      "Epoch 64/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9124.6074\n",
      "Epoch 00064: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9122.9297 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 6.0281\n",
      "Epoch 65/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0212 - mape: 9226.6611\n",
      "Epoch 00065: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0220 - mae: 0.0212 - mape: 9188.7695 - val_loss: 0.0377 - val_mae: 0.0369 - val_mape: 6.2115\n",
      "Epoch 66/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9052.7314\n",
      "Epoch 00066: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9051.0664 - val_loss: 0.0405 - val_mae: 0.0396 - val_mape: 6.4286\n",
      "Epoch 67/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0209 - mape: 9083.4209\n",
      "Epoch 00067: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0218 - mae: 0.0209 - mape: 9072.8418 - val_loss: 0.0365 - val_mae: 0.0357 - val_mape: 6.0204\n",
      "Epoch 68/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9076.5205\n",
      "Epoch 00068: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9039.2510 - val_loss: 0.0342 - val_mae: 0.0334 - val_mape: 5.6559\n",
      "Epoch 69/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0208 - mape: 9033.5391\n",
      "Epoch 00069: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0217 - mae: 0.0208 - mape: 9005.3018 - val_loss: 0.0396 - val_mae: 0.0388 - val_mape: 6.4502\n",
      "Epoch 70/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9120.9199\n",
      "Epoch 00070: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9083.4629 - val_loss: 0.0361 - val_mae: 0.0353 - val_mape: 5.8008\n",
      "Epoch 71/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9097.3145\n",
      "Epoch 00071: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9095.6416 - val_loss: 0.0361 - val_mae: 0.0352 - val_mape: 5.9642\n",
      "Epoch 72/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0206 - mape: 9088.2021\n",
      "Epoch 00072: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0206 - mape: 9086.5312 - val_loss: 0.0380 - val_mae: 0.0372 - val_mape: 6.1542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9105.6396\n",
      "Epoch 00073: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9059.3135 - val_loss: 0.0434 - val_mae: 0.0426 - val_mape: 6.7060\n",
      "Epoch 74/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0205 - mape: 9066.7041\n",
      "Epoch 00074: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0205 - mape: 9065.0361 - val_loss: 0.0346 - val_mae: 0.0338 - val_mape: 5.7416\n",
      "Epoch 75/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0205 - mape: 8978.4404\n",
      "Epoch 00075: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0213 - mae: 0.0205 - mape: 8967.9941 - val_loss: 0.0348 - val_mae: 0.0340 - val_mape: 5.9216\n",
      "Epoch 76/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9057.9648\n",
      "Epoch 00076: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0210 - mae: 0.0202 - mape: 9056.2988 - val_loss: 0.0398 - val_mae: 0.0390 - val_mape: 6.3300\n",
      "Epoch 77/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0203 - mape: 8981.2236\n",
      "Epoch 00077: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0212 - mae: 0.0203 - mape: 8979.5723 - val_loss: 0.0451 - val_mae: 0.0444 - val_mape: 7.0400\n",
      "Epoch 78/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9038.3955\n",
      "Epoch 00078: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0210 - mae: 0.0202 - mape: 8992.4121 - val_loss: 0.0360 - val_mae: 0.0353 - val_mape: 5.9399\n",
      "Epoch 79/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0201 - mape: 8948.2764\n",
      "Epoch 00079: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0209 - mae: 0.0201 - mape: 8920.3037 - val_loss: 0.0350 - val_mae: 0.0342 - val_mape: 5.7982\n",
      "Epoch 80/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0200 - mape: 8990.1953\n",
      "Epoch 00080: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0208 - mae: 0.0200 - mape: 8962.0898 - val_loss: 0.0358 - val_mae: 0.0350 - val_mape: 5.8451\n"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "cW3NWXtXLDA-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 420.56\n",
      "Median Absolute Error (MedAE): 333.06\n",
      "Mean Squared Error (MSE): 313586.16\n",
      "Root Mean Squared Error (RMSE): 559.99\n",
      "Mean Absolute Percentage Error (MAPE): 4.19 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.25 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5'\n",
    "start_epoch= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0228 - mape: 9138.6191- ETA: 3s - loss: 0.023\n",
      "Epoch 00001: val_loss improved from inf to 0.03148, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 56s 2ms/sample - loss: 0.0235 - mae: 0.0228 - mape: 9136.9395 - val_loss: 0.0315 - val_mae: 0.0307 - val_mape: 5.2145\n",
      "Epoch 2/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0225 - mape: 9164.0752\n",
      "Epoch 00002: val_loss improved from 0.03148 to 0.03063, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0232 - mae: 0.0225 - mape: 9162.3906 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1229\n",
      "Epoch 3/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0224 - mape: 9196.9521- ETA: 3s - loss: 0.0\n",
      "Epoch 00003: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0231 - mae: 0.0224 - mape: 9195.2607 - val_loss: 0.0314 - val_mae: 0.0307 - val_mape: 5.2105\n",
      "Epoch 4/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0223 - mape: 9131.6660\n",
      "Epoch 00004: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0230 - mae: 0.0223 - mape: 9121.0312 - val_loss: 0.0310 - val_mae: 0.0304 - val_mape: 5.1898\n",
      "Epoch 5/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.0223 - mape: 9159.3213\n",
      "Epoch 00005: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0229 - mae: 0.0223 - mape: 9148.6533 - val_loss: 0.0321 - val_mae: 0.0314 - val_mape: 5.2817\n",
      "Epoch 6/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9180.6631\n",
      "Epoch 00006: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0228 - mae: 0.0221 - mape: 9178.9824 - val_loss: 0.0309 - val_mae: 0.0303 - val_mape: 5.1679\n",
      "Epoch 7/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9175.2949\n",
      "Epoch 00007: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0227 - mae: 0.0221 - mape: 9173.6084 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.2847\n",
      "Epoch 8/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0220 - mape: 9205.9355\n",
      "Epoch 00008: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0226 - mae: 0.0220 - mape: 9195.2217 - val_loss: 0.0307 - val_mae: 0.0301 - val_mape: 5.2188\n",
      "Epoch 9/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0219 - mape: 9181.9873\n",
      "Epoch 00009: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0225 - mae: 0.0219 - mape: 9180.2988 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.2595\n",
      "Epoch 10/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0218 - mape: 9186.6875\n",
      "Epoch 00010: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0224 - mae: 0.0218 - mape: 9184.9990 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2146\n",
      "Epoch 11/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0217 - mape: 9203.6816\n",
      "Epoch 00011: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0223 - mae: 0.0217 - mape: 9192.9629 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.2609\n",
      "Epoch 12/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9241.5742\n",
      "Epoch 00012: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 9239.8750 - val_loss: 0.0311 - val_mae: 0.0305 - val_mape: 5.2300\n",
      "Epoch 13/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0216 - mape: 9247.9932\n",
      "Epoch 00013: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0216 - mape: 9237.2227 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4177\n",
      "Epoch 14/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0215 - mape: 9297.9727\n",
      "Epoch 00014: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0215 - mape: 9296.2627 - val_loss: 0.0307 - val_mae: 0.0302 - val_mape: 5.2173\n",
      "Epoch 15/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0214 - mape: 9263.5205\n",
      "Epoch 00015: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0220 - mae: 0.0214 - mape: 9261.8164 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2651\n",
      "Epoch 16/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 9198.6846\n",
      "Epoch 00016: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 9196.9941 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.3174\n",
      "Epoch 17/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 9260.9404\n",
      "Epoch 00017: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 9250.1543 - val_loss: 0.0309 - val_mae: 0.0304 - val_mape: 5.2860\n",
      "Epoch 18/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0212 - mape: 9216.3271\n",
      "Epoch 00018: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0212 - mape: 9214.6318 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3829\n",
      "Epoch 19/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0211 - mape: 9211.0586\n",
      "Epoch 00019: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0211 - mape: 9200.3320 - val_loss: 0.0341 - val_mae: 0.0336 - val_mape: 5.5767\n",
      "Epoch 20/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0209 - mape: 9201.0938\n",
      "Epoch 00020: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0215 - mae: 0.0209 - mape: 9190.3779 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3684\n",
      "Epoch 21/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9219.9883\n",
      "Epoch 00021: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0214 - mae: 0.0208 - mape: 9218.2930 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3618\n",
      "Epoch 22/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 9187.7080\n",
      "Epoch 00022: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 9186.0186 - val_loss: 0.0316 - val_mae: 0.0310 - val_mape: 5.3197\n",
      "Epoch 23/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9196.0430\n",
      "Epoch 00023: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0206 - mape: 9194.3525 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2367\n",
      "Epoch 24/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9184.2910\n",
      "Epoch 00024: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 9173.5947 - val_loss: 0.0315 - val_mae: 0.0309 - val_mape: 5.2920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9166.1699\n",
      "Epoch 00025: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0210 - mae: 0.0205 - mape: 9155.4951 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.3492\n",
      "Epoch 26/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9205.6270\n",
      "Epoch 00026: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9203.9385 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2379\n",
      "Epoch 27/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9117.5986\n",
      "Epoch 00027: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9115.9219 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4364\n",
      "Epoch 28/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9204.3340\n",
      "Epoch 00028: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9193.6143 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2468\n",
      "Epoch 29/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9128.1553\n",
      "Epoch 00029: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9126.4766 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3819\n",
      "Epoch 30/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0202 - mape: 9126.3975\n",
      "Epoch 00030: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0207 - mae: 0.0202 - mape: 9124.7197 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.3127\n",
      "Epoch 31/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9127.3574\n",
      "Epoch 00031: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9125.6787 - val_loss: 0.0314 - val_mae: 0.0308 - val_mape: 5.3096\n",
      "Epoch 32/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9139.1289\n",
      "Epoch 00032: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9137.4482 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3143\n",
      "Epoch 33/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0200 - mape: 9129.0029\n",
      "Epoch 00033: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0205 - mae: 0.0200 - mape: 9127.3242 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3023\n",
      "Epoch 34/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9085.1152\n",
      "Epoch 00034: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9083.4443 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3421\n",
      "Epoch 35/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9161.0391\n",
      "Epoch 00035: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9159.3545 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2771\n",
      "Epoch 36/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0198 - mape: 9147.9355\n",
      "Epoch 00036: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0203 - mae: 0.0198 - mape: 9137.2803 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3513\n",
      "Epoch 37/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9170.7051- ETA: 4s - loss: 0. - ETA: 2s - loss: 0.0203\n",
      "Epoch 00037: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9169.0186 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2897\n",
      "Epoch 38/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9186.7959\n",
      "Epoch 00038: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9176.0967 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2613\n",
      "Epoch 39/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0195 - mape: 9121.9736\n",
      "Epoch 00039: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0200 - mae: 0.0195 - mape: 9120.2959 - val_loss: 0.0348 - val_mae: 0.0343 - val_mape: 5.6403\n",
      "Epoch 40/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0194 - mape: 9143.8574\n",
      "Epoch 00040: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0199 - mae: 0.0194 - mape: 9142.1768 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3696\n",
      "Epoch 41/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0192 - mape: 9166.4883\n",
      "Epoch 00041: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0197 - mae: 0.0192 - mape: 9155.8125 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3686\n",
      "Epoch 42/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9157.3496\n",
      "Epoch 00042: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9146.6846 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3299\n",
      "Epoch 43/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9169.7188\n",
      "Epoch 00043: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9159.0391 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2889\n",
      "Epoch 44/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0190 - mape: 9109.2627\n",
      "Epoch 00044: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0195 - mae: 0.0190 - mape: 9107.5879 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3664\n",
      "Epoch 45/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0189 - mape: 9101.7988\n",
      "Epoch 00045: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0195 - mae: 0.0189 - mape: 9100.1250 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4306\n",
      "Epoch 46/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0194 - mae: 0.0189 - mape: 9152.3506\n",
      "Epoch 00046: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0194 - mae: 0.0189 - mape: 9150.6680 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.3450\n",
      "Epoch 47/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0193 - mae: 0.0187 - mape: 9092.9639\n",
      "Epoch 00047: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0193 - mae: 0.0187 - mape: 9091.2920 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3590\n",
      "Epoch 48/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.0186 - mape: 9166.8369\n",
      "Epoch 00048: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0191 - mae: 0.0186 - mape: 9165.1514 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4518\n",
      "Epoch 49/200\n",
      "11776/32614 [=========>....................] - ETA: 32s - loss: 0.0191 - mae: 0.0186 - mape: 7337.6323"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-048f600f12b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 412.44\n",
      "Median Absolute Error (MedAE): 328.08\n",
      "Mean Squared Error (MSE): 301264.17\n",
      "Root Mean Squared Error (RMSE): 548.88\n",
      "Mean Absolute Percentage Error (MAPE): 4.12 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.19 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5'\n",
    "start_epoch= 30+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0218 - mape: 9021.4141\n",
      "Epoch 00001: val_loss improved from inf to 0.03075, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 7s 222us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9155.7246 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1276\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9392.5967\n",
      "Epoch 00002: val_loss improved from 0.03075 to 0.03067, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9142.1660 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1187\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 8595.9766\n",
      "Epoch 00003: val_loss improved from 0.03067 to 0.03054, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9143.0762 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1073\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9385.6611\n",
      "Epoch 00004: val_loss improved from 0.03054 to 0.03050, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9135.4072 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1036\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9377.5986\n",
      "Epoch 00005: val_loss improved from 0.03050 to 0.03046, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9127.5752 - val_loss: 0.0305 - val_mae: 0.0297 - val_mape: 5.0995\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8606.6309\n",
      "Epoch 00006: val_loss improved from 0.03046 to 0.03042, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9137.0928 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0965\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9375.1191\n",
      "Epoch 00007: val_loss improved from 0.03042 to 0.03041, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9125.1416 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9374.5898\n",
      "Epoch 00008: val_loss improved from 0.03041 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9124.6318 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0924\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8210.0459\n",
      "Epoch 00009: val_loss improved from 0.03038 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9126.5254 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0921\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8572.6328  ETA: 1s - loss: 0.0225 - mae: 0.0218 \n",
      "Epoch 00010: val_loss improved from 0.03038 to 0.03035, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9118.1846 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0894\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8207.2930\n",
      "Epoch 00011: val_loss did not improve from 0.03035\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9111.9365 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0895\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9357.2139\n",
      "Epoch 00012: val_loss improved from 0.03035 to 0.03034, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0012-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.7373 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0893\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8967.2090\n",
      "Epoch 00013: val_loss improved from 0.03034 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0859 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0887\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8584.7832\n",
      "Epoch 00014: val_loss improved from 0.03033 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0014-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9115.9170 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9359.8037\n",
      "Epoch 00015: val_loss improved from 0.03033 to 0.03030, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0015-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.2441 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0861\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8970.0029\n",
      "Epoch 00016: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9116.9443 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0868\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8182.7358\n",
      "Epoch 00017: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.9473 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0872\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9360.5244\n",
      "Epoch 00018: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.9551 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8964.7793\n",
      "Epoch 00019: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0879 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8972.5371\n",
      "Epoch 00020: val_loss improved from 0.03030 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0020-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9106.9902 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0856\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8572.1016\n",
      "Epoch 00021: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9105.2520 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0876\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9354.3223\n",
      "Epoch 00022: val_loss improved from 0.03028 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.9277 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0858\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9355.4004- ETA: 1s - loss: 0.0223 - mae: 0.0216 - mape: 9009 - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 869\n",
      "Epoch 00023: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9105.9697 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9358.2070\n",
      "Epoch 00024: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9108.6885 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0860\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8967.9619\n",
      "Epoch 00025: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.4453 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8956.2822\n",
      "Epoch 00026: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.1289 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0879\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9353.0088\n",
      "Epoch 00027: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9103.6240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.5430\n",
      "Epoch 00028: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1504 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0871\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9354.4570\n",
      "Epoch 00029: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.0547 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0886\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.4277\n",
      "Epoch 00030: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.0498 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2637\n",
      "Epoch 00031: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0391 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0890\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.9912\n",
      "Epoch 00032: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.1416 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0896\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8960.5801\n",
      "Epoch 00033: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7783 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0913\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9346.8809\n",
      "Epoch 00034: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9097.6689 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8965.5186\n",
      "Epoch 00035: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8569.2568\n",
      "Epoch 00036: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1602 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0915\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.2012  ETA: 2s - loss: 0.02\n",
      "Epoch 00037: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1436 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0920\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8574.7637\n",
      "Epoch 00038: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.4717 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0902\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.5205\n",
      "Epoch 00039: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.2197 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.8828\n",
      "Epoch 00040: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.5615 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.7637\n",
      "Epoch 00041: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.3633 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0908\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9359.6875\n",
      "Epoch 00042: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0914\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.9414\n",
      "Epoch 00043: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9099.0850 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3818\n",
      "Epoch 00044: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9072 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0957\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2998\n",
      "Epoch 00045: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0811 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8428- ETA: 2s - loss: 0.0223 - mae: 0\n",
      "Epoch 00046: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9570 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0959\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.5625\n",
      "Epoch 00047: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.1621 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0927\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8184\n",
      "Epoch 00048: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.2930 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0955\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8958.4678- ETA: 2s - loss: 0.0224 - mae: 0.0217 - mape: 4847. - ETA: 1s - loss: 0.0222 - mae: 0.0215 \n",
      "Epoch 00049: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.8057 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0937\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8977.0430\n",
      "Epoch 00050: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.9639 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0950\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.6865\n",
      "Epoch 00051: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.2705 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0954\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8964.1064\n",
      "Epoch 00052: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.1123 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0919\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.2607\n",
      "Epoch 00053: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.9170 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0989\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8959.4570\n",
      "Epoch 00054: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.2852 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0969\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.4199- ETA: 2s - loss: 0.0225 - mae: 0\n",
      "Epoch 00055: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.2910 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0990\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6221\n",
      "Epoch 00056: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1025 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0925\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.5264\n",
      "Epoch 00057: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8096 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1009\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8963.5762\n",
      "Epoch 00058: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.3652 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1010\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8584.6045\n",
      "Epoch 00059: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9108.2227 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0963\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.8242\n",
      "Epoch 00060: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.3945 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0996\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3730  ETA: 2s - loss: 0.0\n",
      "Epoch 00061: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8975 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1003\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.2998\n",
      "Epoch 00062: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9113.6562 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0987\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8572.1582\n",
      "Epoch 00063: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.7090 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1015\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6914\n",
      "Epoch 00064: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1013\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.9131\n",
      "Epoch 00065: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.5430 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1008\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9361.4287\n",
      "Epoch 00066: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9111.8457 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1032\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8968.2002\n",
      "Epoch 00067: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.6504 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1028\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.6250\n",
      "Epoch 00068: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7305 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8962.6729- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8456.346\n",
      "Epoch 00069: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9107.0010 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1072\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.2842\n",
      "Epoch 00070: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.8740 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1061\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.1514\n",
      "Epoch 00071: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9108.2100 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1067\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9350.1494\n",
      "Epoch 00072: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9100.8525 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.5088\n",
      "Epoch 00073: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9105.0918 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1070\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8561.8984\n",
      "Epoch 00074: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1073\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.5430\n",
      "Epoch 00075: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.3232 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1101\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.6846\n",
      "Epoch 00076: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.0889 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1018\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.6553\n",
      "Epoch 00077: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.3877 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1131\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.4307\n",
      "Epoch 00078: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.1025 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1066\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8972.5918\n",
      "Epoch 00079: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7490 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.0195\n",
      "Epoch 00080: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.7041 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1059\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.1738\n",
      "Epoch 00081: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.8281 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1086\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9359.2744\n",
      "Epoch 00082: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7461 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1125\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9355.9951\n",
      "Epoch 00083: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.5498 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1091\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.0264\n",
      "Epoch 00084: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9111.4297 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8973.9590\n",
      "Epoch 00085: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.7012 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1117\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8980.7080\n",
      "Epoch 00086: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.9180 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1181\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9362.3105\n",
      "Epoch 00087: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.6982 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1146\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.5547\n",
      "Epoch 00088: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1924 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.5771\n",
      "Epoch 00089: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.1777 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1158\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.7559\n",
      "Epoch 00090: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.4727 - val_loss: 0.0304 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9344.5049\n",
      "Epoch 00091: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9095.3486 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1201\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.3213\n",
      "Epoch 00092: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.6084 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1109\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.4619\n",
      "Epoch 00093: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1074 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9357.2207\n",
      "Epoch 00094: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.7432 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1149\n",
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.8730\n",
      "Epoch 00095: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.5840 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9366.0117\n",
      "Epoch 00096: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9116.2979 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1090\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 7807.1338- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape:\n",
      "Epoch 00097: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9097.1768 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8969.1562\n",
      "Epoch 00098: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9108.1855 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1151\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9350.9482  ETA\n",
      "Epoch 00099: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.6289 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1170\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.7568\n",
      "Epoch 00100: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.3867 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1185\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.9570\n",
      "Epoch 00101: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.5547 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.7695- ETA: 1s - loss: 0.0222 - mae: 0.0215 - ma\n",
      "Epoch 00102: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.2959 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1222\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9360.5537\n",
      "Epoch 00103: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.9805 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1187\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8967.6729\n",
      "Epoch 00104: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1299 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1163\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.5879\n",
      "Epoch 00105: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.3262 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1221\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.9297\n",
      "Epoch 00106: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9114.2559 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1136\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8575.9648\n",
      "Epoch 00107: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.5449 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1218\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.1768\n",
      "Epoch 00108: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9098.9189 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1236\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.6055\n",
      "Epoch 00109: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.2510 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1214\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0214 - mape: 9352.8691\n",
      "Epoch 00110: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0221 - mae: 0.0215 - mape: 9103.5098 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1184\n",
      "Epoch 111/200\n",
      "23552/32614 [====================>.........] - ETA: 0s - loss: 0.0220 - mae: 0.0213 - mape: 9332.0186"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-aa18ff9589c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 411.01\n",
      "Median Absolute Error (MedAE): 326.15\n",
      "Mean Squared Error (MSE): 300114.5\n",
      "Root Mean Squared Error (RMSE): 547.83\n",
      "Mean Absolute Percentage Error (MAPE): 4.11 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.17 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-0022-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
