{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tWP4R-BZmaZi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\pyimagesearch')\n",
    "from timeseires.CNN1D.GoogLeNet1D import PC\n",
    "from timeseires.utils.to_split import to_split\n",
    "#from timeseires.callbacks.EpochCheckpoint import EpochCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeseires.callbacks.TrainingMonitor import TrainingMonitor\n",
    "from timeseires.utils.load_only_multi_step import load_only_multi_step\n",
    "from timeseires.utils.multivariate_multi_step import multivariate_multi_step\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD  \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,MaxPooling1D,Concatenate,AveragePooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "import pandas as pd\n",
    "import time, pickle\n",
    "import pydot\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, Reshape, Dense, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten, LeakyReLU, concatenate\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12130, 21), (24259, 21))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tr = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_train.csv'\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values \n",
    "\n",
    "path_v = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_validation.csv'\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "\n",
    "path_te = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_test.csv'\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "#.......................................................................................................\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_scaler.pkl\", 'rb'))\n",
    "df_te.shape,df_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nmYgpwTPnsf4"
   },
   "outputs": [],
   "source": [
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIYjXnV81ZHC",
    "outputId": "4520a0c1-81f1-4cf6-9b17-d15f5ae9c7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.36436915397644043 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X, train_y = multivariate_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24,ahead=0)\n",
    "validation_X, validation_y = multivariate_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24,ahead=0)\n",
    "test_X, test_y = multivariate_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24,ahead=0)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.2350938320159912 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X_load, train_y_load = load_only_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24)\n",
    "validation_X_load, validation_y_load = load_only_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24)\n",
    "test_X_load, test_y_load = load_only_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PbWIXWwSJN3k"
   },
   "outputs": [],
   "source": [
    "#EXP 1 STEPS 24 LOOKBACK 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_goog():   \n",
    "    load_one_trig = Input(shape=(time_steps, num_features))\n",
    "    #--------------------------------------------------------\n",
    "    inputs_load = Input(shape=(time_steps, 1))\n",
    "    \n",
    "    c11 = PC.inception_module(inputs_load, 32, 32, 64, 8, 32, \"1a\", 0.005)\n",
    "    c12 = Concatenate(axis=-1)([inputs_load, c11])\n",
    "    \n",
    "    c21 = PC.inception_module(c12, 64, 64, 128, 16, 64, \"2a\", 0.005)\n",
    "    c22 = Concatenate(axis=-1)([ c11, c21])\n",
    "    \n",
    "    c31 = PC.inception_module(c22, 64, 64, 128, 16, 64, \"3a\", 0.005)\n",
    "    #---------------------------------------------------------\n",
    " #   x = Concatenate(axis=-1)([inputs_load, l, c21, c31])\n",
    " #   x = Flatten()(x)\n",
    "    #--------------------------------------------------------\n",
    "    calender_load_time_spatial_features = Concatenate(axis=-1)([load_one_trig, c22, c31])\n",
    "    l = Bidirectional(LSTM(16, return_sequences=True))(calender_load_time_spatial_features)\n",
    "    l = Bidirectional(LSTM(16))(l)\n",
    "    #-------------------------------------------------------------------------\n",
    "    lstm1   = LSTM(20, return_sequences = True)(calender_load_time_spatial_features)\n",
    "    lstm2   = LSTM(20)(lstm1)\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "    #y= PC.conv_module(y, 128, 3, 1)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([l, lstm2])\n",
    "    #x = Flatten()(x)\n",
    "    x = Dense(64, activation=\"selu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(24, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inputs_load, load_one_trig], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = review_goog()\n",
    "tensorflow.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 24, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_conv (Conv1D)        (None, 24, 32)       64          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_conv (Conv1D)         (None, 24, 8)        16          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_bn (BatchNormalizati (None, 24, 32)       128         1a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_bn (BatchNormalizatio (None, 24, 8)        32          1a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_act (Activation)     (None, 24, 32)       0           1a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_act (Activation)      (None, 24, 8)        0           1a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_conv (Conv1D)          (None, 24, 32)       64          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_conv (Conv1D)        (None, 24, 64)       6208        1a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_conv (Conv1D)         (None, 24, 32)       1312        1a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_bn (BatchNormalization (None, 24, 32)       128         1a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_bn (BatchNormalizati (None, 24, 64)       256         1a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_bn (BatchNormalizatio (None, 24, 32)       128         1a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_act (Activation)       (None, 24, 32)       0           1a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_act (Activation)     (None, 24, 64)       0           1a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_act (Activation)      (None, 24, 32)       0           1a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_mixed (Concatenate)          (None, 24, 128)      0           1a_first_act[0][0]               \n",
      "                                                                 1a_second2_act[0][0]             \n",
      "                                                                 1a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 24, 129)      0           input_10[0][0]                   \n",
      "                                                                 1a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_conv (Conv1D)        (None, 24, 64)       8320        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_conv (Conv1D)         (None, 24, 16)       2080        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_bn (BatchNormalizati (None, 24, 64)       256         2a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_bn (BatchNormalizatio (None, 24, 16)       64          2a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_act (Activation)     (None, 24, 64)       0           2a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_act (Activation)      (None, 24, 16)       0           2a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_conv (Conv1D)          (None, 24, 64)       8320        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_conv (Conv1D)        (None, 24, 128)      24704       2a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_conv (Conv1D)         (None, 24, 64)       5184        2a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_bn (BatchNormalization (None, 24, 64)       256         2a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_bn (BatchNormalizati (None, 24, 128)      512         2a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_bn (BatchNormalizatio (None, 24, 64)       256         2a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_act (Activation)       (None, 24, 64)       0           2a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_act (Activation)     (None, 24, 128)      0           2a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_act (Activation)      (None, 24, 64)       0           2a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_mixed (Concatenate)          (None, 24, 256)      0           2a_first_act[0][0]               \n",
      "                                                                 2a_second2_act[0][0]             \n",
      "                                                                 2a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 24, 384)      0           1a_mixed[0][0]                   \n",
      "                                                                 2a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_conv (Conv1D)        (None, 24, 64)       24640       concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_conv (Conv1D)         (None, 24, 16)       6160        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_bn (BatchNormalizati (None, 24, 64)       256         3a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_bn (BatchNormalizatio (None, 24, 16)       64          3a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_act (Activation)     (None, 24, 64)       0           3a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_act (Activation)      (None, 24, 16)       0           3a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_conv (Conv1D)          (None, 24, 64)       24640       concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_conv (Conv1D)        (None, 24, 128)      24704       3a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_conv (Conv1D)         (None, 24, 64)       5184        3a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_bn (BatchNormalization (None, 24, 64)       256         3a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_bn (BatchNormalizati (None, 24, 128)      512         3a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_bn (BatchNormalizatio (None, 24, 64)       256         3a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_act (Activation)       (None, 24, 64)       0           3a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_act (Activation)     (None, 24, 128)      0           3a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_act (Activation)      (None, 24, 64)       0           3a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 24, 21)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "3a_mixed (Concatenate)          (None, 24, 256)      0           3a_first_act[0][0]               \n",
      "                                                                 3a_second2_act[0][0]             \n",
      "                                                                 3a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 24, 661)      0           input_9[0][0]                    \n",
      "                                                                 concatenate_17[0][0]             \n",
      "                                                                 3a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 24, 32)       86784       concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  (None, 24, 20)       54560       concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 32)           6272        bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, 20)           3280        lstm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 52)           0           bidirectional_9[0][0]            \n",
      "                                                                 lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           3392        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 24)           1560        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 300,808\n",
      "Trainable params: 299,128\n",
      "Non-trainable params: 1,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9rQrz9B0JWvR"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\7\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\7'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "h6NbczAlOpSQ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NkF7jg_XO346"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-rIEZj0PCPF",
    "outputId": "95edda02-84de-4f8a-83a2-b4d53387049d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = review_goog()\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-o0qgceiPFdI",
    "outputId": "ebb2c588-e73f-4182-baad-2f4c28f3db95",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.1209 - mae: 0.0592 - mape: 3293.7417\n",
      "Epoch 00001: val_loss improved from inf to 0.05073, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\7\\E1-cp-0001-loss0.05.h5\n",
      "2652/2652 [==============================] - 92s 35ms/step - loss: 0.1209 - mae: 0.0592 - mape: 3292.7007 - val_loss: 0.0507 - val_mae: 0.0494 - val_mape: 25.4498\n",
      "Epoch 2/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0495 - mae: 0.0487 - mape: 2787.3462\n",
      "Epoch 00002: val_loss did not improve from 0.05073\n",
      "2652/2652 [==============================] - 84s 32ms/step - loss: 0.0495 - mae: 0.0487 - mape: 2786.4634 - val_loss: 0.0535 - val_mae: 0.0530 - val_mape: 28.4114\n",
      "Epoch 3/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0468 - mae: 0.0463 - mape: 2509.5491\n",
      "Epoch 00003: val_loss improved from 0.05073 to 0.04040, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\7\\E1-cp-0003-loss0.04.h5\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0468 - mae: 0.0463 - mape: 2508.7551 - val_loss: 0.0404 - val_mae: 0.0399 - val_mape: 20.2776\n",
      "Epoch 4/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0455 - mae: 0.0451 - mape: 2444.3918\n",
      "Epoch 00004: val_loss improved from 0.04040 to 0.03744, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\7\\E1-cp-0004-loss0.04.h5\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0455 - mae: 0.0451 - mape: 2443.6177 - val_loss: 0.0374 - val_mae: 0.0370 - val_mape: 16.9736\n",
      "Epoch 5/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0445 - mae: 0.0441 - mape: 2449.1072\n",
      "Epoch 00005: val_loss did not improve from 0.03744\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0445 - mae: 0.0441 - mape: 2448.3325 - val_loss: 0.0609 - val_mae: 0.0605 - val_mape: 31.6799\n",
      "Epoch 6/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0439 - mae: 0.0435 - mape: 2282.1604\n",
      "Epoch 00006: val_loss did not improve from 0.03744\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0439 - mae: 0.0435 - mape: 2282.1604 - val_loss: 0.0536 - val_mae: 0.0533 - val_mape: 26.5621\n",
      "Epoch 7/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0429 - mae: 0.0426 - mape: 2121.4565\n",
      "Epoch 00007: val_loss did not improve from 0.03744\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0429 - mae: 0.0426 - mape: 2121.4565 - val_loss: 0.0380 - val_mae: 0.0377 - val_mape: 16.7338\n",
      "Epoch 8/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0424 - mae: 0.0421 - mape: 2380.2156- ETA: 0s - loss: 0.0424 - mae: 0.0421 - mape: \n",
      "Epoch 00008: val_loss did not improve from 0.03744\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0424 - mae: 0.0421 - mape: 2380.2156 - val_loss: 0.0532 - val_mae: 0.0530 - val_mape: 30.1960\n",
      "Epoch 9/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0417 - mae: 0.0415 - mape: 2265.8984- ETA: 1s - loss: 0.0418 - mae\n",
      "Epoch 00009: val_loss improved from 0.03744 to 0.03466, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\7\\E1-cp-0009-loss0.03.h5\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0417 - mae: 0.0415 - mape: 2265.1821 - val_loss: 0.0347 - val_mae: 0.0344 - val_mape: 15.6699\n",
      "Epoch 10/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0416 - mae: 0.0413 - mape: 2266.2795- ETA: 1s - loss: 0.0415 - ma\n",
      "Epoch 00010: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0416 - mae: 0.0413 - mape: 2265.5620 - val_loss: 0.0351 - val_mae: 0.0349 - val_mape: 15.8227\n",
      "Epoch 11/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0414 - mae: 0.0411 - mape: 2353.1621- ETA: 4s - loss: 0.04 - ETA: 2s - loss: 0.0414 - mae: 0.0411 - mape: 242 - ETA: 2s - loss: 0.0414\n",
      "Epoch 00011: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0414 - mae: 0.0411 - mape: 2353.1621 - val_loss: 0.0364 - val_mae: 0.0362 - val_mape: 16.0658\n",
      "Epoch 12/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0412 - mae: 0.0410 - mape: 2452.5847\n",
      "Epoch 00012: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0412 - mae: 0.0410 - mape: 2452.5847 - val_loss: 0.0373 - val_mae: 0.0371 - val_mape: 15.4684\n",
      "Epoch 13/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0410 - mae: 0.0408 - mape: 2350.3901-\n",
      "Epoch 00013: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0410 - mae: 0.0408 - mape: 2350.3901 - val_loss: 0.0398 - val_mae: 0.0396 - val_mape: 17.0330\n",
      "Epoch 14/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0408 - mae: 0.0406 - mape: 2247.1240\n",
      "Epoch 00014: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0408 - mae: 0.0406 - mape: 2247.1240 - val_loss: 0.0348 - val_mae: 0.0345 - val_mape: 15.3217\n",
      "Epoch 15/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0406 - mae: 0.0404 - mape: 2317.0784\n",
      "Epoch 00015: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0406 - mae: 0.0404 - mape: 2317.0784 - val_loss: 0.0348 - val_mae: 0.0346 - val_mape: 14.8853\n",
      "Epoch 16/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0404 - mae: 0.0402 - mape: 2257.8574\n",
      "Epoch 00016: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0404 - mae: 0.0402 - mape: 2257.1438 - val_loss: 0.0359 - val_mae: 0.0357 - val_mape: 16.5618\n",
      "Epoch 17/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0404 - mae: 0.0402 - mape: 2181.2979\n",
      "Epoch 00017: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0404 - mae: 0.0402 - mape: 2181.2979 - val_loss: 0.0365 - val_mae: 0.0363 - val_mape: 17.6029\n",
      "Epoch 18/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0403 - mae: 0.0401 - mape: 2323.3506- ETA: 0s - loss: 0.0403 - mae: 0.0401 - ma\n",
      "Epoch 00018: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0403 - mae: 0.0401 - mape: 2322.6157 - val_loss: 0.0357 - val_mae: 0.0355 - val_mape: 16.3607\n",
      "Epoch 19/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0402 - mae: 0.0400 - mape: 2135.8245\n",
      "Epoch 00019: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0402 - mae: 0.0400 - mape: 2135.1487 - val_loss: 0.0356 - val_mae: 0.0354 - val_mape: 15.9635\n",
      "Epoch 20/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0400 - mae: 0.0399 - mape: 2495.7268\n",
      "Epoch 00020: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0401 - mae: 0.0399 - mape: 2494.9373 - val_loss: 0.0444 - val_mae: 0.0442 - val_mape: 23.5631\n",
      "Epoch 21/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0399 - mae: 0.0397 - mape: 2076.0146- ETA: 0s - loss: 0.0399 - mae: 0.0397 - mape: \n",
      "Epoch 00021: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0399 - mae: 0.0397 - mape: 2076.0146 - val_loss: 0.0382 - val_mae: 0.0380 - val_mape: 17.5144\n",
      "Epoch 22/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0400 - mae: 0.0398 - mape: 2184.4265\n",
      "Epoch 00022: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0400 - mae: 0.0398 - mape: 2183.7356 - val_loss: 0.0386 - val_mae: 0.0384 - val_mape: 17.5957\n",
      "Epoch 23/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0398 - mae: 0.0396 - mape: 2132.3271\n",
      "Epoch 00023: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 91s 34ms/step - loss: 0.0398 - mae: 0.0396 - mape: 2132.3271 - val_loss: 0.0420 - val_mae: 0.0418 - val_mape: 19.7737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0398 - mae: 0.0396 - mape: 2108.8103\n",
      "Epoch 00024: val_loss did not improve from 0.03466\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0398 - mae: 0.0396 - mape: 2108.8103 - val_loss: 0.0374 - val_mae: 0.0372 - val_mape: 17.0121\n",
      "Epoch 25/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0397 - mae: 0.0395 - mape: 2281.1799\n",
      "Epoch 00025: val_loss improved from 0.03466 to 0.03402, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\7\\E1-cp-0025-loss0.03.h5\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0397 - mae: 0.0395 - mape: 2280.4585 - val_loss: 0.0340 - val_mae: 0.0338 - val_mape: 14.4873\n",
      "Epoch 26/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0395 - mae: 0.0394 - mape: 2154.2646\n",
      "Epoch 00026: val_loss did not improve from 0.03402\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0395 - mae: 0.0394 - mape: 2154.2646 - val_loss: 0.0345 - val_mae: 0.0343 - val_mape: 14.7423\n",
      "Epoch 27/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0394 - mae: 0.0393 - mape: 2275.5649\n",
      "Epoch 00027: val_loss did not improve from 0.03402\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0394 - mae: 0.0393 - mape: 2274.8459 - val_loss: 0.0386 - val_mae: 0.0384 - val_mape: 19.2940\n",
      "Epoch 28/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0394 - mae: 0.0392 - mape: 2227.6001- ETA: 0s - loss: 0.0394 - mae: 0.0392 - mape: 2190\n",
      "Epoch 00028: val_loss did not improve from 0.03402\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0394 - mae: 0.0392 - mape: 2227.6001 - val_loss: 0.0346 - val_mae: 0.0344 - val_mape: 14.7883\n",
      "Epoch 29/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0394 - mae: 0.0392 - mape: 2163.5415\n",
      "Epoch 00029: val_loss did not improve from 0.03402\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0394 - mae: 0.0392 - mape: 2163.5415 - val_loss: 0.0408 - val_mae: 0.0406 - val_mape: 20.1962\n",
      "Epoch 30/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0393 - mae: 0.0392 - mape: 1987.6700- ETA: 1s - loss: 0.0393 - mae: 0.\n",
      "Epoch 00030: val_loss did not improve from 0.03402\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0393 - mae: 0.0392 - mape: 1987.6700 - val_loss: 0.0374 - val_mae: 0.0373 - val_mape: 18.0691\n",
      "Epoch 31/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0393 - mae: 0.0391 - mape: 2408.6897\n",
      "Epoch 00031: val_loss did not improve from 0.03402\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0393 - mae: 0.0391 - mape: 2407.9270 - val_loss: 0.0391 - val_mae: 0.0389 - val_mape: 18.7264\n",
      "Epoch 32/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0392 - mae: 0.0390 - mape: 2137.6272\n",
      "Epoch 00032: val_loss did not improve from 0.03402\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0391 - mae: 0.0390 - mape: 2136.9507 - val_loss: 0.0414 - val_mae: 0.0413 - val_mape: 18.0043\n",
      "Epoch 33/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0391 - mae: 0.0390 - mape: 2166.9597\n",
      "Epoch 00033: val_loss did not improve from 0.03402\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0391 - mae: 0.0390 - mape: 2166.2747 - val_loss: 0.0376 - val_mae: 0.0375 - val_mape: 16.5948\n",
      "Epoch 34/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0391 - mae: 0.0390 - mape: 2203.3328\n",
      "Epoch 00034: val_loss improved from 0.03402 to 0.03329, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\7\\E1-cp-0034-loss0.03.h5\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0391 - mae: 0.0390 - mape: 2202.6355 - val_loss: 0.0333 - val_mae: 0.0331 - val_mape: 13.8734\n",
      "Epoch 35/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0390 - mae: 0.0389 - mape: 2256.6409- ET\n",
      "Epoch 00035: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0390 - mae: 0.0389 - mape: 2255.9272 - val_loss: 0.0369 - val_mae: 0.0368 - val_mape: 16.5024\n",
      "Epoch 36/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0389 - mae: 0.0388 - mape: 2209.9751- ETA: 1s - loss: 0.0389 - mae\n",
      "Epoch 00036: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0389 - mae: 0.0388 - mape: 2209.9751 - val_loss: 0.0370 - val_mae: 0.0368 - val_mape: 16.0991\n",
      "Epoch 37/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0387 - mape: 2091.8311\n",
      "Epoch 00037: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0389 - mae: 0.0387 - mape: 2091.1699 - val_loss: 0.0385 - val_mae: 0.0384 - val_mape: 17.7461\n",
      "Epoch 38/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0388 - mape: 2120.4707\n",
      "Epoch 00038: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0389 - mae: 0.0388 - mape: 2119.8013 - val_loss: 0.0350 - val_mae: 0.0349 - val_mape: 14.9870\n",
      "Epoch 39/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0387 - mae: 0.0386 - mape: 2126.5425\n",
      "Epoch 00039: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0387 - mae: 0.0386 - mape: 2126.5425 - val_loss: 0.0337 - val_mae: 0.0335 - val_mape: 14.3354\n",
      "Epoch 40/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0388 - mae: 0.0386 - mape: 2215.4661\n",
      "Epoch 00040: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0388 - mae: 0.0386 - mape: 2214.7646 - val_loss: 0.0355 - val_mae: 0.0354 - val_mape: 15.2298\n",
      "Epoch 41/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0387 - mae: 0.0386 - mape: 2148.9529\n",
      "Epoch 00041: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0387 - mae: 0.0386 - mape: 2148.9529 - val_loss: 0.0352 - val_mae: 0.0350 - val_mape: 15.5789\n",
      "Epoch 42/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0386 - mae: 0.0385 - mape: 2028.7993- ETA: 1s - loss: 0.0386 - mae: 0.0385\n",
      "Epoch 00042: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0386 - mae: 0.0385 - mape: 2028.1577 - val_loss: 0.0352 - val_mae: 0.0351 - val_mape: 15.4198\n",
      "Epoch 43/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0386 - mae: 0.0384 - mape: 2088.4995- ETA: - ETA: 2s - l\n",
      "Epoch 00043: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0386 - mae: 0.0384 - mape: 2087.8401 - val_loss: 0.0363 - val_mae: 0.0362 - val_mape: 16.3028\n",
      "Epoch 44/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0386 - mae: 0.0384 - mape: 2068.0310\n",
      "Epoch 00044: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0386 - mae: 0.0384 - mape: 2067.3765 - val_loss: 0.0344 - val_mae: 0.0343 - val_mape: 14.4829\n",
      "Epoch 45/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0385 - mae: 0.0383 - mape: 1997.5513\n",
      "Epoch 00045: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0385 - mae: 0.0383 - mape: 1996.9193 - val_loss: 0.0359 - val_mae: 0.0358 - val_mape: 15.6228\n",
      "Epoch 46/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0385 - mae: 0.0384 - mape: 1976.1398- ETA: \n",
      "Epoch 00046: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 90s 34ms/step - loss: 0.0385 - mae: 0.0384 - mape: 1976.1398 - val_loss: 0.0479 - val_mae: 0.0478 - val_mape: 24.0281\n",
      "Epoch 47/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0384 - mae: 0.0383 - mape: 2126.8030- ETA: 0s - loss: 0.0384 - mae: 0.0383 - mape: 213\n",
      "Epoch 00047: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0384 - mae: 0.0383 - mape: 2126.1301 - val_loss: 0.0346 - val_mae: 0.0345 - val_mape: 15.0121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0383 - mae: 0.0382 - mape: 2183.3909\n",
      "Epoch 00048: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0383 - mae: 0.0382 - mape: 2183.3909 - val_loss: 0.0361 - val_mae: 0.0360 - val_mape: 15.5039\n",
      "Epoch 49/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0383 - mae: 0.0382 - mape: 2061.9888\n",
      "Epoch 00049: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 87s 33ms/step - loss: 0.0383 - mae: 0.0382 - mape: 2061.3364 - val_loss: 0.0406 - val_mae: 0.0405 - val_mape: 19.5337\n",
      "Epoch 50/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0382 - mae: 0.0381 - mape: 2142.6099\n",
      "Epoch 00050: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0382 - mae: 0.0381 - mape: 2141.9324 - val_loss: 0.0343 - val_mae: 0.0342 - val_mape: 15.1730\n",
      "Epoch 51/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0382 - mae: 0.0381 - mape: 2181.7258- ETA: 0s - loss: 0.0382 - mae: 0.0381 - mape\n",
      "Epoch 00051: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 87s 33ms/step - loss: 0.0382 - mae: 0.0381 - mape: 2181.7258 - val_loss: 0.0355 - val_mae: 0.0354 - val_mape: 15.6718\n",
      "Epoch 52/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0381 - mae: 0.0380 - mape: 2307.3247\n",
      "Epoch 00052: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0381 - mae: 0.0380 - mape: 2306.5945 - val_loss: 0.0352 - val_mae: 0.0351 - val_mape: 15.8661\n",
      "Epoch 53/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0381 - mae: 0.0380 - mape: 1966.0404\n",
      "Epoch 00053: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0381 - mae: 0.0380 - mape: 1966.0404 - val_loss: 0.0378 - val_mae: 0.0376 - val_mape: 16.5808\n",
      "Epoch 54/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0381 - mae: 0.0380 - mape: 2114.6431\n",
      "Epoch 00054: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0381 - mae: 0.0380 - mape: 2113.9736 - val_loss: 0.0351 - val_mae: 0.0349 - val_mape: 16.0448\n",
      "Epoch 55/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0380 - mae: 0.0379 - mape: 2251.3120\n",
      "Epoch 00055: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 87s 33ms/step - loss: 0.0380 - mae: 0.0379 - mape: 2250.6003 - val_loss: 0.0359 - val_mae: 0.0358 - val_mape: 15.9724\n",
      "Epoch 56/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0379 - mae: 0.0378 - mape: 2144.2065\n",
      "Epoch 00056: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0379 - mae: 0.0378 - mape: 2144.2065 - val_loss: 0.0354 - val_mae: 0.0353 - val_mape: 16.0699\n",
      "Epoch 57/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0380 - mae: 0.0378 - mape: 2110.1892\n",
      "Epoch 00057: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0380 - mae: 0.0378 - mape: 2109.5215 - val_loss: 0.0354 - val_mae: 0.0352 - val_mape: 15.3116\n",
      "Epoch 58/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0379 - mae: 0.0377 - mape: 2023.8385\n",
      "Epoch 00058: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0379 - mae: 0.0377 - mape: 2023.8385 - val_loss: 0.0394 - val_mae: 0.0392 - val_mape: 18.4539\n",
      "Epoch 59/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0379 - mae: 0.0377 - mape: 1992.1053\n",
      "Epoch 00059: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0379 - mae: 0.0377 - mape: 1992.1053 - val_loss: 0.0371 - val_mae: 0.0370 - val_mape: 16.7257\n",
      "Epoch 60/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0377 - mae: 0.0376 - mape: 1938.1600- ETA: 0s - loss: 0.0377 - mae: 0.0376 - mape: 1943.\n",
      "Epoch 00060: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0377 - mae: 0.0376 - mape: 1938.1600 - val_loss: 0.0355 - val_mae: 0.0354 - val_mape: 15.6662\n",
      "Epoch 61/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.0378 - mape: 2122.7534\n",
      "Epoch 00061: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0379 - mae: 0.0378 - mape: 2122.0815 - val_loss: 0.0371 - val_mae: 0.0370 - val_mape: 16.6999\n",
      "Epoch 62/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0377 - mae: 0.0376 - mape: 2240.3599- ETA: 3\n",
      "Epoch 00062: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0377 - mae: 0.0376 - mape: 2239.6504 - val_loss: 0.0347 - val_mae: 0.0346 - val_mape: 15.4847\n",
      "Epoch 63/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0378 - mae: 0.0377 - mape: 1905.0526\n",
      "Epoch 00063: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0378 - mae: 0.0377 - mape: 1905.0526 - val_loss: 0.0370 - val_mae: 0.0368 - val_mape: 15.4816\n",
      "Epoch 64/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0377 - mae: 0.0375 - mape: 2125.5500\n",
      "Epoch 00064: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0377 - mae: 0.0375 - mape: 2124.8779 - val_loss: 0.0362 - val_mae: 0.0361 - val_mape: 16.1588\n",
      "Epoch 65/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0377 - mae: 0.0376 - mape: 1819.7999\n",
      "Epoch 00065: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0377 - mae: 0.0376 - mape: 1819.7999 - val_loss: 0.0385 - val_mae: 0.0383 - val_mape: 19.3997\n",
      "Epoch 66/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0376 - mae: 0.0375 - mape: 2007.3229\n",
      "Epoch 00066: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0376 - mae: 0.0375 - mape: 2007.3229 - val_loss: 0.0361 - val_mae: 0.0360 - val_mape: 15.8710\n",
      "Epoch 67/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0376 - mae: 0.0375 - mape: 2014.3370\n",
      "Epoch 00067: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0376 - mae: 0.0375 - mape: 2013.6996 - val_loss: 0.0369 - val_mae: 0.0368 - val_mape: 15.8139\n",
      "Epoch 68/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0376 - mae: 0.0375 - mape: 1906.4944\n",
      "Epoch 00068: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0376 - mae: 0.0375 - mape: 1905.8921 - val_loss: 0.0403 - val_mae: 0.0402 - val_mape: 19.5195\n",
      "Epoch 69/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0376 - mae: 0.0374 - mape: 2067.0051\n",
      "Epoch 00069: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0376 - mae: 0.0374 - mape: 2066.3516 - val_loss: 0.0374 - val_mae: 0.0372 - val_mape: 16.6550\n",
      "Epoch 70/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0376 - mae: 0.0374 - mape: 2000.3618\n",
      "Epoch 00070: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0376 - mae: 0.0374 - mape: 2000.3618 - val_loss: 0.0341 - val_mae: 0.0340 - val_mape: 14.3148\n",
      "Epoch 71/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0375 - mae: 0.0374 - mape: 1961.8708\n",
      "Epoch 00071: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0375 - mae: 0.0374 - mape: 1961.8708 - val_loss: 0.0369 - val_mae: 0.0368 - val_mape: 16.3381\n",
      "Epoch 72/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0374 - mae: 0.0373 - mape: 2193.9890\n",
      "Epoch 00072: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 34ms/step - loss: 0.0374 - mae: 0.0373 - mape: 2193.2954 - val_loss: 0.0374 - val_mae: 0.0373 - val_mape: 17.8898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0374 - mae: 0.0372 - mape: 1959.4603\n",
      "Epoch 00073: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 87s 33ms/step - loss: 0.0374 - mae: 0.0372 - mape: 1959.4603 - val_loss: 0.0361 - val_mae: 0.0360 - val_mape: 15.7960\n",
      "Epoch 74/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0374 - mae: 0.0373 - mape: 2051.8662\n",
      "Epoch 00074: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0374 - mae: 0.0373 - mape: 2051.8662 - val_loss: 0.0371 - val_mae: 0.0370 - val_mape: 16.4260\n",
      "Epoch 75/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0373 - mae: 0.0372 - mape: 1786.8723- ETA: 1s - loss: 0.0373 - mae: \n",
      "Epoch 00075: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 87s 33ms/step - loss: 0.0373 - mae: 0.0372 - mape: 1786.3074 - val_loss: 0.0386 - val_mae: 0.0384 - val_mape: 16.2603\n",
      "Epoch 76/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0373 - mae: 0.0372 - mape: 1896.1575\n",
      "Epoch 00076: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0373 - mae: 0.0372 - mape: 1895.5580 - val_loss: 0.0369 - val_mae: 0.0368 - val_mape: 15.8820\n",
      "Epoch 77/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0373 - mae: 0.0372 - mape: 2052.3140\n",
      "Epoch 00077: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 87s 33ms/step - loss: 0.0373 - mae: 0.0372 - mape: 2051.6650 - val_loss: 0.0374 - val_mae: 0.0372 - val_mape: 16.9766\n",
      "Epoch 78/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0373 - mae: 0.0372 - mape: 1931.7056\n",
      "Epoch 00078: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0373 - mae: 0.0372 - mape: 1931.0946 - val_loss: 0.0365 - val_mae: 0.0364 - val_mape: 15.9694\n",
      "Epoch 79/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0372 - mae: 0.0371 - mape: 2021.6907\n",
      "Epoch 00079: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 87s 33ms/step - loss: 0.0372 - mae: 0.0371 - mape: 2021.0508 - val_loss: 0.0364 - val_mae: 0.0362 - val_mape: 15.5314\n",
      "Epoch 80/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0372 - mae: 0.0371 - mape: 1839.9637- ETA: - ETA: 1s - loss: 0.0372 - mae: 0.037\n",
      "Epoch 00080: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0372 - mae: 0.0371 - mape: 1839.9637 - val_loss: 0.0353 - val_mae: 0.0352 - val_mape: 15.3278\n",
      "Epoch 81/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0371 - mae: 0.0370 - mape: 1975.4974\n",
      "Epoch 00081: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0371 - mae: 0.0370 - mape: 1975.4974 - val_loss: 0.0349 - val_mae: 0.0347 - val_mape: 14.4575\n",
      "Epoch 82/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0371 - mae: 0.0370 - mape: 1908.0481- ETA: 2s - \n",
      "Epoch 00082: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0371 - mae: 0.0370 - mape: 1908.0481 - val_loss: 0.0350 - val_mae: 0.0349 - val_mape: 15.3662\n",
      "Epoch 83/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0371 - mae: 0.0370 - mape: 1911.2401\n",
      "Epoch 00083: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 88s 33ms/step - loss: 0.0371 - mae: 0.0370 - mape: 1910.6364 - val_loss: 0.0361 - val_mae: 0.0360 - val_mape: 15.8809\n",
      "Epoch 84/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0371 - mae: 0.0370 - mape: 1996.0092\n",
      "Epoch 00084: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 89s 33ms/step - loss: 0.0371 - mae: 0.0370 - mape: 1996.0092 - val_loss: 0.0372 - val_mae: 0.0371 - val_mape: 16.5547\n",
      "Epoch 85/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0370 - mae: 0.0369 - mape: 1983.3132\n",
      "Epoch 00085: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 87s 33ms/step - loss: 0.0370 - mae: 0.0369 - mape: 1982.6854 - val_loss: 0.0370 - val_mae: 0.0369 - val_mape: 15.9817\n",
      "Epoch 86/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0370 - mae: 0.0369 - mape: 1928.8494\n",
      "Epoch 00086: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 84s 32ms/step - loss: 0.0370 - mae: 0.0369 - mape: 1928.8494 - val_loss: 0.0392 - val_mae: 0.0391 - val_mape: 16.8984\n",
      "Epoch 87/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0368 - mape: 2004.1703- ETA:\n",
      "Epoch 00087: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 84s 32ms/step - loss: 0.0369 - mae: 0.0368 - mape: 2003.5364 - val_loss: 0.0364 - val_mae: 0.0363 - val_mape: 15.9899\n",
      "Epoch 88/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0369 - mae: 0.0368 - mape: 2025.6973- ETA: 1s - loss: 0.0369 - \n",
      "Epoch 00088: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 84s 32ms/step - loss: 0.0369 - mae: 0.0368 - mape: 2025.6973 - val_loss: 0.0366 - val_mae: 0.0365 - val_mape: 15.6421\n",
      "Epoch 89/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0370 - mae: 0.0369 - mape: 1961.5481\n",
      "Epoch 00089: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 84s 32ms/step - loss: 0.0370 - mae: 0.0369 - mape: 1961.5481 - val_loss: 0.0367 - val_mae: 0.0366 - val_mape: 15.7806\n",
      "Epoch 90/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0370 - mae: 0.0368 - mape: 1824.0935\n",
      "Epoch 00090: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 83s 31ms/step - loss: 0.0370 - mae: 0.0368 - mape: 1823.5164 - val_loss: 0.0376 - val_mae: 0.0374 - val_mape: 17.0467\n",
      "Epoch 91/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0369 - mae: 0.0368 - mape: 1923.9591\n",
      "Epoch 00091: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 84s 32ms/step - loss: 0.0369 - mae: 0.0368 - mape: 1923.9591 - val_loss: 0.0350 - val_mae: 0.0349 - val_mape: 14.8072\n",
      "Epoch 92/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0366 - mape: 1759.3525\n",
      "Epoch 00092: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 84s 32ms/step - loss: 0.0368 - mae: 0.0366 - mape: 1758.7966 - val_loss: 0.0363 - val_mae: 0.0362 - val_mape: 15.0381\n",
      "Epoch 93/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0367 - mape: 1925.4429\n",
      "Epoch 00093: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 84s 32ms/step - loss: 0.0368 - mae: 0.0367 - mape: 1924.8334 - val_loss: 0.0369 - val_mae: 0.0368 - val_mape: 15.7097\n",
      "Epoch 94/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0367 - mape: 1958.8442- ETA:\n",
      "Epoch 00094: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 83s 31ms/step - loss: 0.0369 - mae: 0.0367 - mape: 1958.2241 - val_loss: 0.0366 - val_mae: 0.0364 - val_mape: 15.4661\n",
      "Epoch 95/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0367 - mae: 0.0366 - mape: 1884.4227\n",
      "Epoch 00095: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 85s 32ms/step - loss: 0.0367 - mae: 0.0366 - mape: 1883.8270 - val_loss: 0.0356 - val_mae: 0.0355 - val_mape: 15.0872\n",
      "Epoch 96/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0366 - mae: 0.0365 - mape: 2009.2133\n",
      "Epoch 00096: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 83s 31ms/step - loss: 0.0366 - mae: 0.0365 - mape: 2008.5774 - val_loss: 0.0365 - val_mae: 0.0364 - val_mape: 15.5742\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0367 - mae: 0.0365 - mape: 1843.1090\n",
      "Epoch 00097: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 84s 32ms/step - loss: 0.0367 - mae: 0.0365 - mape: 1843.1090 - val_loss: 0.0377 - val_mae: 0.0376 - val_mape: 15.7992\n",
      "Epoch 98/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0367 - mae: 0.0365 - mape: 1976.7788\n",
      "Epoch 00098: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 83s 31ms/step - loss: 0.0367 - mae: 0.0365 - mape: 1976.1532 - val_loss: 0.0399 - val_mae: 0.0398 - val_mape: 19.4742\n",
      "Epoch 99/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0367 - mae: 0.0366 - mape: 1892.5155\n",
      "Epoch 00099: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 84s 32ms/step - loss: 0.0367 - mae: 0.0366 - mape: 1891.9169 - val_loss: 0.0360 - val_mae: 0.0359 - val_mape: 14.9622\n",
      "Epoch 100/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0366 - mae: 0.0365 - mape: 2137.7285\n",
      "Epoch 00100: val_loss did not improve from 0.03329\n",
      "2652/2652 [==============================] - 83s 31ms/step - loss: 0.0366 - mae: 0.0365 - mape: 2137.0520 - val_loss: 0.0364 - val_mae: 0.0363 - val_mape: 15.6301\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oS6zzoUoPL-A",
    "outputId": "ce1a0f3b-6b76-4980-d717-d3542c18a37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 527.11\n",
      "Median Absolute Error (MedAE): 368.94\n",
      "Mean Squared Error (MSE): 540083.69\n",
      "Root Mean Squared Error (RMSE): 734.9\n",
      "Mean Absolute Percentage Error (MAPE): 3.52 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.57 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\7\\\\E1-cp-0034-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7y39sVKwLMm"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AVN1WhyzuWFl"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\7\\\\E2-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\7\\\\E1-cp-0019-loss0.03.h5'\n",
    "start_epoch= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WBeDTA9lu5Qm"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2j6fmkOvA2O",
    "outputId": "7fc1b23b-4d93-4b54-a4df-921c557b9a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\7\\E1-cp-0019-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rTCJW6IzvLU4",
    "outputId": "f810e1ca-98cc-4de7-a4a3-465e83705c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0232 - mean_absolute_error: 0.0231 - mean_absolute_percentage_error: 2030.8346\n",
      "Epoch 00001: val_loss improved from inf to 0.03245, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\7\\E2-cp-0001-loss0.03.h5\n",
      "2652/2652 [==============================] - 97s 37ms/step - loss: 0.0232 - mean_absolute_error: 0.0231 - mean_absolute_percentage_error: 2030.8346 - val_loss: 0.0325 - val_mean_absolute_error: 0.0324 - val_mean_absolute_percentage_error: 14.1288\n",
      "Epoch 2/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0229 - mean_absolute_error: 0.0229 - mean_absolute_percentage_error: 2027.2716\n",
      "Epoch 00002: val_loss improved from 0.03245 to 0.03223, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\7\\E2-cp-0002-loss0.03.h5\n",
      "2652/2652 [==============================] - 97s 36ms/step - loss: 0.0229 - mean_absolute_error: 0.0229 - mean_absolute_percentage_error: 2027.2716 - val_loss: 0.0322 - val_mean_absolute_error: 0.0321 - val_mean_absolute_percentage_error: 13.9957\n",
      "Epoch 3/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0228 - mean_absolute_error: 0.0228 - mean_absolute_percentage_error: 2004.5262\n",
      "Epoch 00003: val_loss improved from 0.03223 to 0.03206, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\7\\E2-cp-0003-loss0.03.h5\n",
      "2652/2652 [==============================] - 97s 37ms/step - loss: 0.0228 - mean_absolute_error: 0.0228 - mean_absolute_percentage_error: 2003.8911 - val_loss: 0.0321 - val_mean_absolute_error: 0.0320 - val_mean_absolute_percentage_error: 13.8470\n",
      "Epoch 4/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0228 - mean_absolute_error: 0.0227 - mean_absolute_percentage_error: 2007.4110\n",
      "Epoch 00004: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 101s 38ms/step - loss: 0.0228 - mean_absolute_error: 0.0227 - mean_absolute_percentage_error: 2006.7742 - val_loss: 0.0326 - val_mean_absolute_error: 0.0325 - val_mean_absolute_percentage_error: 14.1215\n",
      "Epoch 5/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0227 - mean_absolute_error: 0.0226 - mean_absolute_percentage_error: 1985.4364\n",
      "Epoch 00005: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 98s 37ms/step - loss: 0.0227 - mean_absolute_error: 0.0226 - mean_absolute_percentage_error: 1985.4364 - val_loss: 0.0321 - val_mean_absolute_error: 0.0320 - val_mean_absolute_percentage_error: 13.8664\n",
      "Epoch 6/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0227 - mean_absolute_error: 0.0226 - mean_absolute_percentage_error: 1989.3361\n",
      "Epoch 00006: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 94s 36ms/step - loss: 0.0227 - mean_absolute_error: 0.0226 - mean_absolute_percentage_error: 1989.3361 - val_loss: 0.0323 - val_mean_absolute_error: 0.0322 - val_mean_absolute_percentage_error: 13.8716\n",
      "Epoch 7/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0226 - mean_absolute_error: 0.0225 - mean_absolute_percentage_error: 1974.3126  ETA: 12s - loss: 0.0226 - mean_absolute_error - ETA: 5s - lo\n",
      "Epoch 00007: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 95s 36ms/step - loss: 0.0226 - mean_absolute_error: 0.0225 - mean_absolute_percentage_error: 1974.3126 - val_loss: 0.0328 - val_mean_absolute_error: 0.0328 - val_mean_absolute_percentage_error: 14.3126\n",
      "Epoch 8/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0225 - mean_absolute_error: 0.0224 - mean_absolute_percentage_error: 1969.1674- ETA: 2s - loss: 0.0225 - mean_absolute_error: 0.0224 - me\n",
      "Epoch 00008: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 94s 36ms/step - loss: 0.0225 - mean_absolute_error: 0.0224 - mean_absolute_percentage_error: 1969.1674 - val_loss: 0.0321 - val_mean_absolute_error: 0.0321 - val_mean_absolute_percentage_error: 13.7343\n",
      "Epoch 9/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0224 - mean_absolute_error: 0.0224 - mean_absolute_percentage_error: 1966.7805\n",
      "Epoch 00009: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 92s 35ms/step - loss: 0.0224 - mean_absolute_error: 0.0224 - mean_absolute_percentage_error: 1966.7805 - val_loss: 0.0322 - val_mean_absolute_error: 0.0321 - val_mean_absolute_percentage_error: 13.8219\n",
      "Epoch 10/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0224 - mean_absolute_error: 0.0223 - mean_absolute_percentage_error: 1971.3497  ETA: 10s - loss: 0.022\n",
      "Epoch 00010: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 93s 35ms/step - loss: 0.0224 - mean_absolute_error: 0.0223 - mean_absolute_percentage_error: 1970.7244 - val_loss: 0.0325 - val_mean_absolute_error: 0.0325 - val_mean_absolute_percentage_error: 13.9739\n",
      "Epoch 11/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0224 - mean_absolute_error: 0.0223 - mean_absolute_percentage_error: 1954.6406\n",
      "Epoch 00011: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 94s 36ms/step - loss: 0.0224 - mean_absolute_error: 0.0223 - mean_absolute_percentage_error: 1954.6406 - val_loss: 0.0324 - val_mean_absolute_error: 0.0323 - val_mean_absolute_percentage_error: 13.9505\n",
      "Epoch 12/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0223 - mean_absolute_error: 0.0222 - mean_absolute_percentage_error: 1958.0768\n",
      "Epoch 00012: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 94s 36ms/step - loss: 0.0223 - mean_absolute_error: 0.0222 - mean_absolute_percentage_error: 1957.4565 - val_loss: 0.0332 - val_mean_absolute_error: 0.0331 - val_mean_absolute_percentage_error: 14.1865\n",
      "Epoch 13/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0223 - mean_absolute_error: 0.0222 - mean_absolute_percentage_error: 1951.5308- ETA: 4s - loss: 0.0223 - mean_absolute_error: 0.0 - ETA: 1s - loss: 0.0223 - mean_absolute_error: 0.0222 - mean_absolute_percenta\n",
      "Epoch 00013: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 97s 37ms/step - loss: 0.0223 - mean_absolute_error: 0.0222 - mean_absolute_percentage_error: 1951.5308 - val_loss: 0.0323 - val_mean_absolute_error: 0.0323 - val_mean_absolute_percentage_error: 13.8254\n",
      "Epoch 14/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0222 - mean_absolute_error: 0.0221 - mean_absolute_percentage_error: 1948.3239- ETA: 5s - loss: 0.0222 - mean_absolute_error: 0.0221 - mean_absolut - ETA: 4s - loss: 0.0222 - mean_abs\n",
      "Epoch 00014: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 92s 35ms/step - loss: 0.0222 - mean_absolute_error: 0.0221 - mean_absolute_percentage_error: 1948.3239 - val_loss: 0.0324 - val_mean_absolute_error: 0.0324 - val_mean_absolute_percentage_error: 13.8896\n",
      "Epoch 15/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0222 - mean_absolute_error: 0.0221 - mean_absolute_percentage_error: 1941.1660- ETA: 5s - loss: 0\n",
      "Epoch 00015: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 93s 35ms/step - loss: 0.0222 - mean_absolute_error: 0.0221 - mean_absolute_percentage_error: 1940.5498 - val_loss: 0.0323 - val_mean_absolute_error: 0.0322 - val_mean_absolute_percentage_error: 13.8110\n",
      "Epoch 16/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0221 - mean_absolute_error: 0.0220 - mean_absolute_percentage_error: 1944.5850- ETA: 0s - loss: 0.0221 - mean_absolute_error: 0.0220 - mean_absolute_percentage_er\n",
      "Epoch 00016: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 94s 35ms/step - loss: 0.0221 - mean_absolute_error: 0.0220 - mean_absolute_percentage_error: 1944.5850 - val_loss: 0.0325 - val_mean_absolute_error: 0.0324 - val_mean_absolute_percentage_error: 14.0443\n",
      "Epoch 17/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0221 - mean_absolute_error: 0.0220 - mean_absolute_percentage_error: 1943.6422\n",
      "Epoch 00017: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 94s 35ms/step - loss: 0.0221 - mean_absolute_error: 0.0220 - mean_absolute_percentage_error: 1943.6422 - val_loss: 0.0330 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 14.2291\n",
      "Epoch 18/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0220 - mean_absolute_error: 0.0220 - mean_absolute_percentage_error: 1934.5261\n",
      "Epoch 00018: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 94s 36ms/step - loss: 0.0220 - mean_absolute_error: 0.0220 - mean_absolute_percentage_error: 1934.5261 - val_loss: 0.0331 - val_mean_absolute_error: 0.0331 - val_mean_absolute_percentage_error: 14.3193\n",
      "Epoch 19/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0220 - mean_absolute_error: 0.0219 - mean_absolute_percentage_error: 1938.2750- ETA: 2s - loss: 0.0220 - mean_absolute_error: 0.0219 - mea\n",
      "Epoch 00019: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 93s 35ms/step - loss: 0.0220 - mean_absolute_error: 0.0219 - mean_absolute_percentage_error: 1937.6609 - val_loss: 0.0325 - val_mean_absolute_error: 0.0324 - val_mean_absolute_percentage_error: 13.7779\n",
      "Epoch 20/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0219 - mean_absolute_error: 0.0219 - mean_absolute_percentage_error: 1924.7518\n",
      "Epoch 00020: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 102s 38ms/step - loss: 0.0219 - mean_absolute_error: 0.0219 - mean_absolute_percentage_error: 1924.1415 - val_loss: 0.0322 - val_mean_absolute_error: 0.0321 - val_mean_absolute_percentage_error: 13.8008\n",
      "Epoch 21/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0219 - mean_absolute_error: 0.0218 - mean_absolute_percentage_error: 1933.0083\n",
      "Epoch 00021: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 92s 35ms/step - loss: 0.0219 - mean_absolute_error: 0.0218 - mean_absolute_percentage_error: 1932.3950 - val_loss: 0.0325 - val_mean_absolute_error: 0.0324 - val_mean_absolute_percentage_error: 13.8541\n",
      "Epoch 22/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0218 - mean_absolute_error: 0.0218 - mean_absolute_percentage_error: 1924.9083\n",
      "Epoch 00022: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 93s 35ms/step - loss: 0.0218 - mean_absolute_error: 0.0218 - mean_absolute_percentage_error: 1924.9083 - val_loss: 0.0327 - val_mean_absolute_error: 0.0326 - val_mean_absolute_percentage_error: 14.0557\n",
      "Epoch 23/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0218 - mean_absolute_error: 0.0217 - mean_absolute_percentage_error: 1931.8993\n",
      "Epoch 00023: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 93s 35ms/step - loss: 0.0218 - mean_absolute_error: 0.0217 - mean_absolute_percentage_error: 1931.2869 - val_loss: 0.0326 - val_mean_absolute_error: 0.0325 - val_mean_absolute_percentage_error: 13.9692\n",
      "Epoch 24/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0218 - mean_absolute_error: 0.0217 - mean_absolute_percentage_error: 1938.5767\n",
      "Epoch 00024: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 93s 35ms/step - loss: 0.0218 - mean_absolute_error: 0.0217 - mean_absolute_percentage_error: 1938.5767 - val_loss: 0.0328 - val_mean_absolute_error: 0.0328 - val_mean_absolute_percentage_error: 14.1728\n",
      "Epoch 25/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0217 - mean_absolute_error: 0.0217 - mean_absolute_percentage_error: 1924.7371- ETA: 0s - loss: 0.0217 - mean_absolute_error: 0.0217 - mean_absolute_percentage_error: 1929.0\n",
      "Epoch 00025: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 93s 35ms/step - loss: 0.0217 - mean_absolute_error: 0.0217 - mean_absolute_percentage_error: 1924.1266 - val_loss: 0.0330 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 14.2245\n",
      "Epoch 26/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0217 - mean_absolute_error: 0.0216 - mean_absolute_percentage_error: 1912.5089- ETA: 2s - loss: 0.0217 - mean_absolute_error: 0.0216 - mean_abso\n",
      "Epoch 00026: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 92s 35ms/step - loss: 0.0217 - mean_absolute_error: 0.0216 - mean_absolute_percentage_error: 1911.9026 - val_loss: 0.0329 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 14.1438\n",
      "Epoch 27/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0216 - mean_absolute_error: 0.0216 - mean_absolute_percentage_error: 1926.5781\n",
      "Epoch 00027: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 93s 35ms/step - loss: 0.0216 - mean_absolute_error: 0.0216 - mean_absolute_percentage_error: 1925.9672 - val_loss: 0.0332 - val_mean_absolute_error: 0.0331 - val_mean_absolute_percentage_error: 14.2252\n",
      "Epoch 28/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0216 - mean_absolute_error: 0.0215 - mean_absolute_percentage_error: 1916.3607- ETA: 5s - ETA: 0s - loss: 0.0216 - mean_absolute_error: 0.0215 - mean_absolute_percentage_error: 1914.9194\n",
      "Epoch 00028: val_loss did not improve from 0.03206\n",
      "2652/2652 [==============================] - 93s 35ms/step - loss: 0.0216 - mean_absolute_error: 0.0215 - mean_absolute_percentage_error: 1914.3123 - val_loss: 0.0323 - val_mean_absolute_error: 0.0323 - val_mean_absolute_percentage_error: 13.7617\n",
      "Epoch 29/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0215 - mean_absolute_error: 0.0215 - mean_absolute_percentage_error: 1913.4227"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11892\\4001373639.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalidation_X_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_X\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y_load\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m               return_dict=True)\n\u001b[0m\u001b[0;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y_load),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qj8HRsBXvf0M",
    "outputId": "084e6fbf-b9cc-46e8-e4d7-808e9031da87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 485.07\n",
      "Median Absolute Error (MedAE): 325.85\n",
      "Mean Squared Error (MSE): 475778.52\n",
      "Root Mean Squared Error (RMSE): 689.77\n",
      "Mean Absolute Percentage Error (MAPE): 3.22 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.28 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\5\\\\E2-cp-0014-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAuKvjoOG-fA"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QZI2LOvEw6Qq"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E2-cp-0006-loss0.04.h5'\n",
    "start_epoch= 59+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qPy-9EGRw6eR"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beh0i6Vww6pP",
    "outputId": "41024e1d-abfa-4ec6-a3ed-cf5e5fdef9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E2-cp-0006-loss0.04.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y-9y1xTeYAaB",
    "outputId": "3e59db7d-d1fa-41c7-a91e-cf2063e855da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0231 - mape: 8601.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03575, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0001-loss0.04.h5\n",
      "32614/32614 [==============================] - 7s 210us/sample - loss: 0.0235 - mae: 0.0231 - mape: 8729.3066 - val_loss: 0.0358 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0230 - mape: 8563.2227\n",
      "Epoch 00002: val_loss improved from 0.03575 to 0.03574, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0002-loss0.04.h5\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0235 - mae: 0.0230 - mape: 8710.7080 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1938\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8942.2402\n",
      "Epoch 00003: val_loss improved from 0.03574 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0003-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 115us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8703.8408 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8589.9561\n",
      "Epoch 00004: val_loss improved from 0.03573 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 114us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8711.1074 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1949\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8953.2998\n",
      "Epoch 00005: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8714.5996 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8595.0498\n",
      "Epoch 00006: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8716.8604 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8175.1738\n",
      "Epoch 00007: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8717.2090 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8577.0596\n",
      "Epoch 00008: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8716.0957 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1954\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2109\n",
      "Epoch 00009: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.7314 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2744\n",
      "Epoch 00010: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.3086 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.7139\n",
      "Epoch 00011: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.5303 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.2705\n",
      "Epoch 00012: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8717.8525 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1962\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2910\n",
      "Epoch 00013: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.2773 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8968.6553\n",
      "Epoch 00014: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.5400 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1970\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8580.6602\n",
      "Epoch 00015: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8719.8223 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1967\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8623\n",
      "Epoch 00016: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.7832 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1963\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8877\n",
      "Epoch 00017: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.8203 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8606.8154\n",
      "Epoch 00018: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0547 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2119\n",
      "Epoch 00019: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.2393 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8973.6885\n",
      "Epoch 00020: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8734.4512 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8217.7236  ETA: 2s - loss: 0.0233 -\n",
      "Epoch 00021: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8730.0449 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8967.8350\n",
      "Epoch 00022: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8728.7363 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8965.8945\n",
      "Epoch 00023: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.8516 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1969\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8607.8672\n",
      "Epoch 00024: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0010 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1964\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8588.1279\n",
      "Epoch 00025: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8735.5576 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8230.3066\n",
      "Epoch 00026: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8738.8379 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 27/200\n",
      "13312/32614 [===========>..................] - ETA: 1s - loss: 0.0233 - mae: 0.0229 - mape: 7589.0488"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-05065ff65640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y, \n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9fPE-jZuQ0r",
    "outputId": "aca09565-e9df-4067-d473-32dd8826a738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 541.71\n",
      "Median Absolute Error (MedAE): 449.47\n",
      "Mean Squared Error (MSE): 473684.02\n",
      "Root Mean Squared Error (RMSE): 688.25\n",
      "Mean Absolute Percentage Error (MAPE): 5.4 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.39 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-0004-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7x0O-u0GbLy"
   },
   "source": [
    "# E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rdBroUOqIeW6"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(200, activation='relu')(x)\n",
    " #       x = Dense(10, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AQaM-IuPIXqN"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QwUKEvSfIW09"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NHgO7mJ4IQVp"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TIsLEDoIN0C",
    "outputId": "3ef77929-b334-4953-faab-b938f376c306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpLkyqBmIKIw",
    "outputId": "3d701f4a-270d-4efc-b52a-cd26d12d4dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.2038 - mae: 0.0831 - mape: 10284.1670\n",
      "Epoch 00001: val_loss improved from inf to 0.10131, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0001-loss0.10.h5\n",
      "32614/32614 [==============================] - 19s 581us/sample - loss: 0.2036 - mae: 0.0830 - mape: 10262.1201 - val_loss: 0.1013 - val_mae: 0.0675 - val_mape: 11.9466\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0654 - mae: 0.0492 - mape: 9617.4668\n",
      "Epoch 00002: val_loss improved from 0.10131 to 0.07428, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0002-loss0.07.h5\n",
      "32614/32614 [==============================] - 14s 420us/sample - loss: 0.0654 - mae: 0.0492 - mape: 9596.8438 - val_loss: 0.0743 - val_mae: 0.0671 - val_mape: 11.8373\n",
      "Epoch 3/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0511 - mae: 0.0461 - mape: 9812.7764\n",
      "Epoch 00003: val_loss improved from 0.07428 to 0.06341, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0003-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0510 - mae: 0.0460 - mape: 9762.8730 - val_loss: 0.0634 - val_mae: 0.0599 - val_mape: 10.4427\n",
      "Epoch 4/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0450 - mae: 0.0419 - mape: 9616.9199\n",
      "Epoch 00004: val_loss did not improve from 0.06341\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0450 - mae: 0.0419 - mape: 9596.3027 - val_loss: 0.0706 - val_mae: 0.0680 - val_mape: 11.4199\n",
      "Epoch 5/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0441 - mae: 0.0414 - mape: 9613.5029\n",
      "Epoch 00005: val_loss improved from 0.06341 to 0.05454, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0005-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0441 - mae: 0.0414 - mape: 9611.7354 - val_loss: 0.0545 - val_mae: 0.0519 - val_mape: 9.2312\n",
      "Epoch 6/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0418 - mae: 0.0397 - mape: 9602.1924\n",
      "Epoch 00006: val_loss did not improve from 0.05454\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0418 - mae: 0.0397 - mape: 9572.1885 - val_loss: 0.0587 - val_mae: 0.0570 - val_mape: 9.8649\n",
      "Epoch 7/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0406 - mae: 0.0389 - mape: 9195.3799\n",
      "Epoch 00007: val_loss improved from 0.05454 to 0.04682, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0007-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0406 - mae: 0.0389 - mape: 9554.0049 - val_loss: 0.0468 - val_mae: 0.0450 - val_mape: 8.7847\n",
      "Epoch 8/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0399 - mae: 0.0384 - mape: 9416.9941\n",
      "Epoch 00008: val_loss did not improve from 0.04682\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0399 - mae: 0.0384 - mape: 9396.7998 - val_loss: 0.0475 - val_mae: 0.0462 - val_mape: 8.9737\n",
      "Epoch 9/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0397 - mae: 0.0383 - mape: 9665.3730\n",
      "Epoch 00009: val_loss improved from 0.04682 to 0.04522, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0009-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0397 - mae: 0.0383 - mape: 9644.6445 - val_loss: 0.0452 - val_mae: 0.0439 - val_mape: 8.5116\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0377 - mape: 9573.1260\n",
      "Epoch 00010: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0389 - mae: 0.0377 - mape: 9533.8271 - val_loss: 0.0940 - val_mae: 0.0928 - val_mape: 15.8112\n",
      "Epoch 11/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0390 - mae: 0.0378 - mape: 9537.1748\n",
      "Epoch 00011: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0390 - mae: 0.0378 - mape: 9535.4209 - val_loss: 0.0501 - val_mae: 0.0490 - val_mape: 8.8788\n",
      "Epoch 12/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0381 - mae: 0.0371 - mape: 9512.4170\n",
      "Epoch 00012: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0381 - mae: 0.0371 - mape: 9492.0146 - val_loss: 0.0511 - val_mae: 0.0502 - val_mape: 9.2328\n",
      "Epoch 13/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0377 - mae: 0.0368 - mape: 9480.7461\n",
      "Epoch 00013: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0377 - mae: 0.0368 - mape: 9479.0029 - val_loss: 0.0494 - val_mae: 0.0485 - val_mape: 9.0546\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.0370 - mape: 9640.1309\n",
      "Epoch 00014: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0379 - mae: 0.0370 - mape: 9600.5547 - val_loss: 0.0475 - val_mae: 0.0466 - val_mape: 8.6730\n",
      "Epoch 15/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0375 - mae: 0.0367 - mape: 9567.5752\n",
      "Epoch 00015: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0375 - mae: 0.0367 - mape: 9565.8154 - val_loss: 0.0497 - val_mae: 0.0489 - val_mape: 9.3880\n",
      "Epoch 16/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0374 - mae: 0.0366 - mape: 9551.2051\n",
      "Epoch 00016: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0374 - mae: 0.0366 - mape: 9540.0859 - val_loss: 0.0464 - val_mae: 0.0456 - val_mape: 8.5940\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0362 - mape: 9563.9082\n",
      "Epoch 00017: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0369 - mae: 0.0362 - mape: 9562.1514 - val_loss: 0.0475 - val_mae: 0.0468 - val_mape: 8.6608\n",
      "Epoch 18/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0361 - mape: 9577.6992\n",
      "Epoch 00018: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0368 - mae: 0.0361 - mape: 9566.5488 - val_loss: 0.0627 - val_mae: 0.0620 - val_mape: 10.5263\n",
      "Epoch 19/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0366 - mae: 0.0359 - mape: 8800.0615\n",
      "Epoch 00019: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0366 - mae: 0.0359 - mape: 9530.1943 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 8.7106\n",
      "Epoch 20/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9487.9727\n",
      "Epoch 00020: val_loss improved from 0.04522 to 0.04432, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0020-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9476.9248 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.3836\n",
      "Epoch 21/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9175.1748\n",
      "Epoch 00021: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9548.0811 - val_loss: 0.0444 - val_mae: 0.0438 - val_mape: 8.4635\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0356 - mape: 9438.7158\n",
      "Epoch 00022: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0362 - mae: 0.0356 - mape: 9427.7275 - val_loss: 0.0463 - val_mae: 0.0456 - val_mape: 8.8022\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0363 - mae: 0.0356 - mape: 9494.2676\n",
      "Epoch 00023: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0363 - mae: 0.0356 - mape: 9492.5234 - val_loss: 0.0502 - val_mae: 0.0496 - val_mape: 8.8956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0360 - mae: 0.0354 - mape: 9481.0664\n",
      "Epoch 00024: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0360 - mae: 0.0354 - mape: 9479.3242 - val_loss: 0.0482 - val_mae: 0.0476 - val_mape: 8.8535\n",
      "Epoch 25/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9452.0479\n",
      "Epoch 00025: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9422.5107 - val_loss: 0.0450 - val_mae: 0.0444 - val_mape: 8.3675\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9389.7109\n",
      "Epoch 00026: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9360.3701 - val_loss: 0.0505 - val_mae: 0.0499 - val_mape: 8.9818\n",
      "Epoch 27/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9449.0146\n",
      "Epoch 00027: val_loss improved from 0.04432 to 0.04404, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0027-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9419.4844 - val_loss: 0.0440 - val_mae: 0.0434 - val_mape: 8.4639\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9357.8389\n",
      "Epoch 00028: val_loss improved from 0.04404 to 0.04385, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0028-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9346.9443 - val_loss: 0.0438 - val_mae: 0.0433 - val_mape: 8.3336\n",
      "Epoch 29/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0350 - mape: 9348.0967- ETA: 2s - loss: 0.\n",
      "Epoch 00029: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0356 - mae: 0.0350 - mape: 9337.2129 - val_loss: 0.0453 - val_mae: 0.0447 - val_mape: 8.8223\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0355 - mae: 0.0349 - mape: 9328.2656\n",
      "Epoch 00030: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0355 - mae: 0.0349 - mape: 9299.1152 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.4327\n",
      "Epoch 31/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0348 - mape: 9367.9561\n",
      "Epoch 00031: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0354 - mae: 0.0348 - mape: 9329.5146 - val_loss: 0.0556 - val_mae: 0.0550 - val_mape: 9.5869\n",
      "Epoch 32/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0346 - mae: 0.0341 - mape: 9331.7041\n",
      "Epoch 00032: val_loss improved from 0.04385 to 0.03952, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0032-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0346 - mae: 0.0341 - mape: 9293.3955 - val_loss: 0.0395 - val_mae: 0.0389 - val_mape: 7.3175\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0313 - mae: 0.0307 - mape: 8705.7988\n",
      "Epoch 00033: val_loss did not improve from 0.03952\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0313 - mae: 0.0307 - mape: 9014.9424 - val_loss: 0.0402 - val_mae: 0.0396 - val_mape: 6.9964\n",
      "Epoch 34/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0310 - mae: 0.0304 - mape: 8956.5234\n",
      "Epoch 00034: val_loss improved from 0.03952 to 0.03736, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0034-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 427us/sample - loss: 0.0310 - mae: 0.0304 - mape: 8937.3135 - val_loss: 0.0374 - val_mae: 0.0368 - val_mape: 6.7796\n",
      "Epoch 35/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0308 - mae: 0.0302 - mape: 9005.4199\n",
      "Epoch 00035: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0308 - mae: 0.0302 - mape: 9003.7646 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.5172\n",
      "Epoch 36/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 8986.6465\n",
      "Epoch 00036: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0307 - mae: 0.0301 - mape: 8984.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.7190\n",
      "Epoch 37/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 9036.8193\n",
      "Epoch 00037: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0307 - mae: 0.0301 - mape: 9017.4414 - val_loss: 0.0397 - val_mae: 0.0391 - val_mape: 6.9239\n",
      "Epoch 38/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0304 - mae: 0.0298 - mape: 8871.0625\n",
      "Epoch 00038: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0305 - mae: 0.0299 - mape: 8852.0391 - val_loss: 0.0418 - val_mae: 0.0413 - val_mape: 7.1686\n",
      "Epoch 39/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8882.9082\n",
      "Epoch 00039: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8881.2744 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.0621\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8929.6758\n",
      "Epoch 00040: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8910.5225 - val_loss: 0.0390 - val_mae: 0.0384 - val_mape: 6.7820\n",
      "Epoch 41/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8876.4180\n",
      "Epoch 00041: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8857.3779 - val_loss: 0.0391 - val_mae: 0.0385 - val_mape: 6.9458\n",
      "Epoch 42/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8984.8389\n",
      "Epoch 00042: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8956.7549 - val_loss: 0.0413 - val_mae: 0.0407 - val_mape: 7.6354\n",
      "Epoch 43/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0301 - mae: 0.0295 - mape: 8793.3711\n",
      "Epoch 00043: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0301 - mae: 0.0295 - mape: 8783.1338 - val_loss: 0.0435 - val_mae: 0.0429 - val_mape: 7.4288\n",
      "Epoch 44/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8822.2676- ETA: 6s - loss: 0.0301 - mae: 0.0 - ETA: 1s - loss: 0.0302 - mae:\n",
      "Epoch 00044: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8811.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.9922\n",
      "Epoch 45/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0298 - mae: 0.0292 - mape: 8836.8652\n",
      "Epoch 00045: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0298 - mae: 0.0292 - mape: 8800.5830 - val_loss: 0.0593 - val_mae: 0.0587 - val_mape: 11.2009\n",
      "Epoch 46/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0287 - mae: 0.0281 - mape: 8818.6738\n",
      "Epoch 00046: val_loss improved from 0.03736 to 0.03728, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0046-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0287 - mae: 0.0281 - mape: 8791.1104 - val_loss: 0.0373 - val_mae: 0.0367 - val_mape: 6.3479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0269 - mape: 8903.1064\n",
      "Epoch 00047: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0276 - mae: 0.0269 - mape: 8866.5479 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 7.3343\n",
      "Epoch 48/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0264 - mape: 8938.7715\n",
      "Epoch 00048: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0270 - mae: 0.0264 - mape: 8919.5977 - val_loss: 0.0419 - val_mae: 0.0413 - val_mape: 6.8349\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0265 - mape: 8954.8320\n",
      "Epoch 00049: val_loss improved from 0.03728 to 0.03701, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0049-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0272 - mae: 0.0265 - mape: 8926.8408 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2637\n",
      "Epoch 50/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0263 - mape: 8925.1689\n",
      "Epoch 00050: val_loss did not improve from 0.03701\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0269 - mae: 0.0263 - mape: 8914.7754 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2835\n",
      "Epoch 51/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0262 - mape: 8973.1758\n",
      "Epoch 00051: val_loss improved from 0.03701 to 0.03644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0051-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0269 - mae: 0.0262 - mape: 8936.3379 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3367\n",
      "Epoch 52/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0267 - mae: 0.0260 - mape: 8984.7070\n",
      "Epoch 00052: val_loss improved from 0.03644 to 0.03603, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0267 - mae: 0.0260 - mape: 8947.8154 - val_loss: 0.0360 - val_mae: 0.0354 - val_mape: 6.2870\n",
      "Epoch 53/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.0259 - mape: 8895.4316\n",
      "Epoch 00053: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0265 - mae: 0.0259 - mape: 8893.7969 - val_loss: 0.0433 - val_mae: 0.0426 - val_mape: 6.9243\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0258 - mape: 8529.0977\n",
      "Epoch 00054: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0264 - mae: 0.0258 - mape: 8883.7363 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.3396\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8902.2764\n",
      "Epoch 00055: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8883.1807 - val_loss: 0.0377 - val_mae: 0.0371 - val_mape: 6.5213\n",
      "Epoch 56/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8834.2520\n",
      "Epoch 00056: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8806.6387 - val_loss: 0.0378 - val_mae: 0.0372 - val_mape: 6.4096\n",
      "Epoch 57/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0255 - mape: 8899.8281\n",
      "Epoch 00057: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0261 - mae: 0.0255 - mape: 8889.4648 - val_loss: 0.0365 - val_mae: 0.0358 - val_mape: 6.4844\n",
      "Epoch 58/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0257 - mape: 8935.8350\n",
      "Epoch 00058: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0263 - mae: 0.0257 - mape: 8899.1426 - val_loss: 0.0389 - val_mae: 0.0382 - val_mape: 6.9754\n",
      "Epoch 59/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0255 - mape: 8985.3438\n",
      "Epoch 00059: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0262 - mae: 0.0255 - mape: 8966.0693 - val_loss: 0.0448 - val_mae: 0.0442 - val_mape: 7.1970\n",
      "Epoch 60/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0254 - mape: 8698.2939\n",
      "Epoch 00060: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0261 - mae: 0.0254 - mape: 9026.8086 - val_loss: 0.0371 - val_mae: 0.0365 - val_mape: 6.4728\n",
      "Epoch 61/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0256 - mape: 8944.7266\n",
      "Epoch 00061: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0262 - mae: 0.0256 - mape: 8943.0820 - val_loss: 0.0376 - val_mae: 0.0369 - val_mape: 6.4816\n",
      "Epoch 62/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0253 - mape: 8861.0674\n",
      "Epoch 00062: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0259 - mae: 0.0253 - mape: 8833.3691 - val_loss: 0.0376 - val_mae: 0.0370 - val_mape: 6.7046\n",
      "Epoch 63/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0252 - mape: 8937.2539\n",
      "Epoch 00063: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0259 - mae: 0.0252 - mape: 8909.3174 - val_loss: 0.0441 - val_mae: 0.0434 - val_mape: 7.0393\n",
      "Epoch 64/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0251 - mape: 8836.8535\n",
      "Epoch 00064: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0257 - mae: 0.0251 - mape: 8826.5635 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.1966\n",
      "Epoch 65/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8864.3887\n",
      "Epoch 00065: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8845.3740 - val_loss: 0.0403 - val_mae: 0.0396 - val_mape: 6.6836\n",
      "Epoch 66/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0251 - mape: 8864.2441\n",
      "Epoch 00066: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0258 - mae: 0.0251 - mape: 8853.9238 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.2001\n",
      "Epoch 67/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8919.5137- ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8391\n",
      "Epoch 00067: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0256 - mae: 0.0249 - mape: 8900.3799 - val_loss: 0.0435 - val_mae: 0.0428 - val_mape: 6.9965\n",
      "Epoch 68/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0249 - mape: 8921.0537\n",
      "Epoch 00068: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0255 - mae: 0.0249 - mape: 8910.6650 - val_loss: 0.0368 - val_mae: 0.0361 - val_mape: 6.4016\n",
      "Epoch 69/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8904.1221\n",
      "Epoch 00069: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8885.0215 - val_loss: 0.0366 - val_mae: 0.0359 - val_mape: 6.4782\n",
      "Epoch 70/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0248 - mape: 8509.3799\n",
      "Epoch 00070: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0255 - mae: 0.0248 - mape: 8848.4863 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0247 - mape: 8894.0742\n",
      "Epoch 00071: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0254 - mae: 0.0247 - mape: 8874.9961 - val_loss: 0.0447 - val_mae: 0.0440 - val_mape: 7.2170\n",
      "Epoch 72/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0246 - mape: 8940.8789\n",
      "Epoch 00072: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0253 - mae: 0.0246 - mape: 8921.6992 - val_loss: 0.0399 - val_mae: 0.0393 - val_mape: 6.6394\n",
      "Epoch 73/80\n",
      "31840/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0246 - mape: 8697.8066"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d15c63a445f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BNTzwnub2Eh-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 577.97\n",
      "Median Absolute Error (MedAE): 474.07\n",
      "Mean Squared Error (MSE): 554533.54\n",
      "Root Mean Squared Error (RMSE): 744.67\n",
      "Mean Absolute Percentage Error (MAPE): 5.86 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.58 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tYU2gaBI4Yl"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fz9kU3b6IvyO"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5'\n",
    "start_epoch= 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2bMbPx8RJKL4"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5T4Ho3YJJKTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "14t41QwoJKZ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0241 - mae: 0.0235 - mape: 8787.5186\n",
      "Epoch 00001: val_loss improved from inf to 0.03371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 57s 2ms/sample - loss: 0.0241 - mae: 0.0235 - mape: 8785.9023 - val_loss: 0.0337 - val_mae: 0.0331 - val_mape: 5.6219\n",
      "Epoch 2/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8818.7939\n",
      "Epoch 00002: val_loss improved from 0.03371 to 0.03268, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 8808.5244 - val_loss: 0.0327 - val_mae: 0.0321 - val_mape: 5.5019\n",
      "Epoch 3/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 8832.4248\n",
      "Epoch 00003: val_loss improved from 0.03268 to 0.03238, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 8830.8018 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4341\n",
      "Epoch 4/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 8807.1309\n",
      "Epoch 00004: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 8805.5117 - val_loss: 0.0330 - val_mae: 0.0324 - val_mape: 5.4931\n",
      "Epoch 5/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0212 - mape: 8834.0068\n",
      "Epoch 00005: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0212 - mape: 8832.3828 - val_loss: 0.0333 - val_mae: 0.0328 - val_mape: 5.5382\n",
      "Epoch 6/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8771.0771\n",
      "Epoch 00006: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8769.4648 - val_loss: 0.0326 - val_mae: 0.0321 - val_mape: 5.4661\n",
      "Epoch 7/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8793.5381\n",
      "Epoch 00007: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8783.2979 - val_loss: 0.0325 - val_mae: 0.0320 - val_mape: 5.4550\n",
      "Epoch 8/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8746.4922\n",
      "Epoch 00008: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8744.8848 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.4740\n",
      "Epoch 9/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8799.6211\n",
      "Epoch 00009: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8789.3721 - val_loss: 0.0332 - val_mae: 0.0327 - val_mape: 5.5137\n",
      "Epoch 10/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8783.9893\n",
      "Epoch 00010: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8782.3740 - val_loss: 0.0327 - val_mae: 0.0322 - val_mape: 5.4856\n",
      "Epoch 11/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8814.5137\n",
      "Epoch 00011: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8804.2490 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.4683\n",
      "Epoch 12/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0208 - mape: 8829.3320\n",
      "Epoch 00012: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0213 - mae: 0.0208 - mape: 8819.0498 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4762\n",
      "Epoch 13/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8766.2891\n",
      "Epoch 00013: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8764.6777 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.6098\n",
      "Epoch 14/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0208 - mape: 8807.6914\n",
      "Epoch 00014: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0208 - mape: 8797.4355 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4454\n",
      "Epoch 15/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8763.3545\n",
      "Epoch 00015: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8761.7432 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4502\n",
      "Epoch 16/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0207 - mape: 8800.9727\n",
      "Epoch 00016: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0207 - mape: 8790.7227 - val_loss: 0.0336 - val_mae: 0.0331 - val_mape: 5.5508\n",
      "Epoch 17/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8805.7637\n",
      "Epoch 00017: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 8804.1465 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5136\n",
      "Epoch 18/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8785.8975\n",
      "Epoch 00018: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8775.6660 - val_loss: 0.0324 - val_mae: 0.0320 - val_mape: 5.4366\n",
      "Epoch 19/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8766.3740\n",
      "Epoch 00019: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8756.1650 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5053\n",
      "Epoch 20/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8869.6514- ETA: 1s - loss: 0.0209 - mae: 0.0205\n",
      "Epoch 00020: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8859.3301 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5394\n",
      "Epoch 21/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8719.9990\n",
      "Epoch 00021: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8718.3955 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.4912\n",
      "Epoch 22/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8816.5059\n",
      "Epoch 00022: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8814.8848 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5017\n",
      "Epoch 23/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8851.2988\n",
      "Epoch 00023: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8840.9902 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5257\n",
      "Epoch 24/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8849.9473\n",
      "Epoch 00024: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8839.6475 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8417.9062\n",
      "Epoch 00025: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8762.0635 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5165\n",
      "Epoch 26/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8781.5459\n",
      "Epoch 00026: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8771.3203 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5305\n",
      "Epoch 27/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8805.2725\n",
      "Epoch 00027: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8795.0176 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5373\n",
      "Epoch 28/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8814.1201\n",
      "Epoch 00028: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8803.8555 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5812\n",
      "Epoch 29/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8835.2373\n",
      "Epoch 00029: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8833.6133 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.5439\n",
      "Epoch 30/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8756.9062\n",
      "Epoch 00030: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8746.7100 - val_loss: 0.0335 - val_mae: 0.0331 - val_mape: 5.5782\n",
      "Epoch 31/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8848.8691\n",
      "Epoch 00031: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8847.2422 - val_loss: 0.0345 - val_mae: 0.0342 - val_mape: 5.6903\n",
      "Epoch 32/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8786.9775\n",
      "Epoch 00032: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8785.3623 - val_loss: 0.0348 - val_mae: 0.0344 - val_mape: 5.7179\n",
      "Epoch 33/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8790.1523\n",
      "Epoch 00033: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8779.9150 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5215\n",
      "Epoch 34/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8812.7725\n",
      "Epoch 00034: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8811.1523 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5479\n",
      "Epoch 35/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8778.3477-\n",
      "Epoch 00035: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8776.7334 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5697\n",
      "Epoch 36/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8766.7051\n",
      "Epoch 00036: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8756.4961 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.6073\n",
      "Epoch 37/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0200 - mape: 8769.1572\n",
      "Epoch 00037: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0200 - mape: 8767.5449 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5441\n",
      "Epoch 38/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0201 - mape: 8747.3877\n",
      "Epoch 00038: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0201 - mape: 8745.7793 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5797\n",
      "Epoch 39/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0200 - mape: 8836.6064\n",
      "Epoch 00039: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0200 - mape: 8834.9814 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5334\n",
      "Epoch 40/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8814.8398\n",
      "Epoch 00040: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8804.5742 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5985\n",
      "Epoch 41/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8839.9746\n",
      "Epoch 00041: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8829.6797 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5734\n",
      "Epoch 42/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0198 - mape: 8831.3848\n",
      "Epoch 00042: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0202 - mae: 0.0198 - mape: 8829.7607 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5997\n",
      "Epoch 43/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0201 - mae: 0.0198 - mape: 8783.8525\n",
      "Epoch 00043: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0201 - mae: 0.0198 - mape: 8782.2383 - val_loss: 0.0337 - val_mae: 0.0334 - val_mape: 5.6261\n",
      "Epoch 44/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0197 - mape: 8822.6426\n",
      "Epoch 00044: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0197 - mape: 8821.0205 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.6350\n",
      "Epoch 45/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8898.9219\n",
      "Epoch 00045: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8888.5576 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5640\n",
      "Epoch 46/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8848.5547\n",
      "Epoch 00046: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8846.9277 - val_loss: 0.0326 - val_mae: 0.0323 - val_mape: 5.5130\n",
      "Epoch 47/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8915.1211\n",
      "Epoch 00047: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8904.7383 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5152\n",
      "Epoch 48/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8884.6523\n",
      "Epoch 00048: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8874.3057 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5568\n",
      "Epoch 49/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0195 - mape: 8901.6914\n",
      "Epoch 00049: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0195 - mape: 8891.3252 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8912.0732\n",
      "Epoch 00050: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8901.6934 - val_loss: 0.0326 - val_mae: 0.0322 - val_mape: 5.5117\n",
      "Epoch 51/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0194 - mape: 8897.9141\n",
      "Epoch 00051: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0194 - mape: 8896.2773 - val_loss: 0.0339 - val_mae: 0.0336 - val_mape: 5.6620\n",
      "Epoch 52/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8820.4697\n",
      "Epoch 00052: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8810.1973 - val_loss: 0.0336 - val_mae: 0.0333 - val_mape: 5.6221\n",
      "Epoch 53/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8908.1738\n",
      "Epoch 00053: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8906.5361 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5511\n",
      "Epoch 54/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8871.0322\n",
      "Epoch 00054: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8869.4014 - val_loss: 0.0333 - val_mae: 0.0330 - val_mape: 5.5471\n",
      "Epoch 55/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8876.8662\n",
      "Epoch 00055: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8866.5283 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5071\n",
      "Epoch 56/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8917.7432- ETA: 2s - loss: 0.0196 - ma\n",
      "Epoch 00056: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8916.1035 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.5076\n",
      "Epoch 57/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8886.0176\n",
      "Epoch 00057: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8884.3838 - val_loss: 0.0330 - val_mae: 0.0327 - val_mape: 5.6687\n",
      "Epoch 58/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8843.5547\n",
      "Epoch 00058: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0195 - mae: 0.0192 - mape: 8833.2559 - val_loss: 0.0340 - val_mae: 0.0337 - val_mape: 5.6366\n",
      "Epoch 59/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8897.1895\n",
      "Epoch 00059: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8886.8271 - val_loss: 0.0344 - val_mae: 0.0340 - val_mape: 5.7448\n",
      "Epoch 60/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8878.5566\n",
      "Epoch 00060: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0192 - mape: 8868.2227 - val_loss: 0.0338 - val_mae: 0.0334 - val_mape: 5.6100\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.92\n",
      "Median Absolute Error (MedAE): 341.57\n",
      "Mean Squared Error (MSE): 329961.29\n",
      "Root Mean Squared Error (RMSE): 574.42\n",
      "Mean Absolute Percentage Error (MAPE): 4.31 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.33 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPZ0OMjDJhnO"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UZgZB020JqhL"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5'\n",
    "start_epoch= 53+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ssm3plQeJn6F"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "u-3UhfYuJlge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "9b_pJN2ZJKff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9133.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03257, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 9s 283us/sample - loss: 0.0214 - mae: 0.0208 - mape: 8890.3867 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4449\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8733.1416\n",
      "Epoch 00002: val_loss improved from 0.03257 to 0.03256, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8869.8643 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4437\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9107.9521\n",
      "Epoch 00003: val_loss improved from 0.03256 to 0.03242, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8865.1064 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8707.8369\n",
      "Epoch 00004: val_loss improved from 0.03242 to 0.03233, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8851.4385 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4285\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9089.2246\n",
      "Epoch 00005: val_loss improved from 0.03233 to 0.03228, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8846.9043 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8326.9922\n",
      "Epoch 00006: val_loss improved from 0.03228 to 0.03226, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8841.4912 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8705.7549\n",
      "Epoch 00007: val_loss improved from 0.03226 to 0.03223, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8839.4424 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4216\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9068.8096\n",
      "Epoch 00008: val_loss improved from 0.03223 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8827.0107 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4208\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8693.2275  ETA: 1s - loss: 0.0213 - mae: 0.020\n",
      "Epoch 00009: val_loss improved from 0.03222 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8828.1250 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8320.9189\n",
      "Epoch 00010: val_loss improved from 0.03222 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8823.5527 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8678.0664\n",
      "Epoch 00011: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8818.6221 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9058.5664\n",
      "Epoch 00012: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8817.0547 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.8018\n",
      "Epoch 00013: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8819.4121 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9055.2490\n",
      "Epoch 00014: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8813.7979 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9056.6045\n",
      "Epoch 00015: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8815.1211 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4180\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9046.7842\n",
      "Epoch 00016: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8805.6162 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.0303\n",
      "Epoch 00017: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8811.1191 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4184\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8288.7080\n",
      "Epoch 00018: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8806.9727 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9051.8076\n",
      "Epoch 00019: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.4658 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8302.6875\n",
      "Epoch 00020: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.8389 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4196\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.1914\n",
      "Epoch 00021: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8803.5537 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8289.9629\n",
      "Epoch 00022: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8808.2617 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.5645\n",
      "Epoch 00023: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8807.9199 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8657.6855\n",
      "Epoch 00024: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.0996 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8660.9229- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00025: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4287 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4194\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.9697\n",
      "Epoch 00026: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.0215 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4190\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.5674- ETA: 1s - loss: 0.0211 - mae: 0.0205\n",
      "Epoch 00027: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.4775 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.4648\n",
      "Epoch 00028: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4198\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8674.4756\n",
      "Epoch 00029: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.5635 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7906.3037\n",
      "Epoch 00030: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.4629 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.0000\n",
      "Epoch 00031: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.7725 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4202\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9045.3105\n",
      "Epoch 00032: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.1299 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.4668  ETA: 2s - loss: 0.0211 - m\n",
      "Epoch 00033: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.4404 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8285.9170\n",
      "Epoch 00034: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.4795 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4204\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.9082\n",
      "Epoch 00035: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6709 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8681.3047- ETA: 2s - loss: 0.0210 - mae: 0.\n",
      "Epoch 00036: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.3027 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1055- ETA: 2s - loss: 0.0211 - mae: \n",
      "Epoch 00037: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.6387 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.3789\n",
      "Epoch 00038: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4212\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7911.5845\n",
      "Epoch 00039: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.6006 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4207\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7988\n",
      "Epoch 00040: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.7549 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4209\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.4160\n",
      "Epoch 00041: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8797.7119 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4218\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.8086\n",
      "Epoch 00042: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.4561 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9040.8076- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8763.4\n",
      "Epoch 00043: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.7510 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8671.4062\n",
      "Epoch 00044: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9043.8984\n",
      "Epoch 00045: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.7754 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.5801\n",
      "Epoch 00046: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4297 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4199\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.4580\n",
      "Epoch 00047: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.2539 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4226\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8656.3926\n",
      "Epoch 00048: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8795.1895 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4227\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.8350\n",
      "Epoch 00049: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6064 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9053.7900\n",
      "Epoch 00050: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8812.3799 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4222\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.2178\n",
      "Epoch 00051: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.1719 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4229\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.2744\n",
      "Epoch 00052: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.0586 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4240\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7900\n",
      "Epoch 00053: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.5859 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8664.6738\n",
      "Epoch 00054: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.6367 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4230\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8658.2246\n",
      "Epoch 00055: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.2617 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4231\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9039.0703\n",
      "Epoch 00056: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.0605 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.2510\n",
      "Epoch 00057: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.1816 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4238\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.6846\n",
      "Epoch 00058: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.4893 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4235\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.6758\n",
      "Epoch 00059: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4250\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.4639\n",
      "Epoch 00060: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.2031 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.5479\n",
      "Epoch 00061: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8801.4482 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.7363\n",
      "Epoch 00062: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.5029 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1113- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7955. - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8069\n",
      "Epoch 00063: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8803.5488 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.6602\n",
      "Epoch 00064: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8809.3477 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8292.0371\n",
      "Epoch 00065: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8800.8613 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4243\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8661.9170\n",
      "Epoch 00066: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.5088 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4233\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.0117\n",
      "Epoch 00067: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.7666 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.6230- ETA: 2s - loss: 0.0212 - mae:\n",
      "Epoch 00068: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.3867 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4254\n",
      "Epoch 69/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8285.9189\n",
      "Epoch 00069: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.3457 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4237\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.5889\n",
      "Epoch 00070: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.3418 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.3438\n",
      "Epoch 00071: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8803.1846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4249\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.1455\n",
      "Epoch 00072: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.9678 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8665.8174\n",
      "Epoch 00073: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.1689 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.3008\n",
      "Epoch 00074: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.6846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.2236\n",
      "Epoch 00075: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.9883 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4261\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.5234  ETA: 2s - loss: 0.0209 - \n",
      "Epoch 00076: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4239\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9045.4512\n",
      "Epoch 00077: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.2715 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4265\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.2568\n",
      "Epoch 00078: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0527 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4256\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9055.4580\n",
      "Epoch 00079: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.0088 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4271\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9054.0859\n",
      "Epoch 00080: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8812.6738 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9052.4775\n",
      "Epoch 00081: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.1074 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4263\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.4385\n",
      "Epoch 00082: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.1035 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.2695\n",
      "Epoch 00083: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.9912 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8680.7773\n",
      "Epoch 00084: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8681.9316\n",
      "Epoch 00085: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.9023 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4258\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.9326\n",
      "Epoch 00086: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.5898 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.4629\n",
      "Epoch 00087: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.1172 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4259\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8666.3701\n",
      "Epoch 00088: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2188 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8664.1152\n",
      "Epoch 00089: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.5791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4258\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.7578\n",
      "Epoch 00090: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4267\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8674.5449\n",
      "Epoch 00091: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.6641 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4264\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.3330\n",
      "Epoch 00092: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.8057 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8680.4805\n",
      "Epoch 00093: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8807.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4268\n",
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8669.4492\n",
      "Epoch 00094: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0020 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8670.0195\n",
      "Epoch 00095: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.6592 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.8623\n",
      "Epoch 00096: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8808.5645 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4274\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8297.3535\n",
      "Epoch 00097: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8814.3789 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.8828\n",
      "Epoch 00098: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8803.7236 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8692.5449\n",
      "Epoch 00099: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8821.6729 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4253\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.0117  ETA: 2s - loss: 0.0212 - mae: 0.0207 - mape - ETA: 1s - loss: 0.0211 - mae: 0.0206 - mape: 13501.83 - ETA: 1s - loss: 0.0211 - mae: 0.0\n",
      "Epoch 00100: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.5918 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8291.2773\n",
      "Epoch 00101: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.7295 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4274\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.1973\n",
      "Epoch 00102: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.7637 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4269\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.2109\n",
      "Epoch 00103: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.1260 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.0146\n",
      "Epoch 00104: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.8604 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8309.9893\n",
      "Epoch 00105: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.9658 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8300.4561\n",
      "Epoch 00106: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.9160 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8687.0166\n",
      "Epoch 00107: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8820.6562 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4257\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.9922  ETA: 1s - loss: 0.021\n",
      "Epoch 00108: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8818.7070 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7918.0552\n",
      "Epoch 00109: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8806.1182 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4266\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9054.2461\n",
      "Epoch 00110: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8812.8340 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 111/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.2012\n",
      "Epoch 00111: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8809.0146 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 112/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.0439\n",
      "Epoch 00112: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.5566 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 113/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8305.8838\n",
      "Epoch 00113: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.7695 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4286\n",
      "Epoch 114/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9051.7646\n",
      "Epoch 00114: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.4189 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4268\n",
      "Epoch 115/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8671.9346\n",
      "Epoch 00115: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.5635 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4283\n",
      "Epoch 116/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.4092  ETA: 2s - loss: \n",
      "Epoch 00116: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.9219 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 117/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8685.8936- ETA: 1s - loss: 0.0209 - mae: 0.02\n",
      "Epoch 00117: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8817.3330 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 118/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8302.0400\n",
      "Epoch 00118: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8812.6797 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4707\n",
      "Epoch 00119: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.0967 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 120/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8304.8125\n",
      "Epoch 00120: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8818.3047 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4265\n",
      "Epoch 121/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9058.0273\n",
      "Epoch 00121: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5293 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4270\n",
      "Epoch 122/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8682.1084  ETA: 1s - loss: 0.0208 - mae:\n",
      "Epoch 00122: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.7100 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 123/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9050.8926\n",
      "Epoch 00123: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.5742 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4287\n",
      "Epoch 124/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.2852- ETA: 2s - loss: 0.0209 - mae:\n",
      "Epoch 00124: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.8037 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4277\n",
      "Epoch 125/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.9121\n",
      "Epoch 00125: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.4541 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 126/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7553.2466\n",
      "Epoch 00126: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.8301 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 127/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.8857\n",
      "Epoch 00127: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.4443 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 128/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.3145\n",
      "Epoch 00128: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.7705 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 129/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9062.5234\n",
      "Epoch 00129: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8820.9062 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 130/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8310.6270\n",
      "Epoch 00130: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.6572 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4276\n",
      "Epoch 131/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8676.7832\n",
      "Epoch 00131: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8813.8525 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 132/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.6250\n",
      "Epoch 00132: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1289 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 133/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.2627\n",
      "Epoch 00133: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.7959 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 134/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9385\n",
      "Epoch 00134: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.0283 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 135/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9990\n",
      "Epoch 00135: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.8438 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 136/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9050.7119- ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8224.\n",
      "Epoch 00136: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.3926 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 137/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9065.7979\n",
      "Epoch 00137: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.0713 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 138/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.0449\n",
      "Epoch 00138: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.1436 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4327\n",
      "Epoch 139/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.4678\n",
      "Epoch 00139: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.9834 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 140/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.8877\n",
      "Epoch 00140: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.4102 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 141/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.1895\n",
      "Epoch 00141: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.6465 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4293\n",
      "Epoch 142/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9068.6006\n",
      "Epoch 00142: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8826.8496 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4311\n",
      "Epoch 143/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.5254\n",
      "Epoch 00143: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.0791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8303.7139\n",
      "Epoch 00144: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.9434 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 145/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9057.6035\n",
      "Epoch 00145: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1113 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 146/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8295.9824\n",
      "Epoch 00146: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 147/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8299.4932- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00147: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.4980 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 148/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8677.7822\n",
      "Epoch 00148: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.8496 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 149/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4521\n",
      "Epoch 00149: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8807.3672 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4317\n",
      "Epoch 150/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8687.6270\n",
      "Epoch 00150: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.2520 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 151/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7550.1221\n",
      "Epoch 00151: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5537 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4318\n",
      "Epoch 152/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8665.8770\n",
      "Epoch 00152: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.2012 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4307\n",
      "Epoch 153/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8305.3477\n",
      "Epoch 00153: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.7168 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 154/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8664.9102\n",
      "Epoch 00154: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8803.5449 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 155/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.1807\n",
      "Epoch 00155: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.1172 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4323\n",
      "Epoch 156/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9061.2051\n",
      "Epoch 00156: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8819.6104 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 157/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9058.3799\n",
      "Epoch 00157: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.8613 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4316\n",
      "Epoch 158/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9053.1445\n",
      "Epoch 00158: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8811.7646 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 159/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.0205\n",
      "Epoch 00159: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8813.5781 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 160/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8673.6445\n",
      "Epoch 00160: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8812.8994 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4306\n",
      "Epoch 161/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.0654\n",
      "Epoch 00161: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8804.1348 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 162/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8678.7412\n",
      "Epoch 00162: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8817.2510 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 163/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9058.5049  ETA: 1s - loss: 0.0209 - mae: 0.0204 - map\n",
      "Epoch 00163: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.9990 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4288\n",
      "Epoch 164/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8671.3730\n",
      "Epoch 00164: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8807.5947 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 165/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8304.8350\n",
      "Epoch 00165: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8825.1436 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4294\n",
      "Epoch 166/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7930.7822\n",
      "Epoch 00166: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.1318 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 167/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8676.1992  ETA: 1s - loss: 0.0209 - mae: 0.0203 - mape:  - ETA: 1s - loss: 0.0208 - mae: 0.0202 - mape: - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9\n",
      "Epoch 00167: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.6123 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8682.7363\n",
      "Epoch 00168: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.8896 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 169/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9065.5938\n",
      "Epoch 00169: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8823.9004 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 170/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9051.8340\n",
      "Epoch 00170: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.4756 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 171/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8678.6602- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8967.800\n",
      "Epoch 00171: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8815.0049 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 172/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.9189\n",
      "Epoch 00172: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.3682 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 173/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9055.6904\n",
      "Epoch 00173: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2402 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 174/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.9043- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8588.017\n",
      "Epoch 00174: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.3857 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 175/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.7666\n",
      "Epoch 00175: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3809 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 176/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8673.9277\n",
      "Epoch 00176: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.3027 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4343\n",
      "Epoch 177/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.8848\n",
      "Epoch 00177: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2305 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4321\n",
      "Epoch 178/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9054.9736  ETA: 1s - loss: 0.0208 - mae: 0.0203 \n",
      "Epoch 00178: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.5439 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4330\n",
      "Epoch 179/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8675.4736\n",
      "Epoch 00179: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8809.1797 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4327\n",
      "Epoch 180/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9061.8086\n",
      "Epoch 00180: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2021 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4329\n",
      "Epoch 181/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9060.0869\n",
      "Epoch 00181: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5107 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4319\n",
      "Epoch 182/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.6191\n",
      "Epoch 00182: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5000 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 183/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.7178\n",
      "Epoch 00183: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2656 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4324\n",
      "Epoch 184/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8291.2705\n",
      "Epoch 00184: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.1533 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4331\n",
      "Epoch 185/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.7324\n",
      "Epoch 00185: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.8145 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 186/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.2549\n",
      "Epoch 00186: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8817.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4305\n",
      "Epoch 187/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.2168\n",
      "Epoch 00187: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8819.0938 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4315\n",
      "Epoch 188/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.7939\n",
      "Epoch 00188: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.2949 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 189/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.9834\n",
      "Epoch 00189: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.4678 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 190/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.2012\n",
      "Epoch 00190: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7588 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4332\n",
      "Epoch 191/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8679.1182\n",
      "Epoch 00191: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.3721 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4325\n",
      "Epoch 192/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8303.0508\n",
      "Epoch 00192: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7334 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 193/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8677.2891\n",
      "Epoch 00193: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.0742 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4315\n",
      "Epoch 194/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9051.3916\n",
      "Epoch 00194: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.0596 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4353\n",
      "Epoch 195/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 7911.0254\n",
      "Epoch 00195: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.6299 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 196/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9053.1436\n",
      "Epoch 00196: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.7627 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4345\n",
      "Epoch 197/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9056.7646- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 73\n",
      "Epoch 00197: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.2822 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 198/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8293.9717\n",
      "Epoch 00198: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3867 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4346\n",
      "Epoch 199/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.1162\n",
      "Epoch 00199: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.7646 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4328\n",
      "Epoch 200/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.9668- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 796\n",
      "Epoch 00200: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8808.9336 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4344\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.57\n",
      "Median Absolute Error (MedAE): 338.68\n",
      "Mean Squared Error (MSE): 332645.96\n",
      "Root Mean Squared Error (RMSE): 576.75\n",
      "Mean Absolute Percentage Error (MAPE): 4.3 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.3 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-0013-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aW5pp6LD-1"
   },
   "source": [
    "# E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "j9RoOmzuLgHR"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B2H5iBwQLfhj"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "k06fUuK7LerZ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mB2kXVVCLRBA"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sizKJbygLNBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Q6uDSkmdLJlR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.1973 - mae: 0.0794 - mape: 9692.2637\n",
      "Epoch 00001: val_loss improved from inf to 0.10644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0001-loss0.11.h5\n",
      "32614/32614 [==============================] - 19s 584us/sample - loss: 0.1972 - mae: 0.0794 - mape: 9680.9932 - val_loss: 0.1064 - val_mae: 0.0766 - val_mape: 12.9892\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0557 - mae: 0.0421 - mape: 9249.9512\n",
      "Epoch 00002: val_loss improved from 0.10644 to 0.05840, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0002-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0556 - mae: 0.0420 - mape: 9230.1133 - val_loss: 0.0584 - val_mae: 0.0523 - val_mape: 8.7301\n",
      "Epoch 3/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0416 - mae: 0.0375 - mape: 9262.3770\n",
      "Epoch 00003: val_loss improved from 0.05840 to 0.05401, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0003-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0416 - mae: 0.0375 - mape: 9224.3594 - val_loss: 0.0540 - val_mae: 0.0503 - val_mape: 8.7703\n",
      "Epoch 4/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0398 - mae: 0.0367 - mape: 9258.6729\n",
      "Epoch 00004: val_loss improved from 0.05401 to 0.04371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 431us/sample - loss: 0.0398 - mae: 0.0367 - mape: 9247.8994 - val_loss: 0.0437 - val_mae: 0.0407 - val_mape: 6.7483\n",
      "Epoch 5/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0336 - mape: 9132.0303  ETA: 4s - \n",
      "Epoch 00005: val_loss did not improve from 0.04371\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0362 - mae: 0.0336 - mape: 9121.3975 - val_loss: 0.0456 - val_mae: 0.0433 - val_mape: 7.2883\n",
      "Epoch 6/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0332 - mape: 9247.4834\n",
      "Epoch 00006: val_loss improved from 0.04371 to 0.03429, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0358 - mae: 0.0332 - mape: 9227.6475 - val_loss: 0.0343 - val_mae: 0.0317 - val_mape: 5.5244\n",
      "Epoch 7/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0334 - mae: 0.0314 - mape: 8832.6689\n",
      "Epoch 00007: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0334 - mae: 0.0314 - mape: 9193.4805 - val_loss: 0.0770 - val_mae: 0.0752 - val_mape: 10.8461\n",
      "Epoch 8/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0309 - mape: 9279.7900\n",
      "Epoch 00008: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0326 - mae: 0.0309 - mape: 9241.6865 - val_loss: 0.0911 - val_mae: 0.0895 - val_mape: 12.6461\n",
      "Epoch 9/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0301 - mape: 9240.0088\n",
      "Epoch 00009: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0315 - mae: 0.0301 - mape: 9211.1299 - val_loss: 0.0497 - val_mae: 0.0483 - val_mape: 7.3868\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0289 - mape: 8889.9570\n",
      "Epoch 00010: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0303 - mae: 0.0289 - mape: 9243.3398 - val_loss: 0.0727 - val_mae: 0.0714 - val_mape: 10.6575\n",
      "Epoch 11/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0290 - mape: 9401.6436\n",
      "Epoch 00011: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0303 - mae: 0.0290 - mape: 9372.2676 - val_loss: 0.0601 - val_mae: 0.0589 - val_mape: 8.9566\n",
      "Epoch 12/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0292 - mae: 0.0280 - mape: 9176.0518\n",
      "Epoch 00012: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0292 - mae: 0.0280 - mape: 9147.3730 - val_loss: 0.0464 - val_mae: 0.0452 - val_mape: 7.1787\n",
      "Epoch 13/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0291 - mae: 0.0279 - mape: 9234.7246\n",
      "Epoch 00013: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0291 - mae: 0.0279 - mape: 9214.9170 - val_loss: 0.0398 - val_mae: 0.0387 - val_mape: 6.3188\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.0278 - mape: 9259.1611\n",
      "Epoch 00014: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0290 - mae: 0.0278 - mape: 9221.1396 - val_loss: 0.0613 - val_mae: 0.0602 - val_mape: 8.9035\n",
      "Epoch 15/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0285 - mae: 0.0274 - mape: 9332.2979\n",
      "Epoch 00015: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0285 - mae: 0.0274 - mape: 9293.9756 - val_loss: 0.0448 - val_mae: 0.0438 - val_mape: 6.8179\n",
      "Epoch 16/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9270.8516- ETA: 0s - loss: 0.0278 - mae: 0.0268 - ma\n",
      "Epoch 00016: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9269.1475 - val_loss: 0.0455 - val_mae: 0.0446 - val_mape: 7.0034\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9212.3809\n",
      "Epoch 00017: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9210.6865 - val_loss: 0.0670 - val_mae: 0.0661 - val_mape: 9.5463\n",
      "Epoch 18/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9337.9092\n",
      "Epoch 00018: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9308.7227 - val_loss: 0.0456 - val_mae: 0.0447 - val_mape: 7.0638\n",
      "Epoch 19/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9224.4463\n",
      "Epoch 00019: val_loss improved from 0.03429 to 0.03249, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0019-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9213.7041 - val_loss: 0.0325 - val_mae: 0.0316 - val_mape: 5.3572\n",
      "Epoch 20/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9302.4258\n",
      "Epoch 00020: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9264.2266 - val_loss: 0.0382 - val_mae: 0.0373 - val_mape: 6.0539\n",
      "Epoch 21/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9244.2197\n",
      "Epoch 00021: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 436us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9206.2666 - val_loss: 0.0409 - val_mae: 0.0400 - val_mape: 6.2748\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0260 - mape: 9348.6699\n",
      "Epoch 00022: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0269 - mae: 0.0260 - mape: 9337.7842 - val_loss: 0.0328 - val_mae: 0.0319 - val_mape: 5.4575\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9254.0684\n",
      "Epoch 00023: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0266 - mae: 0.0257 - mape: 9252.3672 - val_loss: 0.0552 - val_mae: 0.0543 - val_mape: 8.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9298.8604\n",
      "Epoch 00024: val_loss improved from 0.03249 to 0.03114, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0024-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0265 - mae: 0.0257 - mape: 9260.6738 - val_loss: 0.0311 - val_mae: 0.0303 - val_mape: 5.1846\n",
      "Epoch 25/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0254 - mape: 9225.4922\n",
      "Epoch 00025: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0262 - mae: 0.0254 - mape: 9223.7969 - val_loss: 0.0519 - val_mae: 0.0511 - val_mape: 7.9640\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0254 - mape: 9168.1641\n",
      "Epoch 00026: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0263 - mae: 0.0254 - mape: 9139.5078 - val_loss: 0.0324 - val_mae: 0.0315 - val_mape: 5.4029\n",
      "Epoch 27/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0252 - mape: 9292.8506\n",
      "Epoch 00027: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 435us/sample - loss: 0.0261 - mae: 0.0252 - mape: 9282.0293 - val_loss: 0.0368 - val_mae: 0.0359 - val_mape: 5.8316\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0250 - mape: 9257.2148\n",
      "Epoch 00028: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0259 - mae: 0.0250 - mape: 9246.4346 - val_loss: 0.0322 - val_mae: 0.0313 - val_mape: 5.3347\n",
      "Epoch 29/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0249 - mape: 9201.8809\n",
      "Epoch 00029: val_loss improved from 0.03114 to 0.03017, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0257 - mae: 0.0249 - mape: 9173.1152 - val_loss: 0.0302 - val_mae: 0.0293 - val_mape: 5.0982\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0246 - mape: 9224.1299\n",
      "Epoch 00030: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0255 - mae: 0.0246 - mape: 9195.2969 - val_loss: 0.0316 - val_mae: 0.0308 - val_mape: 5.3475\n",
      "Epoch 31/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0247 - mape: 9221.6504\n",
      "Epoch 00031: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0256 - mae: 0.0247 - mape: 9219.9551 - val_loss: 0.0383 - val_mae: 0.0375 - val_mape: 6.0447\n",
      "Epoch 32/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9185.3818\n",
      "Epoch 00032: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0253 - mae: 0.0244 - mape: 9156.6689 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.6061\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9151.5020\n",
      "Epoch 00033: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0252 - mae: 0.0243 - mape: 9104.9512 - val_loss: 0.0435 - val_mae: 0.0426 - val_mape: 6.6821\n",
      "Epoch 34/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0240 - mape: 9209.8779\n",
      "Epoch 00034: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0248 - mae: 0.0240 - mape: 9163.0439 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 5.9090\n",
      "Epoch 35/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0243 - mape: 9190.4863\n",
      "Epoch 00035: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0251 - mae: 0.0243 - mape: 9170.7715 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.4763\n",
      "Epoch 36/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0241 - mape: 9201.8359\n",
      "Epoch 00036: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0250 - mae: 0.0241 - mape: 9182.0967 - val_loss: 0.0393 - val_mae: 0.0384 - val_mape: 6.1283\n",
      "Epoch 37/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0247 - mae: 0.0238 - mape: 9064.8945\n",
      "Epoch 00037: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0247 - mae: 0.0238 - mape: 9063.2285 - val_loss: 0.0322 - val_mae: 0.0314 - val_mape: 5.4603\n",
      "Epoch 38/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0236 - mape: 9153.4014\n",
      "Epoch 00038: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0245 - mae: 0.0236 - mape: 9142.7412 - val_loss: 0.0396 - val_mae: 0.0387 - val_mape: 6.1510\n",
      "Epoch 39/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9212.5186\n",
      "Epoch 00039: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9183.7207 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5752\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0237 - mape: 9298.7363\n",
      "Epoch 00040: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0246 - mae: 0.0237 - mape: 9278.7891 - val_loss: 0.0358 - val_mae: 0.0349 - val_mape: 5.7883\n",
      "Epoch 41/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9290.7383\n",
      "Epoch 00041: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9252.5859 - val_loss: 0.0359 - val_mae: 0.0350 - val_mape: 6.1253\n",
      "Epoch 42/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0244 - mae: 0.0235 - mape: 9100.7422\n",
      "Epoch 00042: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0244 - mae: 0.0235 - mape: 9099.0762 - val_loss: 0.0365 - val_mae: 0.0356 - val_mape: 5.8232\n",
      "Epoch 43/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.0231 - mape: 9206.7373\n",
      "Epoch 00043: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0240 - mae: 0.0231 - mape: 9186.9854 - val_loss: 0.0428 - val_mae: 0.0419 - val_mape: 6.5949\n",
      "Epoch 44/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.0229 - mape: 9111.2773\n",
      "Epoch 00044: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0238 - mae: 0.0229 - mape: 9109.6025 - val_loss: 0.0335 - val_mae: 0.0326 - val_mape: 5.5624\n",
      "Epoch 45/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0229 - mape: 9251.8613\n",
      "Epoch 00045: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0229 - mape: 9250.1602 - val_loss: 0.0383 - val_mae: 0.0374 - val_mape: 6.0213\n",
      "Epoch 46/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0228 - mape: 9249.3652\n",
      "Epoch 00046: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9229.5244 - val_loss: 0.0470 - val_mae: 0.0461 - val_mape: 7.1372\n",
      "Epoch 47/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0228 - mape: 9240.2764\n",
      "Epoch 00047: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9220.4561 - val_loss: 0.0355 - val_mae: 0.0346 - val_mape: 5.8071\n",
      "Epoch 48/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0227 - mape: 9228.7842\n",
      "Epoch 00048: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0236 - mae: 0.0227 - mape: 9227.0869 - val_loss: 0.0335 - val_mae: 0.0327 - val_mape: 5.7051\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0225 - mape: 9308.8867\n",
      "Epoch 00049: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0234 - mae: 0.0225 - mape: 9279.7959 - val_loss: 0.0391 - val_mae: 0.0382 - val_mape: 6.2820\n",
      "Epoch 50/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0223 - mape: 8910.0205\n",
      "Epoch 00050: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0232 - mae: 0.0223 - mape: 9253.5254 - val_loss: 0.0347 - val_mae: 0.0338 - val_mape: 5.7545\n",
      "Epoch 51/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0224 - mape: 9279.0811\n",
      "Epoch 00051: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0233 - mae: 0.0224 - mape: 9277.3750 - val_loss: 0.0390 - val_mae: 0.0381 - val_mape: 6.1803\n",
      "Epoch 52/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0222 - mape: 9221.8008\n",
      "Epoch 00052: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0231 - mae: 0.0222 - mape: 9202.0244 - val_loss: 0.0355 - val_mae: 0.0347 - val_mape: 5.8992\n",
      "Epoch 53/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0218 - mape: 9313.6279\n",
      "Epoch 00053: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0227 - mae: 0.0219 - mape: 9284.5176 - val_loss: 0.0346 - val_mae: 0.0337 - val_mape: 5.8458\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0221 - mape: 9235.9512\n",
      "Epoch 00054: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0230 - mae: 0.0221 - mape: 9207.0791 - val_loss: 0.0399 - val_mae: 0.0391 - val_mape: 6.3575\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.0219 - mape: 9211.0430\n",
      "Epoch 00055: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0228 - mae: 0.0219 - mape: 9191.2910 - val_loss: 0.0397 - val_mae: 0.0388 - val_mape: 6.2600\n",
      "Epoch 56/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0217 - mape: 9251.5020\n",
      "Epoch 00056: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0226 - mae: 0.0217 - mape: 9240.7275 - val_loss: 0.0405 - val_mae: 0.0397 - val_mape: 6.3565\n",
      "Epoch 57/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 9213.3115\n",
      "Epoch 00057: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9166.4385 - val_loss: 0.0343 - val_mae: 0.0334 - val_mape: 5.7472\n",
      "Epoch 58/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0216 - mape: 9254.9170\n",
      "Epoch 00058: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9253.2207 - val_loss: 0.0376 - val_mae: 0.0367 - val_mape: 6.0893\n",
      "Epoch 59/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0220 - mape: 9354.7930\n",
      "Epoch 00059: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0230 - mae: 0.0220 - mape: 9325.5488 - val_loss: 0.0341 - val_mae: 0.0332 - val_mape: 5.7046\n",
      "Epoch 60/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0214 - mape: 9235.1680\n",
      "Epoch 00060: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0222 - mae: 0.0214 - mape: 9188.1836 - val_loss: 0.0366 - val_mae: 0.0358 - val_mape: 5.9825\n",
      "Epoch 61/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0215 - mape: 9196.2959\n",
      "Epoch 00061: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0223 - mae: 0.0215 - mape: 9167.5479 - val_loss: 0.0399 - val_mae: 0.0390 - val_mape: 6.2796\n",
      "Epoch 62/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0213 - mape: 9107.4619\n",
      "Epoch 00062: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0222 - mae: 0.0213 - mape: 9087.9248 - val_loss: 0.0385 - val_mae: 0.0376 - val_mape: 6.1628\n",
      "Epoch 63/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0213 - mape: 9210.2637\n",
      "Epoch 00063: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0221 - mae: 0.0213 - mape: 9172.4463 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5972\n",
      "Epoch 64/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9124.6074\n",
      "Epoch 00064: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9122.9297 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 6.0281\n",
      "Epoch 65/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0212 - mape: 9226.6611\n",
      "Epoch 00065: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0220 - mae: 0.0212 - mape: 9188.7695 - val_loss: 0.0377 - val_mae: 0.0369 - val_mape: 6.2115\n",
      "Epoch 66/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9052.7314\n",
      "Epoch 00066: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9051.0664 - val_loss: 0.0405 - val_mae: 0.0396 - val_mape: 6.4286\n",
      "Epoch 67/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0209 - mape: 9083.4209\n",
      "Epoch 00067: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0218 - mae: 0.0209 - mape: 9072.8418 - val_loss: 0.0365 - val_mae: 0.0357 - val_mape: 6.0204\n",
      "Epoch 68/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9076.5205\n",
      "Epoch 00068: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9039.2510 - val_loss: 0.0342 - val_mae: 0.0334 - val_mape: 5.6559\n",
      "Epoch 69/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0208 - mape: 9033.5391\n",
      "Epoch 00069: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0217 - mae: 0.0208 - mape: 9005.3018 - val_loss: 0.0396 - val_mae: 0.0388 - val_mape: 6.4502\n",
      "Epoch 70/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9120.9199\n",
      "Epoch 00070: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9083.4629 - val_loss: 0.0361 - val_mae: 0.0353 - val_mape: 5.8008\n",
      "Epoch 71/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9097.3145\n",
      "Epoch 00071: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9095.6416 - val_loss: 0.0361 - val_mae: 0.0352 - val_mape: 5.9642\n",
      "Epoch 72/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0206 - mape: 9088.2021\n",
      "Epoch 00072: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0206 - mape: 9086.5312 - val_loss: 0.0380 - val_mae: 0.0372 - val_mape: 6.1542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9105.6396\n",
      "Epoch 00073: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9059.3135 - val_loss: 0.0434 - val_mae: 0.0426 - val_mape: 6.7060\n",
      "Epoch 74/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0205 - mape: 9066.7041\n",
      "Epoch 00074: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0205 - mape: 9065.0361 - val_loss: 0.0346 - val_mae: 0.0338 - val_mape: 5.7416\n",
      "Epoch 75/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0205 - mape: 8978.4404\n",
      "Epoch 00075: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0213 - mae: 0.0205 - mape: 8967.9941 - val_loss: 0.0348 - val_mae: 0.0340 - val_mape: 5.9216\n",
      "Epoch 76/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9057.9648\n",
      "Epoch 00076: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0210 - mae: 0.0202 - mape: 9056.2988 - val_loss: 0.0398 - val_mae: 0.0390 - val_mape: 6.3300\n",
      "Epoch 77/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0203 - mape: 8981.2236\n",
      "Epoch 00077: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0212 - mae: 0.0203 - mape: 8979.5723 - val_loss: 0.0451 - val_mae: 0.0444 - val_mape: 7.0400\n",
      "Epoch 78/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9038.3955\n",
      "Epoch 00078: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0210 - mae: 0.0202 - mape: 8992.4121 - val_loss: 0.0360 - val_mae: 0.0353 - val_mape: 5.9399\n",
      "Epoch 79/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0201 - mape: 8948.2764\n",
      "Epoch 00079: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0209 - mae: 0.0201 - mape: 8920.3037 - val_loss: 0.0350 - val_mae: 0.0342 - val_mape: 5.7982\n",
      "Epoch 80/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0200 - mape: 8990.1953\n",
      "Epoch 00080: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0208 - mae: 0.0200 - mape: 8962.0898 - val_loss: 0.0358 - val_mae: 0.0350 - val_mape: 5.8451\n"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "cW3NWXtXLDA-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 420.56\n",
      "Median Absolute Error (MedAE): 333.06\n",
      "Mean Squared Error (MSE): 313586.16\n",
      "Root Mean Squared Error (RMSE): 559.99\n",
      "Mean Absolute Percentage Error (MAPE): 4.19 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.25 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5'\n",
    "start_epoch= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0228 - mape: 9138.6191- ETA: 3s - loss: 0.023\n",
      "Epoch 00001: val_loss improved from inf to 0.03148, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 56s 2ms/sample - loss: 0.0235 - mae: 0.0228 - mape: 9136.9395 - val_loss: 0.0315 - val_mae: 0.0307 - val_mape: 5.2145\n",
      "Epoch 2/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0225 - mape: 9164.0752\n",
      "Epoch 00002: val_loss improved from 0.03148 to 0.03063, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0232 - mae: 0.0225 - mape: 9162.3906 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1229\n",
      "Epoch 3/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0224 - mape: 9196.9521- ETA: 3s - loss: 0.0\n",
      "Epoch 00003: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0231 - mae: 0.0224 - mape: 9195.2607 - val_loss: 0.0314 - val_mae: 0.0307 - val_mape: 5.2105\n",
      "Epoch 4/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0223 - mape: 9131.6660\n",
      "Epoch 00004: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0230 - mae: 0.0223 - mape: 9121.0312 - val_loss: 0.0310 - val_mae: 0.0304 - val_mape: 5.1898\n",
      "Epoch 5/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.0223 - mape: 9159.3213\n",
      "Epoch 00005: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0229 - mae: 0.0223 - mape: 9148.6533 - val_loss: 0.0321 - val_mae: 0.0314 - val_mape: 5.2817\n",
      "Epoch 6/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9180.6631\n",
      "Epoch 00006: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0228 - mae: 0.0221 - mape: 9178.9824 - val_loss: 0.0309 - val_mae: 0.0303 - val_mape: 5.1679\n",
      "Epoch 7/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9175.2949\n",
      "Epoch 00007: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0227 - mae: 0.0221 - mape: 9173.6084 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.2847\n",
      "Epoch 8/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0220 - mape: 9205.9355\n",
      "Epoch 00008: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0226 - mae: 0.0220 - mape: 9195.2217 - val_loss: 0.0307 - val_mae: 0.0301 - val_mape: 5.2188\n",
      "Epoch 9/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0219 - mape: 9181.9873\n",
      "Epoch 00009: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0225 - mae: 0.0219 - mape: 9180.2988 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.2595\n",
      "Epoch 10/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0218 - mape: 9186.6875\n",
      "Epoch 00010: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0224 - mae: 0.0218 - mape: 9184.9990 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2146\n",
      "Epoch 11/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0217 - mape: 9203.6816\n",
      "Epoch 00011: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0223 - mae: 0.0217 - mape: 9192.9629 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.2609\n",
      "Epoch 12/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9241.5742\n",
      "Epoch 00012: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 9239.8750 - val_loss: 0.0311 - val_mae: 0.0305 - val_mape: 5.2300\n",
      "Epoch 13/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0216 - mape: 9247.9932\n",
      "Epoch 00013: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0216 - mape: 9237.2227 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4177\n",
      "Epoch 14/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0215 - mape: 9297.9727\n",
      "Epoch 00014: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0215 - mape: 9296.2627 - val_loss: 0.0307 - val_mae: 0.0302 - val_mape: 5.2173\n",
      "Epoch 15/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0214 - mape: 9263.5205\n",
      "Epoch 00015: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0220 - mae: 0.0214 - mape: 9261.8164 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2651\n",
      "Epoch 16/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 9198.6846\n",
      "Epoch 00016: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 9196.9941 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.3174\n",
      "Epoch 17/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 9260.9404\n",
      "Epoch 00017: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 9250.1543 - val_loss: 0.0309 - val_mae: 0.0304 - val_mape: 5.2860\n",
      "Epoch 18/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0212 - mape: 9216.3271\n",
      "Epoch 00018: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0212 - mape: 9214.6318 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3829\n",
      "Epoch 19/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0211 - mape: 9211.0586\n",
      "Epoch 00019: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0211 - mape: 9200.3320 - val_loss: 0.0341 - val_mae: 0.0336 - val_mape: 5.5767\n",
      "Epoch 20/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0209 - mape: 9201.0938\n",
      "Epoch 00020: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0215 - mae: 0.0209 - mape: 9190.3779 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3684\n",
      "Epoch 21/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9219.9883\n",
      "Epoch 00021: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0214 - mae: 0.0208 - mape: 9218.2930 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3618\n",
      "Epoch 22/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 9187.7080\n",
      "Epoch 00022: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 9186.0186 - val_loss: 0.0316 - val_mae: 0.0310 - val_mape: 5.3197\n",
      "Epoch 23/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9196.0430\n",
      "Epoch 00023: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0206 - mape: 9194.3525 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2367\n",
      "Epoch 24/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9184.2910\n",
      "Epoch 00024: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 9173.5947 - val_loss: 0.0315 - val_mae: 0.0309 - val_mape: 5.2920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9166.1699\n",
      "Epoch 00025: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0210 - mae: 0.0205 - mape: 9155.4951 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.3492\n",
      "Epoch 26/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9205.6270\n",
      "Epoch 00026: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9203.9385 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2379\n",
      "Epoch 27/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9117.5986\n",
      "Epoch 00027: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9115.9219 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4364\n",
      "Epoch 28/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9204.3340\n",
      "Epoch 00028: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9193.6143 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2468\n",
      "Epoch 29/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9128.1553\n",
      "Epoch 00029: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9126.4766 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3819\n",
      "Epoch 30/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0202 - mape: 9126.3975\n",
      "Epoch 00030: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0207 - mae: 0.0202 - mape: 9124.7197 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.3127\n",
      "Epoch 31/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9127.3574\n",
      "Epoch 00031: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9125.6787 - val_loss: 0.0314 - val_mae: 0.0308 - val_mape: 5.3096\n",
      "Epoch 32/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9139.1289\n",
      "Epoch 00032: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9137.4482 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3143\n",
      "Epoch 33/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0200 - mape: 9129.0029\n",
      "Epoch 00033: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0205 - mae: 0.0200 - mape: 9127.3242 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3023\n",
      "Epoch 34/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9085.1152\n",
      "Epoch 00034: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9083.4443 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3421\n",
      "Epoch 35/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9161.0391\n",
      "Epoch 00035: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9159.3545 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2771\n",
      "Epoch 36/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0198 - mape: 9147.9355\n",
      "Epoch 00036: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0203 - mae: 0.0198 - mape: 9137.2803 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3513\n",
      "Epoch 37/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9170.7051- ETA: 4s - loss: 0. - ETA: 2s - loss: 0.0203\n",
      "Epoch 00037: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9169.0186 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2897\n",
      "Epoch 38/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9186.7959\n",
      "Epoch 00038: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9176.0967 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2613\n",
      "Epoch 39/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0195 - mape: 9121.9736\n",
      "Epoch 00039: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0200 - mae: 0.0195 - mape: 9120.2959 - val_loss: 0.0348 - val_mae: 0.0343 - val_mape: 5.6403\n",
      "Epoch 40/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0194 - mape: 9143.8574\n",
      "Epoch 00040: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0199 - mae: 0.0194 - mape: 9142.1768 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3696\n",
      "Epoch 41/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0192 - mape: 9166.4883\n",
      "Epoch 00041: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0197 - mae: 0.0192 - mape: 9155.8125 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3686\n",
      "Epoch 42/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9157.3496\n",
      "Epoch 00042: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9146.6846 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3299\n",
      "Epoch 43/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9169.7188\n",
      "Epoch 00043: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9159.0391 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2889\n",
      "Epoch 44/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0190 - mape: 9109.2627\n",
      "Epoch 00044: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0195 - mae: 0.0190 - mape: 9107.5879 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3664\n",
      "Epoch 45/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0189 - mape: 9101.7988\n",
      "Epoch 00045: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0195 - mae: 0.0189 - mape: 9100.1250 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4306\n",
      "Epoch 46/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0194 - mae: 0.0189 - mape: 9152.3506\n",
      "Epoch 00046: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0194 - mae: 0.0189 - mape: 9150.6680 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.3450\n",
      "Epoch 47/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0193 - mae: 0.0187 - mape: 9092.9639\n",
      "Epoch 00047: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0193 - mae: 0.0187 - mape: 9091.2920 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3590\n",
      "Epoch 48/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.0186 - mape: 9166.8369\n",
      "Epoch 00048: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0191 - mae: 0.0186 - mape: 9165.1514 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4518\n",
      "Epoch 49/200\n",
      "11776/32614 [=========>....................] - ETA: 32s - loss: 0.0191 - mae: 0.0186 - mape: 7337.6323"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-048f600f12b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 412.44\n",
      "Median Absolute Error (MedAE): 328.08\n",
      "Mean Squared Error (MSE): 301264.17\n",
      "Root Mean Squared Error (RMSE): 548.88\n",
      "Mean Absolute Percentage Error (MAPE): 4.12 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.19 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5'\n",
    "start_epoch= 30+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0218 - mape: 9021.4141\n",
      "Epoch 00001: val_loss improved from inf to 0.03075, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 7s 222us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9155.7246 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1276\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9392.5967\n",
      "Epoch 00002: val_loss improved from 0.03075 to 0.03067, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9142.1660 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1187\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 8595.9766\n",
      "Epoch 00003: val_loss improved from 0.03067 to 0.03054, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9143.0762 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1073\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9385.6611\n",
      "Epoch 00004: val_loss improved from 0.03054 to 0.03050, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9135.4072 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1036\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9377.5986\n",
      "Epoch 00005: val_loss improved from 0.03050 to 0.03046, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9127.5752 - val_loss: 0.0305 - val_mae: 0.0297 - val_mape: 5.0995\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8606.6309\n",
      "Epoch 00006: val_loss improved from 0.03046 to 0.03042, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9137.0928 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0965\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9375.1191\n",
      "Epoch 00007: val_loss improved from 0.03042 to 0.03041, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9125.1416 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9374.5898\n",
      "Epoch 00008: val_loss improved from 0.03041 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9124.6318 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0924\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8210.0459\n",
      "Epoch 00009: val_loss improved from 0.03038 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9126.5254 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0921\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8572.6328  ETA: 1s - loss: 0.0225 - mae: 0.0218 \n",
      "Epoch 00010: val_loss improved from 0.03038 to 0.03035, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9118.1846 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0894\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8207.2930\n",
      "Epoch 00011: val_loss did not improve from 0.03035\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9111.9365 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0895\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9357.2139\n",
      "Epoch 00012: val_loss improved from 0.03035 to 0.03034, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0012-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.7373 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0893\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8967.2090\n",
      "Epoch 00013: val_loss improved from 0.03034 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0859 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0887\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8584.7832\n",
      "Epoch 00014: val_loss improved from 0.03033 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0014-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9115.9170 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9359.8037\n",
      "Epoch 00015: val_loss improved from 0.03033 to 0.03030, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0015-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.2441 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0861\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8970.0029\n",
      "Epoch 00016: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9116.9443 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0868\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8182.7358\n",
      "Epoch 00017: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.9473 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0872\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9360.5244\n",
      "Epoch 00018: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.9551 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8964.7793\n",
      "Epoch 00019: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0879 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8972.5371\n",
      "Epoch 00020: val_loss improved from 0.03030 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0020-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9106.9902 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0856\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8572.1016\n",
      "Epoch 00021: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9105.2520 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0876\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9354.3223\n",
      "Epoch 00022: val_loss improved from 0.03028 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.9277 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0858\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9355.4004- ETA: 1s - loss: 0.0223 - mae: 0.0216 - mape: 9009 - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 869\n",
      "Epoch 00023: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9105.9697 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9358.2070\n",
      "Epoch 00024: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9108.6885 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0860\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8967.9619\n",
      "Epoch 00025: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.4453 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8956.2822\n",
      "Epoch 00026: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.1289 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0879\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9353.0088\n",
      "Epoch 00027: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9103.6240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.5430\n",
      "Epoch 00028: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1504 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0871\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9354.4570\n",
      "Epoch 00029: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.0547 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0886\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.4277\n",
      "Epoch 00030: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.0498 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2637\n",
      "Epoch 00031: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0391 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0890\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.9912\n",
      "Epoch 00032: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.1416 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0896\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8960.5801\n",
      "Epoch 00033: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7783 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0913\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9346.8809\n",
      "Epoch 00034: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9097.6689 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8965.5186\n",
      "Epoch 00035: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8569.2568\n",
      "Epoch 00036: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1602 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0915\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.2012  ETA: 2s - loss: 0.02\n",
      "Epoch 00037: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1436 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0920\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8574.7637\n",
      "Epoch 00038: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.4717 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0902\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.5205\n",
      "Epoch 00039: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.2197 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.8828\n",
      "Epoch 00040: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.5615 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.7637\n",
      "Epoch 00041: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.3633 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0908\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9359.6875\n",
      "Epoch 00042: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0914\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.9414\n",
      "Epoch 00043: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9099.0850 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3818\n",
      "Epoch 00044: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9072 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0957\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2998\n",
      "Epoch 00045: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0811 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8428- ETA: 2s - loss: 0.0223 - mae: 0\n",
      "Epoch 00046: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9570 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0959\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.5625\n",
      "Epoch 00047: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.1621 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0927\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8184\n",
      "Epoch 00048: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.2930 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0955\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8958.4678- ETA: 2s - loss: 0.0224 - mae: 0.0217 - mape: 4847. - ETA: 1s - loss: 0.0222 - mae: 0.0215 \n",
      "Epoch 00049: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.8057 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0937\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8977.0430\n",
      "Epoch 00050: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.9639 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0950\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.6865\n",
      "Epoch 00051: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.2705 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0954\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8964.1064\n",
      "Epoch 00052: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.1123 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0919\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.2607\n",
      "Epoch 00053: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.9170 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0989\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8959.4570\n",
      "Epoch 00054: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.2852 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0969\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.4199- ETA: 2s - loss: 0.0225 - mae: 0\n",
      "Epoch 00055: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.2910 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0990\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6221\n",
      "Epoch 00056: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1025 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0925\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.5264\n",
      "Epoch 00057: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8096 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1009\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8963.5762\n",
      "Epoch 00058: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.3652 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1010\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8584.6045\n",
      "Epoch 00059: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9108.2227 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0963\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.8242\n",
      "Epoch 00060: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.3945 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0996\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3730  ETA: 2s - loss: 0.0\n",
      "Epoch 00061: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8975 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1003\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.2998\n",
      "Epoch 00062: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9113.6562 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0987\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8572.1582\n",
      "Epoch 00063: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.7090 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1015\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6914\n",
      "Epoch 00064: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1013\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.9131\n",
      "Epoch 00065: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.5430 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1008\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9361.4287\n",
      "Epoch 00066: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9111.8457 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1032\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8968.2002\n",
      "Epoch 00067: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.6504 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1028\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.6250\n",
      "Epoch 00068: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7305 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8962.6729- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8456.346\n",
      "Epoch 00069: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9107.0010 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1072\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.2842\n",
      "Epoch 00070: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.8740 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1061\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.1514\n",
      "Epoch 00071: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9108.2100 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1067\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9350.1494\n",
      "Epoch 00072: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9100.8525 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.5088\n",
      "Epoch 00073: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9105.0918 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1070\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8561.8984\n",
      "Epoch 00074: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1073\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.5430\n",
      "Epoch 00075: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.3232 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1101\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.6846\n",
      "Epoch 00076: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.0889 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1018\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.6553\n",
      "Epoch 00077: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.3877 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1131\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.4307\n",
      "Epoch 00078: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.1025 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1066\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8972.5918\n",
      "Epoch 00079: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7490 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.0195\n",
      "Epoch 00080: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.7041 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1059\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.1738\n",
      "Epoch 00081: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.8281 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1086\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9359.2744\n",
      "Epoch 00082: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7461 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1125\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9355.9951\n",
      "Epoch 00083: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.5498 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1091\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.0264\n",
      "Epoch 00084: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9111.4297 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8973.9590\n",
      "Epoch 00085: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.7012 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1117\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8980.7080\n",
      "Epoch 00086: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.9180 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1181\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9362.3105\n",
      "Epoch 00087: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.6982 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1146\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.5547\n",
      "Epoch 00088: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1924 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.5771\n",
      "Epoch 00089: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.1777 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1158\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.7559\n",
      "Epoch 00090: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.4727 - val_loss: 0.0304 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9344.5049\n",
      "Epoch 00091: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9095.3486 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1201\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.3213\n",
      "Epoch 00092: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.6084 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1109\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.4619\n",
      "Epoch 00093: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1074 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9357.2207\n",
      "Epoch 00094: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.7432 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1149\n",
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.8730\n",
      "Epoch 00095: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.5840 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9366.0117\n",
      "Epoch 00096: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9116.2979 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1090\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 7807.1338- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape:\n",
      "Epoch 00097: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9097.1768 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8969.1562\n",
      "Epoch 00098: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9108.1855 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1151\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9350.9482  ETA\n",
      "Epoch 00099: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.6289 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1170\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.7568\n",
      "Epoch 00100: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.3867 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1185\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.9570\n",
      "Epoch 00101: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.5547 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.7695- ETA: 1s - loss: 0.0222 - mae: 0.0215 - ma\n",
      "Epoch 00102: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.2959 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1222\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9360.5537\n",
      "Epoch 00103: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.9805 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1187\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8967.6729\n",
      "Epoch 00104: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1299 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1163\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.5879\n",
      "Epoch 00105: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.3262 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1221\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.9297\n",
      "Epoch 00106: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9114.2559 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1136\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8575.9648\n",
      "Epoch 00107: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.5449 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1218\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.1768\n",
      "Epoch 00108: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9098.9189 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1236\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.6055\n",
      "Epoch 00109: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.2510 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1214\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0214 - mape: 9352.8691\n",
      "Epoch 00110: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0221 - mae: 0.0215 - mape: 9103.5098 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1184\n",
      "Epoch 111/200\n",
      "23552/32614 [====================>.........] - ETA: 0s - loss: 0.0220 - mae: 0.0213 - mape: 9332.0186"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-aa18ff9589c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 411.01\n",
      "Median Absolute Error (MedAE): 326.15\n",
      "Mean Squared Error (MSE): 300114.5\n",
      "Root Mean Squared Error (RMSE): 547.83\n",
      "Mean Absolute Percentage Error (MAPE): 4.11 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.17 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-0022-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
