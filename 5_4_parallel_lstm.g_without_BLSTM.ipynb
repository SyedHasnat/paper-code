{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tWP4R-BZmaZi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\pyimagesearch')\n",
    "from timeseires.CNN1D.GoogLeNet1D import PC\n",
    "from timeseires.utils.to_split import to_split\n",
    "#from timeseires.callbacks.EpochCheckpoint import EpochCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeseires.callbacks.TrainingMonitor import TrainingMonitor\n",
    "from timeseires.utils.load_only_multi_step import load_only_multi_step\n",
    "from timeseires.utils.multivariate_multi_step import multivariate_multi_step\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD  \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,MaxPooling1D,Concatenate,AveragePooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "import pandas as pd\n",
    "import time, pickle\n",
    "import pydot\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, Reshape, Dense, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten, LeakyReLU, concatenate\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12130, 21), (24259, 21))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tr = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_train.csv'\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values \n",
    "\n",
    "path_v = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_validation.csv'\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "\n",
    "path_te = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_test.csv'\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "#.......................................................................................................\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_scaler.pkl\", 'rb'))\n",
    "df_te.shape,df_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nmYgpwTPnsf4"
   },
   "outputs": [],
   "source": [
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIYjXnV81ZHC",
    "outputId": "4520a0c1-81f1-4cf6-9b17-d15f5ae9c7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.34510064125061035 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X, train_y = multivariate_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24,ahead=0)\n",
    "validation_X, validation_y = multivariate_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24,ahead=0)\n",
    "test_X, test_y = multivariate_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24,ahead=0)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.2535519599914551 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X_load, train_y_load = load_only_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24)\n",
    "validation_X_load, validation_y_load = load_only_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24)\n",
    "test_X_load, test_y_load = load_only_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PbWIXWwSJN3k"
   },
   "outputs": [],
   "source": [
    "#EXP 1 STEPS 24 LOOKBACK 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import the necessary packages\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None, kernel_initializer='lecun_normal'):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"selu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = PC.conv_module(inputs, 128, 5, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(1)(x)\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_goog():   \n",
    "    inputs_combine = Input(shape=(time_steps, num_features))\n",
    "    lstm1   = LSTM(20, return_sequences = True)(inputs_combine)\n",
    "    lstm2   = LSTM(20)(lstm1)\n",
    "    #--------------------------------------------------------\n",
    "    inputs_load = Input(shape=(time_steps, 1))\n",
    "    #l = Bidirectional(LSTM(32, return_sequences=True))(inputs_load)\n",
    "   # l = Bidirectional(LSTM(32, return_sequences=True))(l)\n",
    "    \n",
    "    c11 = PC.inception_module(inputs_load, 32, 32, 64, 8, 32, \"1a\", 0.01)\n",
    "    c12 = Concatenate(axis=-1)([inputs_load, c11])\n",
    "    \n",
    "    c21 = PC.inception_module(c12, 64, 64, 128, 16, 64, \"2a\", 0.01)\n",
    "    c22 = Concatenate(axis=-1)([ c11, c21])\n",
    "    \n",
    "    c31 = PC.inception_module(c22, 64, 64, 128, 16, 64, \"3a\", 0.01)\n",
    "    #---------------------------------------------------------\n",
    "    y = Concatenate(axis=-1)([inputs_load, c21, c31])\n",
    "\n",
    "    #y= PC.conv_module(y, 128, 3, 1)\n",
    "    x = Flatten()(y)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([x, lstm2])\n",
    "    \n",
    "    x = Dense(24, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inputs_load,inputs_combine], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = review_goog()\n",
    "tensorflow.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 24, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_conv (Conv1D)        (None, 24, 32)       64          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_conv (Conv1D)         (None, 24, 8)        16          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_bn (BatchNormalizati (None, 24, 32)       128         1a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_bn (BatchNormalizatio (None, 24, 8)        32          1a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_act (Activation)     (None, 24, 32)       0           1a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_act (Activation)      (None, 24, 8)        0           1a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_conv (Conv1D)          (None, 24, 32)       64          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_conv (Conv1D)        (None, 24, 64)       6208        1a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_conv (Conv1D)         (None, 24, 32)       1312        1a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_bn (BatchNormalization (None, 24, 32)       128         1a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_bn (BatchNormalizati (None, 24, 64)       256         1a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_bn (BatchNormalizatio (None, 24, 32)       128         1a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_act (Activation)       (None, 24, 32)       0           1a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_act (Activation)     (None, 24, 64)       0           1a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_act (Activation)      (None, 24, 32)       0           1a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_mixed (Concatenate)          (None, 24, 128)      0           1a_first_act[0][0]               \n",
      "                                                                 1a_second2_act[0][0]             \n",
      "                                                                 1a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 24, 129)      0           input_2[0][0]                    \n",
      "                                                                 1a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_conv (Conv1D)        (None, 24, 64)       8320        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_conv (Conv1D)         (None, 24, 16)       2080        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_bn (BatchNormalizati (None, 24, 64)       256         2a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_bn (BatchNormalizatio (None, 24, 16)       64          2a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_act (Activation)     (None, 24, 64)       0           2a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_act (Activation)      (None, 24, 16)       0           2a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_conv (Conv1D)          (None, 24, 64)       8320        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_conv (Conv1D)        (None, 24, 128)      24704       2a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_conv (Conv1D)         (None, 24, 64)       5184        2a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_bn (BatchNormalization (None, 24, 64)       256         2a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_bn (BatchNormalizati (None, 24, 128)      512         2a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_bn (BatchNormalizatio (None, 24, 64)       256         2a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_act (Activation)       (None, 24, 64)       0           2a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_act (Activation)     (None, 24, 128)      0           2a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_act (Activation)      (None, 24, 64)       0           2a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_mixed (Concatenate)          (None, 24, 256)      0           2a_first_act[0][0]               \n",
      "                                                                 2a_second2_act[0][0]             \n",
      "                                                                 2a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24, 384)      0           1a_mixed[0][0]                   \n",
      "                                                                 2a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_conv (Conv1D)        (None, 24, 64)       24640       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_conv (Conv1D)         (None, 24, 16)       6160        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_bn (BatchNormalizati (None, 24, 64)       256         3a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_bn (BatchNormalizatio (None, 24, 16)       64          3a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_act (Activation)     (None, 24, 64)       0           3a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_act (Activation)      (None, 24, 16)       0           3a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_conv (Conv1D)          (None, 24, 64)       24640       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_conv (Conv1D)        (None, 24, 128)      24704       3a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_conv (Conv1D)         (None, 24, 64)       5184        3a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_bn (BatchNormalization (None, 24, 64)       256         3a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_bn (BatchNormalizati (None, 24, 128)      512         3a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_bn (BatchNormalizatio (None, 24, 64)       256         3a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_act (Activation)       (None, 24, 64)       0           3a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_act (Activation)     (None, 24, 128)      0           3a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_act (Activation)      (None, 24, 64)       0           3a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "3a_mixed (Concatenate)          (None, 24, 256)      0           3a_first_act[0][0]               \n",
      "                                                                 3a_second2_act[0][0]             \n",
      "                                                                 3a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 24, 21)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 24, 513)      0           input_2[0][0]                    \n",
      "                                                                 2a_mixed[0][0]                   \n",
      "                                                                 3a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 24, 20)       3360        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12312)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20)           3280        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 12332)        0           flatten[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 24)           295992      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 447,592\n",
      "Trainable params: 445,912\n",
      "Non-trainable params: 1,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9rQrz9B0JWvR"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\4_without_BLSTM\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\4_without_BLSTM'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h6NbczAlOpSQ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NkF7jg_XO346"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-rIEZj0PCPF",
    "outputId": "95edda02-84de-4f8a-83a2-b4d53387049d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = review_goog()\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-o0qgceiPFdI",
    "outputId": "ebb2c588-e73f-4182-baad-2f4c28f3db95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.1804 - mae: 0.0563 - mape: 2245.8743\n",
      "Epoch 00001: val_loss improved from inf to 0.04194, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E1-cp-0001-loss0.04.h5\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.1804 - mae: 0.0563 - mape: 2244.3206 - val_loss: 0.0419 - val_mae: 0.0408 - val_mape: 17.0846\n",
      "Epoch 2/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.0369 - mape: 2295.8149\n",
      "Epoch 00002: val_loss improved from 0.04194 to 0.04167, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E1-cp-0002-loss0.04.h5\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0379 - mae: 0.0369 - mape: 2295.0879 - val_loss: 0.0417 - val_mae: 0.0408 - val_mape: 17.7174\n",
      "Epoch 3/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0355 - mae: 0.0347 - mape: 2405.8293\n",
      "Epoch 00003: val_loss did not improve from 0.04167\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0355 - mae: 0.0347 - mape: 2404.1670 - val_loss: 0.0535 - val_mae: 0.0528 - val_mape: 28.2460\n",
      "Epoch 4/40\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0343 - mae: 0.0336 - mape: 2378.5886\n",
      "Epoch 00004: val_loss did not improve from 0.04167\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0343 - mae: 0.0336 - mape: 2376.0491 - val_loss: 0.0639 - val_mae: 0.0633 - val_mape: 27.7318\n",
      "Epoch 5/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0336 - mae: 0.0329 - mape: 2332.1335\n",
      "Epoch 00005: val_loss did not improve from 0.04167\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0336 - mae: 0.0329 - mape: 2332.1335 - val_loss: 0.0755 - val_mae: 0.0749 - val_mape: 38.8375\n",
      "Epoch 6/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0330 - mae: 0.0324 - mape: 2297.7859\n",
      "Epoch 00006: val_loss did not improve from 0.04167\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0330 - mae: 0.0324 - mape: 2297.7859 - val_loss: 0.0706 - val_mae: 0.0701 - val_mape: 27.1201\n",
      "Epoch 7/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0327 - mae: 0.0321 - mape: 2275.4907\n",
      "Epoch 00007: val_loss improved from 0.04167 to 0.04114, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E1-cp-0007-loss0.04.h5\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0327 - mae: 0.0321 - mape: 2275.4907 - val_loss: 0.0411 - val_mae: 0.0406 - val_mape: 19.8800\n",
      "Epoch 8/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0323 - mae: 0.0317 - mape: 2239.8918\n",
      "Epoch 00008: val_loss did not improve from 0.04114\n",
      "2652/2652 [==============================] - 46s 18ms/step - loss: 0.0323 - mae: 0.0317 - mape: 2238.3406 - val_loss: 0.0859 - val_mae: 0.0854 - val_mape: 46.2280\n",
      "Epoch 9/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0320 - mae: 0.0315 - mape: 2195.7354\n",
      "Epoch 00009: val_loss did not improve from 0.04114\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0320 - mae: 0.0315 - mape: 2194.2151 - val_loss: 0.0452 - val_mae: 0.0447 - val_mape: 17.7959\n",
      "Epoch 10/40\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0318 - mae: 0.0313 - mape: 2199.3267\n",
      "Epoch 00010: val_loss did not improve from 0.04114\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0318 - mae: 0.0313 - mape: 2196.9778 - val_loss: 0.0476 - val_mae: 0.0471 - val_mape: 19.9375\n",
      "Epoch 11/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0316 - mae: 0.0311 - mape: 2252.0549\n",
      "Epoch 00011: val_loss improved from 0.04114 to 0.03742, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E1-cp-0011-loss0.04.h5\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0316 - mae: 0.0311 - mape: 2252.0549 - val_loss: 0.0374 - val_mae: 0.0369 - val_mape: 16.1918\n",
      "Epoch 12/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0314 - mae: 0.0309 - mape: 2239.6326\n",
      "Epoch 00012: val_loss did not improve from 0.03742\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0314 - mae: 0.0309 - mape: 2239.6326 - val_loss: 0.1004 - val_mae: 0.0999 - val_mape: 42.2408\n",
      "Epoch 13/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0312 - mae: 0.0307 - mape: 2184.0710\n",
      "Epoch 00013: val_loss did not improve from 0.03742\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0312 - mae: 0.0307 - mape: 2184.0710 - val_loss: 0.0445 - val_mae: 0.0441 - val_mape: 17.3803\n",
      "Epoch 14/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0309 - mae: 0.0305 - mape: 2182.3577\n",
      "Epoch 00014: val_loss did not improve from 0.03742\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0309 - mae: 0.0305 - mape: 2180.8462 - val_loss: 0.1280 - val_mae: 0.1275 - val_mape: 53.4428\n",
      "Epoch 15/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0308 - mae: 0.0304 - mape: 2141.6545\n",
      "Epoch 00015: val_loss did not improve from 0.03742\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0308 - mae: 0.0304 - mape: 2140.9753 - val_loss: 0.0384 - val_mae: 0.0380 - val_mape: 15.7135\n",
      "Epoch 16/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0305 - mae: 0.0302 - mape: 2144.5864\n",
      "Epoch 00016: val_loss did not improve from 0.03742\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0305 - mae: 0.0302 - mape: 2143.1011 - val_loss: 0.0455 - val_mae: 0.0452 - val_mape: 17.1764\n",
      "Epoch 17/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0299 - mape: 2176.4053\n",
      "Epoch 00017: val_loss did not improve from 0.03742\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0303 - mae: 0.0299 - mape: 2174.8982 - val_loss: 0.0497 - val_mae: 0.0494 - val_mape: 24.2320\n",
      "Epoch 18/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0300 - mae: 0.0297 - mape: 2131.3525\n",
      "Epoch 00018: val_loss did not improve from 0.03742\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0300 - mae: 0.0297 - mape: 2129.8770 - val_loss: 0.0391 - val_mae: 0.0388 - val_mape: 15.7308\n",
      "Epoch 19/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0298 - mae: 0.0295 - mape: 2138.6509\n",
      "Epoch 00019: val_loss did not improve from 0.03742\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0298 - mae: 0.0295 - mape: 2138.6509 - val_loss: 0.0417 - val_mae: 0.0414 - val_mape: 20.4964\n",
      "Epoch 20/40\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0296 - mae: 0.0293 - mape: 2120.3218\n",
      "Epoch 00020: val_loss did not improve from 0.03742\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0296 - mae: 0.0293 - mape: 2118.0581 - val_loss: 0.0631 - val_mae: 0.0629 - val_mape: 21.7696\n",
      "Epoch 21/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0294 - mae: 0.0291 - mape: 2122.9343\n",
      "Epoch 00021: val_loss improved from 0.03742 to 0.03569, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E1-cp-0021-loss0.04.h5\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0294 - mae: 0.0291 - mape: 2122.9343 - val_loss: 0.0357 - val_mae: 0.0354 - val_mape: 15.4749\n",
      "Epoch 22/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0293 - mae: 0.0290 - mape: 2131.9739\n",
      "Epoch 00022: val_loss did not improve from 0.03569\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0293 - mae: 0.0290 - mape: 2131.9739 - val_loss: 0.1141 - val_mae: 0.1139 - val_mape: 49.6759\n",
      "Epoch 23/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0291 - mae: 0.0288 - mape: 2070.6826\n",
      "Epoch 00023: val_loss did not improve from 0.03569\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0291 - mae: 0.0288 - mape: 2070.6826 - val_loss: 0.0395 - val_mae: 0.0393 - val_mape: 17.4595\n",
      "Epoch 24/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.0287 - mape: 2101.4980\n",
      "Epoch 00024: val_loss did not improve from 0.03569\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0290 - mae: 0.0287 - mape: 2100.8323 - val_loss: 0.0378 - val_mae: 0.0376 - val_mape: 16.8510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0288 - mae: 0.0286 - mape: 2066.7102\n",
      "Epoch 00025: val_loss did not improve from 0.03569\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0288 - mae: 0.0286 - mape: 2066.0554 - val_loss: 0.0363 - val_mae: 0.0361 - val_mape: 15.3892\n",
      "Epoch 26/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0286 - mae: 0.0284 - mape: 2063.1165\n",
      "Epoch 00026: val_loss did not improve from 0.03569\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0286 - mae: 0.0284 - mape: 2062.4634 - val_loss: 0.0375 - val_mae: 0.0373 - val_mape: 16.7037\n",
      "Epoch 27/40\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0285 - mae: 0.0283 - mape: 2081.5913\n",
      "Epoch 00027: val_loss did not improve from 0.03569\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0285 - mae: 0.0283 - mape: 2079.3694 - val_loss: 0.0373 - val_mae: 0.0371 - val_mape: 15.6240\n",
      "Epoch 28/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0284 - mae: 0.0282 - mape: 2061.5991\n",
      "Epoch 00028: val_loss did not improve from 0.03569\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0284 - mae: 0.0282 - mape: 2061.5991 - val_loss: 0.0405 - val_mae: 0.0403 - val_mape: 17.1018\n",
      "Epoch 29/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0283 - mae: 0.0281 - mape: 2105.4851\n",
      "Epoch 00029: val_loss improved from 0.03569 to 0.03568, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E1-cp-0029-loss0.04.h5\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0283 - mae: 0.0281 - mape: 2104.0273 - val_loss: 0.0357 - val_mae: 0.0355 - val_mape: 15.0001\n",
      "Epoch 30/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0282 - mae: 0.0280 - mape: 2060.8447\n",
      "Epoch 00030: val_loss did not improve from 0.03568\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0282 - mae: 0.0280 - mape: 2060.8447 - val_loss: 0.0359 - val_mae: 0.0357 - val_mape: 15.7536\n",
      "Epoch 31/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0281 - mae: 0.0279 - mape: 2046.1327- ETA: 2s - loss: \n",
      "Epoch 00031: val_loss did not improve from 0.03568\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0281 - mae: 0.0279 - mape: 2044.7161 - val_loss: 0.0372 - val_mae: 0.0370 - val_mape: 17.3177\n",
      "Epoch 32/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0279 - mae: 0.0278 - mape: 2051.1235\n",
      "Epoch 00032: val_loss did not improve from 0.03568\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0279 - mae: 0.0278 - mape: 2051.1235 - val_loss: 0.0384 - val_mae: 0.0382 - val_mape: 16.3269\n",
      "Epoch 33/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0279 - mae: 0.0277 - mape: 2074.8127\n",
      "Epoch 00033: val_loss improved from 0.03568 to 0.03566, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E1-cp-0033-loss0.04.h5\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0279 - mae: 0.0277 - mape: 2074.8127 - val_loss: 0.0357 - val_mae: 0.0355 - val_mape: 15.8579\n",
      "Epoch 34/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0278 - mae: 0.0276 - mape: 2046.5752\n",
      "Epoch 00034: val_loss did not improve from 0.03566\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0278 - mae: 0.0276 - mape: 2045.9272 - val_loss: 0.0376 - val_mae: 0.0374 - val_mape: 15.7834\n",
      "Epoch 35/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0277 - mae: 0.0275 - mape: 2065.6865\n",
      "Epoch 00035: val_loss improved from 0.03566 to 0.03361, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E1-cp-0035-loss0.03.h5\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0277 - mae: 0.0275 - mape: 2065.0320 - val_loss: 0.0336 - val_mae: 0.0335 - val_mape: 16.0524\n",
      "Epoch 36/40\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0277 - mae: 0.0275 - mape: 2077.1511\n",
      "Epoch 00036: val_loss did not improve from 0.03361\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0277 - mae: 0.0275 - mape: 2074.9319 - val_loss: 0.0356 - val_mae: 0.0355 - val_mape: 14.8392\n",
      "Epoch 37/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0274 - mape: 2052.2791\n",
      "Epoch 00037: val_loss did not improve from 0.03361\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0276 - mae: 0.0274 - mape: 2051.6287 - val_loss: 0.0346 - val_mae: 0.0345 - val_mape: 15.9416\n",
      "Epoch 38/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0275 - mae: 0.0273 - mape: 2057.4307\n",
      "Epoch 00038: val_loss did not improve from 0.03361\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0275 - mae: 0.0274 - mape: 2056.0061 - val_loss: 0.0343 - val_mae: 0.0341 - val_mape: 15.1717\n",
      "Epoch 39/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0274 - mae: 0.0272 - mape: 2049.5012\n",
      "Epoch 00039: val_loss did not improve from 0.03361\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0274 - mae: 0.0272 - mape: 2048.0823 - val_loss: 0.0349 - val_mae: 0.0347 - val_mape: 15.4922\n",
      "Epoch 40/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0272 - mape: 2061.2754\n",
      "Epoch 00040: val_loss did not improve from 0.03361\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0273 - mae: 0.0272 - mape: 2060.6221 - val_loss: 0.0358 - val_mae: 0.0356 - val_mape: 15.8306\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oS6zzoUoPL-A",
    "outputId": "ce1a0f3b-6b76-4980-d717-d3542c18a37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 528.32\n",
      "Median Absolute Error (MedAE): 377.81\n",
      "Mean Squared Error (MSE): 533302.42\n",
      "Root Mean Squared Error (RMSE): 730.28\n",
      "Mean Absolute Percentage Error (MAPE): 3.55 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.65 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\4_without_BLSTM\\\\E1-cp-0035-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7y39sVKwLMm"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "AVN1WhyzuWFl"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\4_without_BLSTM\\\\E2-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\4_without_BLSTM\\\\E1-cp-0035-loss0.03.h5'\n",
    "start_epoch= 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WBeDTA9lu5Qm"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2j6fmkOvA2O",
    "outputId": "7fc1b23b-4d93-4b54-a4df-921c557b9a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E1-cp-0035-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rTCJW6IzvLU4",
    "outputId": "f810e1ca-98cc-4de7-a4a3-465e83705c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0266 - mean_absolute_error: 0.0264 - mean_absolute_percentage_error: 2031.0204\n",
      "Epoch 00001: val_loss improved from inf to 0.03314, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E2-cp-0001-loss0.03.h5\n",
      "2652/2652 [==============================] - 48s 18ms/step - loss: 0.0266 - mean_absolute_error: 0.0264 - mean_absolute_percentage_error: 2029.6141 - val_loss: 0.0331 - val_mean_absolute_error: 0.0330 - val_mean_absolute_percentage_error: 14.8460\n",
      "Epoch 2/30\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0265 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2037.4191\n",
      "Epoch 00002: val_loss improved from 0.03314 to 0.03305, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E2-cp-0002-loss0.03.h5\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0265 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2036.0074 - val_loss: 0.0330 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 15.0145\n",
      "Epoch 3/30\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0264 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2032.7812- ETA: 0s - loss: 0.0264 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 1\n",
      "Epoch 00003: val_loss improved from 0.03305 to 0.03300, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E2-cp-0003-loss0.03.h5\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0264 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2032.1368 - val_loss: 0.0330 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 14.8970\n",
      "Epoch 4/30\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0264 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2037.0931\n",
      "Epoch 00004: val_loss did not improve from 0.03300\n",
      "2652/2652 [==============================] - 48s 18ms/step - loss: 0.0264 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2036.4475 - val_loss: 0.0332 - val_mean_absolute_error: 0.0331 - val_mean_absolute_percentage_error: 15.0924\n",
      "Epoch 5/30\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0264 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2029.5454\n",
      "Epoch 00005: val_loss did not improve from 0.03300\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0264 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2028.9021 - val_loss: 0.0331 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 14.9460\n",
      "Epoch 6/30\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0264 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2033.1565\n",
      "Epoch 00006: val_loss did not improve from 0.03300\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0264 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2030.9852 - val_loss: 0.0331 - val_mean_absolute_error: 0.0330 - val_mean_absolute_percentage_error: 15.1229\n",
      "Epoch 7/30\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2033.5366\n",
      "Epoch 00007: val_loss did not improve from 0.03300\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0263 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2033.5366 - val_loss: 0.0330 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 14.8006\n",
      "Epoch 8/30\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2032.9962- ET\n",
      "Epoch 00008: val_loss did not improve from 0.03300\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0263 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2031.5886 - val_loss: 0.0331 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 14.8769\n",
      "Epoch 9/30\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2029.9913\n",
      "Epoch 00009: val_loss did not improve from 0.03300\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0263 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2028.5858 - val_loss: 0.0332 - val_mean_absolute_error: 0.0330 - val_mean_absolute_percentage_error: 14.9832\n",
      "Epoch 10/30\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2038.0775\n",
      "Epoch 00010: val_loss did not improve from 0.03300\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0263 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2038.0775 - val_loss: 0.0335 - val_mean_absolute_error: 0.0334 - val_mean_absolute_percentage_error: 15.1078\n",
      "Epoch 11/30\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2038.6841\n",
      "Epoch 00011: val_loss improved from 0.03300 to 0.03298, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E2-cp-0011-loss0.03.h5\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0263 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2036.5054 - val_loss: 0.0330 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 14.8715\n",
      "Epoch 12/30\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2019.7233\n",
      "Epoch 00012: val_loss did not improve from 0.03298\n",
      "2652/2652 [==============================] - 44s 17ms/step - loss: 0.0263 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2019.0834 - val_loss: 0.0330 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 14.9273\n",
      "Epoch 13/30\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2030.2722\n",
      "Epoch 00013: val_loss improved from 0.03298 to 0.03293, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E2-cp-0013-loss0.03.h5\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2029.6287 - val_loss: 0.0329 - val_mean_absolute_error: 0.0328 - val_mean_absolute_percentage_error: 14.7401\n",
      "Epoch 14/30\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2035.3043\n",
      "Epoch 00014: val_loss did not improve from 0.03293\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2033.8951 - val_loss: 0.0332 - val_mean_absolute_error: 0.0331 - val_mean_absolute_percentage_error: 14.7713\n",
      "Epoch 15/30\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2026.7748\n",
      "Epoch 00015: val_loss improved from 0.03293 to 0.03275, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E2-cp-0015-loss0.03.h5\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2026.7748 - val_loss: 0.0328 - val_mean_absolute_error: 0.0326 - val_mean_absolute_percentage_error: 14.6846\n",
      "Epoch 16/30\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2014.8027- ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 203\n",
      "Epoch 00016: val_loss improved from 0.03275 to 0.03264, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\4_without_BLSTM\\E2-cp-0016-loss0.03.h5\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2012.6499 - val_loss: 0.0326 - val_mean_absolute_error: 0.0325 - val_mean_absolute_percentage_error: 14.5985\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2020.6400\n",
      "Epoch 00017: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 44s 17ms/step - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2019.2406 - val_loss: 0.0334 - val_mean_absolute_error: 0.0333 - val_mean_absolute_percentage_error: 15.1296\n",
      "Epoch 18/30\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2025.4885\n",
      "Epoch 00018: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0262 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2024.0857 - val_loss: 0.0331 - val_mean_absolute_error: 0.0330 - val_mean_absolute_percentage_error: 14.7232\n",
      "Epoch 19/30\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2010.5155\n",
      "Epoch 00019: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0262 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2009.1229 - val_loss: 0.0334 - val_mean_absolute_error: 0.0333 - val_mean_absolute_percentage_error: 14.9819\n",
      "Epoch 20/30\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2017.2322\n",
      "Epoch 00020: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2015.0791 - val_loss: 0.0328 - val_mean_absolute_error: 0.0327 - val_mean_absolute_percentage_error: 14.6828\n",
      "Epoch 21/30\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2023.8507\n",
      "Epoch 00021: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 48s 18ms/step - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2022.4487 - val_loss: 0.0328 - val_mean_absolute_error: 0.0327 - val_mean_absolute_percentage_error: 14.5886\n",
      "Epoch 22/30\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2007.7020\n",
      "Epoch 00022: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2006.3119 - val_loss: 0.0334 - val_mean_absolute_error: 0.0332 - val_mean_absolute_percentage_error: 15.0849\n",
      "Epoch 23/30\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2024.5936\n",
      "Epoch 00023: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 47s 18ms/step - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2022.4314 - val_loss: 0.0330 - val_mean_absolute_error: 0.0329 - val_mean_absolute_percentage_error: 14.6646\n",
      "Epoch 24/30\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2024.7057\n",
      "Epoch 00024: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2022.5422 - val_loss: 0.0334 - val_mean_absolute_error: 0.0333 - val_mean_absolute_percentage_error: 15.0226\n",
      "Epoch 25/30\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2005.9053\n",
      "Epoch 00025: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 46s 17ms/step - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2005.2694 - val_loss: 0.0330 - val_mean_absolute_error: 0.0328 - val_mean_absolute_percentage_error: 14.8881\n",
      "Epoch 26/30\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2011.1066\n",
      "Epoch 00026: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0261 - mean_absolute_error: 0.0260 - mean_absolute_percentage_error: 2010.4694 - val_loss: 0.0334 - val_mean_absolute_error: 0.0333 - val_mean_absolute_percentage_error: 14.7669\n",
      "Epoch 27/30\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0260 - mean_absolute_error: 0.0259 - mean_absolute_percentage_error: 2021.5413\n",
      "Epoch 00027: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 45s 17ms/step - loss: 0.0260 - mean_absolute_error: 0.0259 - mean_absolute_percentage_error: 2020.9008 - val_loss: 0.0329 - val_mean_absolute_error: 0.0328 - val_mean_absolute_percentage_error: 14.6045\n",
      "Epoch 28/30\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0260 - mean_absolute_error: 0.0259 - mean_absolute_percentage_error: 2015.5214\n",
      "Epoch 00028: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 46s 18ms/step - loss: 0.0260 - mean_absolute_error: 0.0259 - mean_absolute_percentage_error: 2013.3679 - val_loss: 0.0329 - val_mean_absolute_error: 0.0327 - val_mean_absolute_percentage_error: 14.4898\n",
      "Epoch 29/30\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0260 - mean_absolute_error: 0.0259 - mean_absolute_percentage_error: 2035.9761\n",
      "Epoch 00029: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 48s 18ms/step - loss: 0.0260 - mean_absolute_error: 0.0259 - mean_absolute_percentage_error: 2033.8000 - val_loss: 0.0335 - val_mean_absolute_error: 0.0334 - val_mean_absolute_percentage_error: 15.1374\n",
      "Epoch 30/30\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0260 - mean_absolute_error: 0.0259 - mean_absolute_percentage_error: 2022.2651- ETA: 4s - loss: 0.0260 - mea\n",
      "Epoch 00030: val_loss did not improve from 0.03264\n",
      "2652/2652 [==============================] - 48s 18ms/step - loss: 0.0260 - mean_absolute_error: 0.0259 - mean_absolute_percentage_error: 2020.1058 - val_loss: 0.0331 - val_mean_absolute_error: 0.0330 - val_mean_absolute_percentage_error: 14.6863\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y_load),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qj8HRsBXvf0M",
    "outputId": "084e6fbf-b9cc-46e8-e4d7-808e9031da87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 517.21\n",
      "Median Absolute Error (MedAE): 353.74\n",
      "Mean Squared Error (MSE): 534040.11\n",
      "Root Mean Squared Error (RMSE): 730.78\n",
      "Mean Absolute Percentage Error (MAPE): 3.45 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.48 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\4_without_BLSTM\\\\E2-cp-0016-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAuKvjoOG-fA"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QZI2LOvEw6Qq"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E2-cp-0006-loss0.04.h5'\n",
    "start_epoch= 59+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qPy-9EGRw6eR"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beh0i6Vww6pP",
    "outputId": "41024e1d-abfa-4ec6-a3ed-cf5e5fdef9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E2-cp-0006-loss0.04.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y-9y1xTeYAaB",
    "outputId": "3e59db7d-d1fa-41c7-a91e-cf2063e855da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0231 - mape: 8601.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03575, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0001-loss0.04.h5\n",
      "32614/32614 [==============================] - 7s 210us/sample - loss: 0.0235 - mae: 0.0231 - mape: 8729.3066 - val_loss: 0.0358 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0230 - mape: 8563.2227\n",
      "Epoch 00002: val_loss improved from 0.03575 to 0.03574, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0002-loss0.04.h5\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0235 - mae: 0.0230 - mape: 8710.7080 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1938\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8942.2402\n",
      "Epoch 00003: val_loss improved from 0.03574 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0003-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 115us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8703.8408 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8589.9561\n",
      "Epoch 00004: val_loss improved from 0.03573 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 114us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8711.1074 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1949\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8953.2998\n",
      "Epoch 00005: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8714.5996 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8595.0498\n",
      "Epoch 00006: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8716.8604 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8175.1738\n",
      "Epoch 00007: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8717.2090 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8577.0596\n",
      "Epoch 00008: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8716.0957 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1954\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2109\n",
      "Epoch 00009: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.7314 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2744\n",
      "Epoch 00010: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.3086 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.7139\n",
      "Epoch 00011: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.5303 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.2705\n",
      "Epoch 00012: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8717.8525 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1962\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2910\n",
      "Epoch 00013: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.2773 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8968.6553\n",
      "Epoch 00014: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.5400 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1970\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8580.6602\n",
      "Epoch 00015: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8719.8223 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1967\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8623\n",
      "Epoch 00016: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.7832 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1963\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8877\n",
      "Epoch 00017: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.8203 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8606.8154\n",
      "Epoch 00018: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0547 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2119\n",
      "Epoch 00019: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.2393 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8973.6885\n",
      "Epoch 00020: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8734.4512 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8217.7236  ETA: 2s - loss: 0.0233 -\n",
      "Epoch 00021: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8730.0449 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8967.8350\n",
      "Epoch 00022: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8728.7363 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8965.8945\n",
      "Epoch 00023: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.8516 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1969\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8607.8672\n",
      "Epoch 00024: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0010 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1964\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8588.1279\n",
      "Epoch 00025: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8735.5576 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8230.3066\n",
      "Epoch 00026: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8738.8379 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 27/200\n",
      "13312/32614 [===========>..................] - ETA: 1s - loss: 0.0233 - mae: 0.0229 - mape: 7589.0488"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-05065ff65640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y, \n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9fPE-jZuQ0r",
    "outputId": "aca09565-e9df-4067-d473-32dd8826a738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 541.71\n",
      "Median Absolute Error (MedAE): 449.47\n",
      "Mean Squared Error (MSE): 473684.02\n",
      "Root Mean Squared Error (RMSE): 688.25\n",
      "Mean Absolute Percentage Error (MAPE): 5.4 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.39 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-0004-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7x0O-u0GbLy"
   },
   "source": [
    "# E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rdBroUOqIeW6"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(200, activation='relu')(x)\n",
    " #       x = Dense(10, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AQaM-IuPIXqN"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QwUKEvSfIW09"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NHgO7mJ4IQVp"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TIsLEDoIN0C",
    "outputId": "3ef77929-b334-4953-faab-b938f376c306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpLkyqBmIKIw",
    "outputId": "3d701f4a-270d-4efc-b52a-cd26d12d4dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.2038 - mae: 0.0831 - mape: 10284.1670\n",
      "Epoch 00001: val_loss improved from inf to 0.10131, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0001-loss0.10.h5\n",
      "32614/32614 [==============================] - 19s 581us/sample - loss: 0.2036 - mae: 0.0830 - mape: 10262.1201 - val_loss: 0.1013 - val_mae: 0.0675 - val_mape: 11.9466\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0654 - mae: 0.0492 - mape: 9617.4668\n",
      "Epoch 00002: val_loss improved from 0.10131 to 0.07428, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0002-loss0.07.h5\n",
      "32614/32614 [==============================] - 14s 420us/sample - loss: 0.0654 - mae: 0.0492 - mape: 9596.8438 - val_loss: 0.0743 - val_mae: 0.0671 - val_mape: 11.8373\n",
      "Epoch 3/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0511 - mae: 0.0461 - mape: 9812.7764\n",
      "Epoch 00003: val_loss improved from 0.07428 to 0.06341, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0003-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0510 - mae: 0.0460 - mape: 9762.8730 - val_loss: 0.0634 - val_mae: 0.0599 - val_mape: 10.4427\n",
      "Epoch 4/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0450 - mae: 0.0419 - mape: 9616.9199\n",
      "Epoch 00004: val_loss did not improve from 0.06341\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0450 - mae: 0.0419 - mape: 9596.3027 - val_loss: 0.0706 - val_mae: 0.0680 - val_mape: 11.4199\n",
      "Epoch 5/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0441 - mae: 0.0414 - mape: 9613.5029\n",
      "Epoch 00005: val_loss improved from 0.06341 to 0.05454, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0005-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0441 - mae: 0.0414 - mape: 9611.7354 - val_loss: 0.0545 - val_mae: 0.0519 - val_mape: 9.2312\n",
      "Epoch 6/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0418 - mae: 0.0397 - mape: 9602.1924\n",
      "Epoch 00006: val_loss did not improve from 0.05454\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0418 - mae: 0.0397 - mape: 9572.1885 - val_loss: 0.0587 - val_mae: 0.0570 - val_mape: 9.8649\n",
      "Epoch 7/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0406 - mae: 0.0389 - mape: 9195.3799\n",
      "Epoch 00007: val_loss improved from 0.05454 to 0.04682, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0007-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0406 - mae: 0.0389 - mape: 9554.0049 - val_loss: 0.0468 - val_mae: 0.0450 - val_mape: 8.7847\n",
      "Epoch 8/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0399 - mae: 0.0384 - mape: 9416.9941\n",
      "Epoch 00008: val_loss did not improve from 0.04682\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0399 - mae: 0.0384 - mape: 9396.7998 - val_loss: 0.0475 - val_mae: 0.0462 - val_mape: 8.9737\n",
      "Epoch 9/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0397 - mae: 0.0383 - mape: 9665.3730\n",
      "Epoch 00009: val_loss improved from 0.04682 to 0.04522, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0009-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0397 - mae: 0.0383 - mape: 9644.6445 - val_loss: 0.0452 - val_mae: 0.0439 - val_mape: 8.5116\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0377 - mape: 9573.1260\n",
      "Epoch 00010: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0389 - mae: 0.0377 - mape: 9533.8271 - val_loss: 0.0940 - val_mae: 0.0928 - val_mape: 15.8112\n",
      "Epoch 11/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0390 - mae: 0.0378 - mape: 9537.1748\n",
      "Epoch 00011: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0390 - mae: 0.0378 - mape: 9535.4209 - val_loss: 0.0501 - val_mae: 0.0490 - val_mape: 8.8788\n",
      "Epoch 12/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0381 - mae: 0.0371 - mape: 9512.4170\n",
      "Epoch 00012: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0381 - mae: 0.0371 - mape: 9492.0146 - val_loss: 0.0511 - val_mae: 0.0502 - val_mape: 9.2328\n",
      "Epoch 13/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0377 - mae: 0.0368 - mape: 9480.7461\n",
      "Epoch 00013: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0377 - mae: 0.0368 - mape: 9479.0029 - val_loss: 0.0494 - val_mae: 0.0485 - val_mape: 9.0546\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.0370 - mape: 9640.1309\n",
      "Epoch 00014: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0379 - mae: 0.0370 - mape: 9600.5547 - val_loss: 0.0475 - val_mae: 0.0466 - val_mape: 8.6730\n",
      "Epoch 15/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0375 - mae: 0.0367 - mape: 9567.5752\n",
      "Epoch 00015: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0375 - mae: 0.0367 - mape: 9565.8154 - val_loss: 0.0497 - val_mae: 0.0489 - val_mape: 9.3880\n",
      "Epoch 16/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0374 - mae: 0.0366 - mape: 9551.2051\n",
      "Epoch 00016: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0374 - mae: 0.0366 - mape: 9540.0859 - val_loss: 0.0464 - val_mae: 0.0456 - val_mape: 8.5940\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0362 - mape: 9563.9082\n",
      "Epoch 00017: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0369 - mae: 0.0362 - mape: 9562.1514 - val_loss: 0.0475 - val_mae: 0.0468 - val_mape: 8.6608\n",
      "Epoch 18/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0361 - mape: 9577.6992\n",
      "Epoch 00018: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0368 - mae: 0.0361 - mape: 9566.5488 - val_loss: 0.0627 - val_mae: 0.0620 - val_mape: 10.5263\n",
      "Epoch 19/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0366 - mae: 0.0359 - mape: 8800.0615\n",
      "Epoch 00019: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0366 - mae: 0.0359 - mape: 9530.1943 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 8.7106\n",
      "Epoch 20/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9487.9727\n",
      "Epoch 00020: val_loss improved from 0.04522 to 0.04432, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0020-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9476.9248 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.3836\n",
      "Epoch 21/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9175.1748\n",
      "Epoch 00021: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9548.0811 - val_loss: 0.0444 - val_mae: 0.0438 - val_mape: 8.4635\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0356 - mape: 9438.7158\n",
      "Epoch 00022: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0362 - mae: 0.0356 - mape: 9427.7275 - val_loss: 0.0463 - val_mae: 0.0456 - val_mape: 8.8022\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0363 - mae: 0.0356 - mape: 9494.2676\n",
      "Epoch 00023: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0363 - mae: 0.0356 - mape: 9492.5234 - val_loss: 0.0502 - val_mae: 0.0496 - val_mape: 8.8956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0360 - mae: 0.0354 - mape: 9481.0664\n",
      "Epoch 00024: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0360 - mae: 0.0354 - mape: 9479.3242 - val_loss: 0.0482 - val_mae: 0.0476 - val_mape: 8.8535\n",
      "Epoch 25/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9452.0479\n",
      "Epoch 00025: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9422.5107 - val_loss: 0.0450 - val_mae: 0.0444 - val_mape: 8.3675\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9389.7109\n",
      "Epoch 00026: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9360.3701 - val_loss: 0.0505 - val_mae: 0.0499 - val_mape: 8.9818\n",
      "Epoch 27/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9449.0146\n",
      "Epoch 00027: val_loss improved from 0.04432 to 0.04404, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0027-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9419.4844 - val_loss: 0.0440 - val_mae: 0.0434 - val_mape: 8.4639\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9357.8389\n",
      "Epoch 00028: val_loss improved from 0.04404 to 0.04385, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0028-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9346.9443 - val_loss: 0.0438 - val_mae: 0.0433 - val_mape: 8.3336\n",
      "Epoch 29/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0350 - mape: 9348.0967- ETA: 2s - loss: 0.\n",
      "Epoch 00029: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0356 - mae: 0.0350 - mape: 9337.2129 - val_loss: 0.0453 - val_mae: 0.0447 - val_mape: 8.8223\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0355 - mae: 0.0349 - mape: 9328.2656\n",
      "Epoch 00030: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0355 - mae: 0.0349 - mape: 9299.1152 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.4327\n",
      "Epoch 31/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0348 - mape: 9367.9561\n",
      "Epoch 00031: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0354 - mae: 0.0348 - mape: 9329.5146 - val_loss: 0.0556 - val_mae: 0.0550 - val_mape: 9.5869\n",
      "Epoch 32/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0346 - mae: 0.0341 - mape: 9331.7041\n",
      "Epoch 00032: val_loss improved from 0.04385 to 0.03952, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0032-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0346 - mae: 0.0341 - mape: 9293.3955 - val_loss: 0.0395 - val_mae: 0.0389 - val_mape: 7.3175\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0313 - mae: 0.0307 - mape: 8705.7988\n",
      "Epoch 00033: val_loss did not improve from 0.03952\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0313 - mae: 0.0307 - mape: 9014.9424 - val_loss: 0.0402 - val_mae: 0.0396 - val_mape: 6.9964\n",
      "Epoch 34/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0310 - mae: 0.0304 - mape: 8956.5234\n",
      "Epoch 00034: val_loss improved from 0.03952 to 0.03736, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0034-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 427us/sample - loss: 0.0310 - mae: 0.0304 - mape: 8937.3135 - val_loss: 0.0374 - val_mae: 0.0368 - val_mape: 6.7796\n",
      "Epoch 35/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0308 - mae: 0.0302 - mape: 9005.4199\n",
      "Epoch 00035: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0308 - mae: 0.0302 - mape: 9003.7646 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.5172\n",
      "Epoch 36/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 8986.6465\n",
      "Epoch 00036: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0307 - mae: 0.0301 - mape: 8984.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.7190\n",
      "Epoch 37/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 9036.8193\n",
      "Epoch 00037: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0307 - mae: 0.0301 - mape: 9017.4414 - val_loss: 0.0397 - val_mae: 0.0391 - val_mape: 6.9239\n",
      "Epoch 38/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0304 - mae: 0.0298 - mape: 8871.0625\n",
      "Epoch 00038: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0305 - mae: 0.0299 - mape: 8852.0391 - val_loss: 0.0418 - val_mae: 0.0413 - val_mape: 7.1686\n",
      "Epoch 39/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8882.9082\n",
      "Epoch 00039: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8881.2744 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.0621\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8929.6758\n",
      "Epoch 00040: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8910.5225 - val_loss: 0.0390 - val_mae: 0.0384 - val_mape: 6.7820\n",
      "Epoch 41/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8876.4180\n",
      "Epoch 00041: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8857.3779 - val_loss: 0.0391 - val_mae: 0.0385 - val_mape: 6.9458\n",
      "Epoch 42/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8984.8389\n",
      "Epoch 00042: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8956.7549 - val_loss: 0.0413 - val_mae: 0.0407 - val_mape: 7.6354\n",
      "Epoch 43/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0301 - mae: 0.0295 - mape: 8793.3711\n",
      "Epoch 00043: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0301 - mae: 0.0295 - mape: 8783.1338 - val_loss: 0.0435 - val_mae: 0.0429 - val_mape: 7.4288\n",
      "Epoch 44/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8822.2676- ETA: 6s - loss: 0.0301 - mae: 0.0 - ETA: 1s - loss: 0.0302 - mae:\n",
      "Epoch 00044: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8811.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.9922\n",
      "Epoch 45/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0298 - mae: 0.0292 - mape: 8836.8652\n",
      "Epoch 00045: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0298 - mae: 0.0292 - mape: 8800.5830 - val_loss: 0.0593 - val_mae: 0.0587 - val_mape: 11.2009\n",
      "Epoch 46/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0287 - mae: 0.0281 - mape: 8818.6738\n",
      "Epoch 00046: val_loss improved from 0.03736 to 0.03728, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0046-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0287 - mae: 0.0281 - mape: 8791.1104 - val_loss: 0.0373 - val_mae: 0.0367 - val_mape: 6.3479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0269 - mape: 8903.1064\n",
      "Epoch 00047: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0276 - mae: 0.0269 - mape: 8866.5479 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 7.3343\n",
      "Epoch 48/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0264 - mape: 8938.7715\n",
      "Epoch 00048: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0270 - mae: 0.0264 - mape: 8919.5977 - val_loss: 0.0419 - val_mae: 0.0413 - val_mape: 6.8349\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0265 - mape: 8954.8320\n",
      "Epoch 00049: val_loss improved from 0.03728 to 0.03701, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0049-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0272 - mae: 0.0265 - mape: 8926.8408 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2637\n",
      "Epoch 50/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0263 - mape: 8925.1689\n",
      "Epoch 00050: val_loss did not improve from 0.03701\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0269 - mae: 0.0263 - mape: 8914.7754 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2835\n",
      "Epoch 51/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0262 - mape: 8973.1758\n",
      "Epoch 00051: val_loss improved from 0.03701 to 0.03644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0051-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0269 - mae: 0.0262 - mape: 8936.3379 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3367\n",
      "Epoch 52/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0267 - mae: 0.0260 - mape: 8984.7070\n",
      "Epoch 00052: val_loss improved from 0.03644 to 0.03603, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0267 - mae: 0.0260 - mape: 8947.8154 - val_loss: 0.0360 - val_mae: 0.0354 - val_mape: 6.2870\n",
      "Epoch 53/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.0259 - mape: 8895.4316\n",
      "Epoch 00053: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0265 - mae: 0.0259 - mape: 8893.7969 - val_loss: 0.0433 - val_mae: 0.0426 - val_mape: 6.9243\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0258 - mape: 8529.0977\n",
      "Epoch 00054: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0264 - mae: 0.0258 - mape: 8883.7363 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.3396\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8902.2764\n",
      "Epoch 00055: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8883.1807 - val_loss: 0.0377 - val_mae: 0.0371 - val_mape: 6.5213\n",
      "Epoch 56/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8834.2520\n",
      "Epoch 00056: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8806.6387 - val_loss: 0.0378 - val_mae: 0.0372 - val_mape: 6.4096\n",
      "Epoch 57/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0255 - mape: 8899.8281\n",
      "Epoch 00057: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0261 - mae: 0.0255 - mape: 8889.4648 - val_loss: 0.0365 - val_mae: 0.0358 - val_mape: 6.4844\n",
      "Epoch 58/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0257 - mape: 8935.8350\n",
      "Epoch 00058: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0263 - mae: 0.0257 - mape: 8899.1426 - val_loss: 0.0389 - val_mae: 0.0382 - val_mape: 6.9754\n",
      "Epoch 59/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0255 - mape: 8985.3438\n",
      "Epoch 00059: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0262 - mae: 0.0255 - mape: 8966.0693 - val_loss: 0.0448 - val_mae: 0.0442 - val_mape: 7.1970\n",
      "Epoch 60/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0254 - mape: 8698.2939\n",
      "Epoch 00060: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0261 - mae: 0.0254 - mape: 9026.8086 - val_loss: 0.0371 - val_mae: 0.0365 - val_mape: 6.4728\n",
      "Epoch 61/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0256 - mape: 8944.7266\n",
      "Epoch 00061: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0262 - mae: 0.0256 - mape: 8943.0820 - val_loss: 0.0376 - val_mae: 0.0369 - val_mape: 6.4816\n",
      "Epoch 62/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0253 - mape: 8861.0674\n",
      "Epoch 00062: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0259 - mae: 0.0253 - mape: 8833.3691 - val_loss: 0.0376 - val_mae: 0.0370 - val_mape: 6.7046\n",
      "Epoch 63/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0252 - mape: 8937.2539\n",
      "Epoch 00063: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0259 - mae: 0.0252 - mape: 8909.3174 - val_loss: 0.0441 - val_mae: 0.0434 - val_mape: 7.0393\n",
      "Epoch 64/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0251 - mape: 8836.8535\n",
      "Epoch 00064: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0257 - mae: 0.0251 - mape: 8826.5635 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.1966\n",
      "Epoch 65/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8864.3887\n",
      "Epoch 00065: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8845.3740 - val_loss: 0.0403 - val_mae: 0.0396 - val_mape: 6.6836\n",
      "Epoch 66/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0251 - mape: 8864.2441\n",
      "Epoch 00066: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0258 - mae: 0.0251 - mape: 8853.9238 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.2001\n",
      "Epoch 67/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8919.5137- ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8391\n",
      "Epoch 00067: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0256 - mae: 0.0249 - mape: 8900.3799 - val_loss: 0.0435 - val_mae: 0.0428 - val_mape: 6.9965\n",
      "Epoch 68/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0249 - mape: 8921.0537\n",
      "Epoch 00068: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0255 - mae: 0.0249 - mape: 8910.6650 - val_loss: 0.0368 - val_mae: 0.0361 - val_mape: 6.4016\n",
      "Epoch 69/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8904.1221\n",
      "Epoch 00069: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8885.0215 - val_loss: 0.0366 - val_mae: 0.0359 - val_mape: 6.4782\n",
      "Epoch 70/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0248 - mape: 8509.3799\n",
      "Epoch 00070: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0255 - mae: 0.0248 - mape: 8848.4863 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0247 - mape: 8894.0742\n",
      "Epoch 00071: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0254 - mae: 0.0247 - mape: 8874.9961 - val_loss: 0.0447 - val_mae: 0.0440 - val_mape: 7.2170\n",
      "Epoch 72/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0246 - mape: 8940.8789\n",
      "Epoch 00072: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0253 - mae: 0.0246 - mape: 8921.6992 - val_loss: 0.0399 - val_mae: 0.0393 - val_mape: 6.6394\n",
      "Epoch 73/80\n",
      "31840/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0246 - mape: 8697.8066"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d15c63a445f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BNTzwnub2Eh-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 577.97\n",
      "Median Absolute Error (MedAE): 474.07\n",
      "Mean Squared Error (MSE): 554533.54\n",
      "Root Mean Squared Error (RMSE): 744.67\n",
      "Mean Absolute Percentage Error (MAPE): 5.86 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.58 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tYU2gaBI4Yl"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fz9kU3b6IvyO"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5'\n",
    "start_epoch= 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2bMbPx8RJKL4"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5T4Ho3YJJKTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "14t41QwoJKZ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0241 - mae: 0.0235 - mape: 8787.5186\n",
      "Epoch 00001: val_loss improved from inf to 0.03371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 57s 2ms/sample - loss: 0.0241 - mae: 0.0235 - mape: 8785.9023 - val_loss: 0.0337 - val_mae: 0.0331 - val_mape: 5.6219\n",
      "Epoch 2/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8818.7939\n",
      "Epoch 00002: val_loss improved from 0.03371 to 0.03268, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 8808.5244 - val_loss: 0.0327 - val_mae: 0.0321 - val_mape: 5.5019\n",
      "Epoch 3/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 8832.4248\n",
      "Epoch 00003: val_loss improved from 0.03268 to 0.03238, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 8830.8018 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4341\n",
      "Epoch 4/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 8807.1309\n",
      "Epoch 00004: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 8805.5117 - val_loss: 0.0330 - val_mae: 0.0324 - val_mape: 5.4931\n",
      "Epoch 5/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0212 - mape: 8834.0068\n",
      "Epoch 00005: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0212 - mape: 8832.3828 - val_loss: 0.0333 - val_mae: 0.0328 - val_mape: 5.5382\n",
      "Epoch 6/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8771.0771\n",
      "Epoch 00006: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8769.4648 - val_loss: 0.0326 - val_mae: 0.0321 - val_mape: 5.4661\n",
      "Epoch 7/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8793.5381\n",
      "Epoch 00007: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8783.2979 - val_loss: 0.0325 - val_mae: 0.0320 - val_mape: 5.4550\n",
      "Epoch 8/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8746.4922\n",
      "Epoch 00008: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8744.8848 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.4740\n",
      "Epoch 9/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8799.6211\n",
      "Epoch 00009: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8789.3721 - val_loss: 0.0332 - val_mae: 0.0327 - val_mape: 5.5137\n",
      "Epoch 10/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8783.9893\n",
      "Epoch 00010: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8782.3740 - val_loss: 0.0327 - val_mae: 0.0322 - val_mape: 5.4856\n",
      "Epoch 11/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8814.5137\n",
      "Epoch 00011: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8804.2490 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.4683\n",
      "Epoch 12/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0208 - mape: 8829.3320\n",
      "Epoch 00012: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0213 - mae: 0.0208 - mape: 8819.0498 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4762\n",
      "Epoch 13/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8766.2891\n",
      "Epoch 00013: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8764.6777 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.6098\n",
      "Epoch 14/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0208 - mape: 8807.6914\n",
      "Epoch 00014: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0208 - mape: 8797.4355 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4454\n",
      "Epoch 15/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8763.3545\n",
      "Epoch 00015: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8761.7432 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4502\n",
      "Epoch 16/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0207 - mape: 8800.9727\n",
      "Epoch 00016: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0207 - mape: 8790.7227 - val_loss: 0.0336 - val_mae: 0.0331 - val_mape: 5.5508\n",
      "Epoch 17/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8805.7637\n",
      "Epoch 00017: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 8804.1465 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5136\n",
      "Epoch 18/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8785.8975\n",
      "Epoch 00018: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8775.6660 - val_loss: 0.0324 - val_mae: 0.0320 - val_mape: 5.4366\n",
      "Epoch 19/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8766.3740\n",
      "Epoch 00019: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8756.1650 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5053\n",
      "Epoch 20/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8869.6514- ETA: 1s - loss: 0.0209 - mae: 0.0205\n",
      "Epoch 00020: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8859.3301 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5394\n",
      "Epoch 21/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8719.9990\n",
      "Epoch 00021: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8718.3955 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.4912\n",
      "Epoch 22/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8816.5059\n",
      "Epoch 00022: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8814.8848 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5017\n",
      "Epoch 23/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8851.2988\n",
      "Epoch 00023: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8840.9902 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5257\n",
      "Epoch 24/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8849.9473\n",
      "Epoch 00024: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8839.6475 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8417.9062\n",
      "Epoch 00025: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8762.0635 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5165\n",
      "Epoch 26/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8781.5459\n",
      "Epoch 00026: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8771.3203 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5305\n",
      "Epoch 27/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8805.2725\n",
      "Epoch 00027: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8795.0176 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5373\n",
      "Epoch 28/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8814.1201\n",
      "Epoch 00028: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8803.8555 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5812\n",
      "Epoch 29/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8835.2373\n",
      "Epoch 00029: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8833.6133 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.5439\n",
      "Epoch 30/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8756.9062\n",
      "Epoch 00030: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8746.7100 - val_loss: 0.0335 - val_mae: 0.0331 - val_mape: 5.5782\n",
      "Epoch 31/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8848.8691\n",
      "Epoch 00031: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8847.2422 - val_loss: 0.0345 - val_mae: 0.0342 - val_mape: 5.6903\n",
      "Epoch 32/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8786.9775\n",
      "Epoch 00032: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8785.3623 - val_loss: 0.0348 - val_mae: 0.0344 - val_mape: 5.7179\n",
      "Epoch 33/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8790.1523\n",
      "Epoch 00033: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8779.9150 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5215\n",
      "Epoch 34/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8812.7725\n",
      "Epoch 00034: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8811.1523 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5479\n",
      "Epoch 35/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8778.3477-\n",
      "Epoch 00035: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8776.7334 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5697\n",
      "Epoch 36/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8766.7051\n",
      "Epoch 00036: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8756.4961 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.6073\n",
      "Epoch 37/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0200 - mape: 8769.1572\n",
      "Epoch 00037: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0200 - mape: 8767.5449 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5441\n",
      "Epoch 38/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0201 - mape: 8747.3877\n",
      "Epoch 00038: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0201 - mape: 8745.7793 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5797\n",
      "Epoch 39/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0200 - mape: 8836.6064\n",
      "Epoch 00039: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0200 - mape: 8834.9814 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5334\n",
      "Epoch 40/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8814.8398\n",
      "Epoch 00040: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8804.5742 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5985\n",
      "Epoch 41/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8839.9746\n",
      "Epoch 00041: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8829.6797 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5734\n",
      "Epoch 42/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0198 - mape: 8831.3848\n",
      "Epoch 00042: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0202 - mae: 0.0198 - mape: 8829.7607 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5997\n",
      "Epoch 43/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0201 - mae: 0.0198 - mape: 8783.8525\n",
      "Epoch 00043: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0201 - mae: 0.0198 - mape: 8782.2383 - val_loss: 0.0337 - val_mae: 0.0334 - val_mape: 5.6261\n",
      "Epoch 44/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0197 - mape: 8822.6426\n",
      "Epoch 00044: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0197 - mape: 8821.0205 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.6350\n",
      "Epoch 45/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8898.9219\n",
      "Epoch 00045: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8888.5576 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5640\n",
      "Epoch 46/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8848.5547\n",
      "Epoch 00046: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8846.9277 - val_loss: 0.0326 - val_mae: 0.0323 - val_mape: 5.5130\n",
      "Epoch 47/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8915.1211\n",
      "Epoch 00047: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8904.7383 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5152\n",
      "Epoch 48/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8884.6523\n",
      "Epoch 00048: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8874.3057 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5568\n",
      "Epoch 49/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0195 - mape: 8901.6914\n",
      "Epoch 00049: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0195 - mape: 8891.3252 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8912.0732\n",
      "Epoch 00050: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8901.6934 - val_loss: 0.0326 - val_mae: 0.0322 - val_mape: 5.5117\n",
      "Epoch 51/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0194 - mape: 8897.9141\n",
      "Epoch 00051: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0194 - mape: 8896.2773 - val_loss: 0.0339 - val_mae: 0.0336 - val_mape: 5.6620\n",
      "Epoch 52/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8820.4697\n",
      "Epoch 00052: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8810.1973 - val_loss: 0.0336 - val_mae: 0.0333 - val_mape: 5.6221\n",
      "Epoch 53/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8908.1738\n",
      "Epoch 00053: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8906.5361 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5511\n",
      "Epoch 54/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8871.0322\n",
      "Epoch 00054: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8869.4014 - val_loss: 0.0333 - val_mae: 0.0330 - val_mape: 5.5471\n",
      "Epoch 55/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8876.8662\n",
      "Epoch 00055: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8866.5283 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5071\n",
      "Epoch 56/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8917.7432- ETA: 2s - loss: 0.0196 - ma\n",
      "Epoch 00056: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8916.1035 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.5076\n",
      "Epoch 57/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8886.0176\n",
      "Epoch 00057: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8884.3838 - val_loss: 0.0330 - val_mae: 0.0327 - val_mape: 5.6687\n",
      "Epoch 58/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8843.5547\n",
      "Epoch 00058: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0195 - mae: 0.0192 - mape: 8833.2559 - val_loss: 0.0340 - val_mae: 0.0337 - val_mape: 5.6366\n",
      "Epoch 59/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8897.1895\n",
      "Epoch 00059: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8886.8271 - val_loss: 0.0344 - val_mae: 0.0340 - val_mape: 5.7448\n",
      "Epoch 60/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8878.5566\n",
      "Epoch 00060: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0192 - mape: 8868.2227 - val_loss: 0.0338 - val_mae: 0.0334 - val_mape: 5.6100\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.92\n",
      "Median Absolute Error (MedAE): 341.57\n",
      "Mean Squared Error (MSE): 329961.29\n",
      "Root Mean Squared Error (RMSE): 574.42\n",
      "Mean Absolute Percentage Error (MAPE): 4.31 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.33 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPZ0OMjDJhnO"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UZgZB020JqhL"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5'\n",
    "start_epoch= 53+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ssm3plQeJn6F"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "u-3UhfYuJlge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "9b_pJN2ZJKff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9133.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03257, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 9s 283us/sample - loss: 0.0214 - mae: 0.0208 - mape: 8890.3867 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4449\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8733.1416\n",
      "Epoch 00002: val_loss improved from 0.03257 to 0.03256, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8869.8643 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4437\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9107.9521\n",
      "Epoch 00003: val_loss improved from 0.03256 to 0.03242, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8865.1064 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8707.8369\n",
      "Epoch 00004: val_loss improved from 0.03242 to 0.03233, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8851.4385 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4285\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9089.2246\n",
      "Epoch 00005: val_loss improved from 0.03233 to 0.03228, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8846.9043 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8326.9922\n",
      "Epoch 00006: val_loss improved from 0.03228 to 0.03226, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8841.4912 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8705.7549\n",
      "Epoch 00007: val_loss improved from 0.03226 to 0.03223, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8839.4424 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4216\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9068.8096\n",
      "Epoch 00008: val_loss improved from 0.03223 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8827.0107 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4208\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8693.2275  ETA: 1s - loss: 0.0213 - mae: 0.020\n",
      "Epoch 00009: val_loss improved from 0.03222 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8828.1250 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8320.9189\n",
      "Epoch 00010: val_loss improved from 0.03222 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8823.5527 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8678.0664\n",
      "Epoch 00011: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8818.6221 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9058.5664\n",
      "Epoch 00012: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8817.0547 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.8018\n",
      "Epoch 00013: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8819.4121 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9055.2490\n",
      "Epoch 00014: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8813.7979 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9056.6045\n",
      "Epoch 00015: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8815.1211 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4180\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9046.7842\n",
      "Epoch 00016: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8805.6162 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.0303\n",
      "Epoch 00017: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8811.1191 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4184\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8288.7080\n",
      "Epoch 00018: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8806.9727 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9051.8076\n",
      "Epoch 00019: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.4658 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8302.6875\n",
      "Epoch 00020: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.8389 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4196\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.1914\n",
      "Epoch 00021: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8803.5537 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8289.9629\n",
      "Epoch 00022: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8808.2617 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.5645\n",
      "Epoch 00023: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8807.9199 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8657.6855\n",
      "Epoch 00024: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.0996 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8660.9229- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00025: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4287 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4194\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.9697\n",
      "Epoch 00026: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.0215 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4190\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.5674- ETA: 1s - loss: 0.0211 - mae: 0.0205\n",
      "Epoch 00027: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.4775 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.4648\n",
      "Epoch 00028: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4198\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8674.4756\n",
      "Epoch 00029: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.5635 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7906.3037\n",
      "Epoch 00030: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.4629 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.0000\n",
      "Epoch 00031: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.7725 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4202\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9045.3105\n",
      "Epoch 00032: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.1299 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.4668  ETA: 2s - loss: 0.0211 - m\n",
      "Epoch 00033: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.4404 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8285.9170\n",
      "Epoch 00034: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.4795 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4204\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.9082\n",
      "Epoch 00035: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6709 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8681.3047- ETA: 2s - loss: 0.0210 - mae: 0.\n",
      "Epoch 00036: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.3027 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1055- ETA: 2s - loss: 0.0211 - mae: \n",
      "Epoch 00037: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.6387 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.3789\n",
      "Epoch 00038: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4212\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7911.5845\n",
      "Epoch 00039: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.6006 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4207\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7988\n",
      "Epoch 00040: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.7549 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4209\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.4160\n",
      "Epoch 00041: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8797.7119 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4218\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.8086\n",
      "Epoch 00042: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.4561 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9040.8076- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8763.4\n",
      "Epoch 00043: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.7510 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8671.4062\n",
      "Epoch 00044: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9043.8984\n",
      "Epoch 00045: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.7754 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.5801\n",
      "Epoch 00046: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4297 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4199\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.4580\n",
      "Epoch 00047: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.2539 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4226\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8656.3926\n",
      "Epoch 00048: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8795.1895 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4227\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.8350\n",
      "Epoch 00049: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6064 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9053.7900\n",
      "Epoch 00050: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8812.3799 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4222\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.2178\n",
      "Epoch 00051: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.1719 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4229\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.2744\n",
      "Epoch 00052: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.0586 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4240\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7900\n",
      "Epoch 00053: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.5859 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8664.6738\n",
      "Epoch 00054: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.6367 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4230\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8658.2246\n",
      "Epoch 00055: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.2617 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4231\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9039.0703\n",
      "Epoch 00056: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.0605 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.2510\n",
      "Epoch 00057: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.1816 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4238\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.6846\n",
      "Epoch 00058: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.4893 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4235\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.6758\n",
      "Epoch 00059: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4250\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.4639\n",
      "Epoch 00060: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.2031 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.5479\n",
      "Epoch 00061: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8801.4482 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.7363\n",
      "Epoch 00062: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.5029 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1113- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7955. - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8069\n",
      "Epoch 00063: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8803.5488 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.6602\n",
      "Epoch 00064: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8809.3477 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8292.0371\n",
      "Epoch 00065: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8800.8613 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4243\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8661.9170\n",
      "Epoch 00066: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.5088 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4233\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.0117\n",
      "Epoch 00067: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.7666 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.6230- ETA: 2s - loss: 0.0212 - mae:\n",
      "Epoch 00068: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.3867 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4254\n",
      "Epoch 69/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8285.9189\n",
      "Epoch 00069: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.3457 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4237\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.5889\n",
      "Epoch 00070: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.3418 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.3438\n",
      "Epoch 00071: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8803.1846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4249\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.1455\n",
      "Epoch 00072: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.9678 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8665.8174\n",
      "Epoch 00073: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.1689 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.3008\n",
      "Epoch 00074: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.6846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.2236\n",
      "Epoch 00075: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.9883 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4261\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.5234  ETA: 2s - loss: 0.0209 - \n",
      "Epoch 00076: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4239\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9045.4512\n",
      "Epoch 00077: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.2715 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4265\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.2568\n",
      "Epoch 00078: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0527 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4256\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9055.4580\n",
      "Epoch 00079: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.0088 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4271\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9054.0859\n",
      "Epoch 00080: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8812.6738 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9052.4775\n",
      "Epoch 00081: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.1074 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4263\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.4385\n",
      "Epoch 00082: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.1035 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.2695\n",
      "Epoch 00083: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.9912 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8680.7773\n",
      "Epoch 00084: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8681.9316\n",
      "Epoch 00085: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.9023 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4258\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.9326\n",
      "Epoch 00086: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.5898 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.4629\n",
      "Epoch 00087: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.1172 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4259\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8666.3701\n",
      "Epoch 00088: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2188 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8664.1152\n",
      "Epoch 00089: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.5791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4258\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.7578\n",
      "Epoch 00090: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4267\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8674.5449\n",
      "Epoch 00091: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.6641 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4264\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.3330\n",
      "Epoch 00092: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.8057 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8680.4805\n",
      "Epoch 00093: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8807.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4268\n",
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8669.4492\n",
      "Epoch 00094: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0020 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8670.0195\n",
      "Epoch 00095: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.6592 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.8623\n",
      "Epoch 00096: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8808.5645 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4274\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8297.3535\n",
      "Epoch 00097: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8814.3789 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.8828\n",
      "Epoch 00098: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8803.7236 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8692.5449\n",
      "Epoch 00099: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8821.6729 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4253\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.0117  ETA: 2s - loss: 0.0212 - mae: 0.0207 - mape - ETA: 1s - loss: 0.0211 - mae: 0.0206 - mape: 13501.83 - ETA: 1s - loss: 0.0211 - mae: 0.0\n",
      "Epoch 00100: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.5918 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8291.2773\n",
      "Epoch 00101: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.7295 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4274\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.1973\n",
      "Epoch 00102: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.7637 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4269\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.2109\n",
      "Epoch 00103: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.1260 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.0146\n",
      "Epoch 00104: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.8604 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8309.9893\n",
      "Epoch 00105: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.9658 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8300.4561\n",
      "Epoch 00106: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.9160 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8687.0166\n",
      "Epoch 00107: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8820.6562 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4257\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.9922  ETA: 1s - loss: 0.021\n",
      "Epoch 00108: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8818.7070 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7918.0552\n",
      "Epoch 00109: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8806.1182 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4266\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9054.2461\n",
      "Epoch 00110: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8812.8340 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 111/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.2012\n",
      "Epoch 00111: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8809.0146 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 112/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.0439\n",
      "Epoch 00112: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.5566 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 113/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8305.8838\n",
      "Epoch 00113: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.7695 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4286\n",
      "Epoch 114/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9051.7646\n",
      "Epoch 00114: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.4189 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4268\n",
      "Epoch 115/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8671.9346\n",
      "Epoch 00115: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.5635 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4283\n",
      "Epoch 116/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.4092  ETA: 2s - loss: \n",
      "Epoch 00116: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.9219 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 117/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8685.8936- ETA: 1s - loss: 0.0209 - mae: 0.02\n",
      "Epoch 00117: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8817.3330 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 118/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8302.0400\n",
      "Epoch 00118: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8812.6797 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4707\n",
      "Epoch 00119: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.0967 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 120/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8304.8125\n",
      "Epoch 00120: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8818.3047 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4265\n",
      "Epoch 121/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9058.0273\n",
      "Epoch 00121: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5293 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4270\n",
      "Epoch 122/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8682.1084  ETA: 1s - loss: 0.0208 - mae:\n",
      "Epoch 00122: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.7100 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 123/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9050.8926\n",
      "Epoch 00123: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.5742 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4287\n",
      "Epoch 124/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.2852- ETA: 2s - loss: 0.0209 - mae:\n",
      "Epoch 00124: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.8037 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4277\n",
      "Epoch 125/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.9121\n",
      "Epoch 00125: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.4541 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 126/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7553.2466\n",
      "Epoch 00126: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.8301 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 127/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.8857\n",
      "Epoch 00127: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.4443 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 128/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.3145\n",
      "Epoch 00128: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.7705 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 129/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9062.5234\n",
      "Epoch 00129: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8820.9062 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 130/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8310.6270\n",
      "Epoch 00130: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.6572 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4276\n",
      "Epoch 131/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8676.7832\n",
      "Epoch 00131: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8813.8525 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 132/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.6250\n",
      "Epoch 00132: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1289 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 133/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.2627\n",
      "Epoch 00133: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.7959 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 134/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9385\n",
      "Epoch 00134: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.0283 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 135/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9990\n",
      "Epoch 00135: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.8438 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 136/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9050.7119- ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8224.\n",
      "Epoch 00136: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.3926 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 137/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9065.7979\n",
      "Epoch 00137: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.0713 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 138/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.0449\n",
      "Epoch 00138: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.1436 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4327\n",
      "Epoch 139/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.4678\n",
      "Epoch 00139: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.9834 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 140/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.8877\n",
      "Epoch 00140: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.4102 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 141/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.1895\n",
      "Epoch 00141: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.6465 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4293\n",
      "Epoch 142/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9068.6006\n",
      "Epoch 00142: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8826.8496 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4311\n",
      "Epoch 143/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.5254\n",
      "Epoch 00143: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.0791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8303.7139\n",
      "Epoch 00144: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.9434 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 145/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9057.6035\n",
      "Epoch 00145: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1113 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 146/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8295.9824\n",
      "Epoch 00146: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 147/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8299.4932- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00147: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.4980 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 148/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8677.7822\n",
      "Epoch 00148: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.8496 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 149/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4521\n",
      "Epoch 00149: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8807.3672 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4317\n",
      "Epoch 150/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8687.6270\n",
      "Epoch 00150: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.2520 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 151/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7550.1221\n",
      "Epoch 00151: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5537 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4318\n",
      "Epoch 152/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8665.8770\n",
      "Epoch 00152: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.2012 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4307\n",
      "Epoch 153/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8305.3477\n",
      "Epoch 00153: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.7168 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 154/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8664.9102\n",
      "Epoch 00154: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8803.5449 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 155/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.1807\n",
      "Epoch 00155: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.1172 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4323\n",
      "Epoch 156/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9061.2051\n",
      "Epoch 00156: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8819.6104 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 157/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9058.3799\n",
      "Epoch 00157: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.8613 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4316\n",
      "Epoch 158/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9053.1445\n",
      "Epoch 00158: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8811.7646 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 159/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.0205\n",
      "Epoch 00159: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8813.5781 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 160/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8673.6445\n",
      "Epoch 00160: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8812.8994 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4306\n",
      "Epoch 161/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.0654\n",
      "Epoch 00161: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8804.1348 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 162/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8678.7412\n",
      "Epoch 00162: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8817.2510 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 163/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9058.5049  ETA: 1s - loss: 0.0209 - mae: 0.0204 - map\n",
      "Epoch 00163: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.9990 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4288\n",
      "Epoch 164/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8671.3730\n",
      "Epoch 00164: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8807.5947 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 165/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8304.8350\n",
      "Epoch 00165: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8825.1436 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4294\n",
      "Epoch 166/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7930.7822\n",
      "Epoch 00166: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.1318 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 167/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8676.1992  ETA: 1s - loss: 0.0209 - mae: 0.0203 - mape:  - ETA: 1s - loss: 0.0208 - mae: 0.0202 - mape: - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9\n",
      "Epoch 00167: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.6123 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8682.7363\n",
      "Epoch 00168: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.8896 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 169/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9065.5938\n",
      "Epoch 00169: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8823.9004 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 170/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9051.8340\n",
      "Epoch 00170: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.4756 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 171/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8678.6602- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8967.800\n",
      "Epoch 00171: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8815.0049 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 172/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.9189\n",
      "Epoch 00172: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.3682 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 173/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9055.6904\n",
      "Epoch 00173: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2402 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 174/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.9043- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8588.017\n",
      "Epoch 00174: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.3857 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 175/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.7666\n",
      "Epoch 00175: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3809 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 176/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8673.9277\n",
      "Epoch 00176: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.3027 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4343\n",
      "Epoch 177/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.8848\n",
      "Epoch 00177: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2305 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4321\n",
      "Epoch 178/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9054.9736  ETA: 1s - loss: 0.0208 - mae: 0.0203 \n",
      "Epoch 00178: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.5439 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4330\n",
      "Epoch 179/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8675.4736\n",
      "Epoch 00179: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8809.1797 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4327\n",
      "Epoch 180/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9061.8086\n",
      "Epoch 00180: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2021 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4329\n",
      "Epoch 181/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9060.0869\n",
      "Epoch 00181: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5107 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4319\n",
      "Epoch 182/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.6191\n",
      "Epoch 00182: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5000 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 183/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.7178\n",
      "Epoch 00183: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2656 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4324\n",
      "Epoch 184/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8291.2705\n",
      "Epoch 00184: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.1533 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4331\n",
      "Epoch 185/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.7324\n",
      "Epoch 00185: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.8145 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 186/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.2549\n",
      "Epoch 00186: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8817.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4305\n",
      "Epoch 187/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.2168\n",
      "Epoch 00187: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8819.0938 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4315\n",
      "Epoch 188/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.7939\n",
      "Epoch 00188: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.2949 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 189/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.9834\n",
      "Epoch 00189: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.4678 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 190/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.2012\n",
      "Epoch 00190: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7588 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4332\n",
      "Epoch 191/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8679.1182\n",
      "Epoch 00191: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.3721 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4325\n",
      "Epoch 192/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8303.0508\n",
      "Epoch 00192: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7334 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 193/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8677.2891\n",
      "Epoch 00193: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.0742 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4315\n",
      "Epoch 194/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9051.3916\n",
      "Epoch 00194: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.0596 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4353\n",
      "Epoch 195/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 7911.0254\n",
      "Epoch 00195: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.6299 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 196/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9053.1436\n",
      "Epoch 00196: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.7627 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4345\n",
      "Epoch 197/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9056.7646- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 73\n",
      "Epoch 00197: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.2822 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 198/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8293.9717\n",
      "Epoch 00198: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3867 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4346\n",
      "Epoch 199/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.1162\n",
      "Epoch 00199: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.7646 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4328\n",
      "Epoch 200/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.9668- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 796\n",
      "Epoch 00200: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8808.9336 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4344\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.57\n",
      "Median Absolute Error (MedAE): 338.68\n",
      "Mean Squared Error (MSE): 332645.96\n",
      "Root Mean Squared Error (RMSE): 576.75\n",
      "Mean Absolute Percentage Error (MAPE): 4.3 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.3 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-0013-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aW5pp6LD-1"
   },
   "source": [
    "# E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "j9RoOmzuLgHR"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B2H5iBwQLfhj"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "k06fUuK7LerZ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mB2kXVVCLRBA"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sizKJbygLNBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Q6uDSkmdLJlR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.1973 - mae: 0.0794 - mape: 9692.2637\n",
      "Epoch 00001: val_loss improved from inf to 0.10644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0001-loss0.11.h5\n",
      "32614/32614 [==============================] - 19s 584us/sample - loss: 0.1972 - mae: 0.0794 - mape: 9680.9932 - val_loss: 0.1064 - val_mae: 0.0766 - val_mape: 12.9892\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0557 - mae: 0.0421 - mape: 9249.9512\n",
      "Epoch 00002: val_loss improved from 0.10644 to 0.05840, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0002-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0556 - mae: 0.0420 - mape: 9230.1133 - val_loss: 0.0584 - val_mae: 0.0523 - val_mape: 8.7301\n",
      "Epoch 3/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0416 - mae: 0.0375 - mape: 9262.3770\n",
      "Epoch 00003: val_loss improved from 0.05840 to 0.05401, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0003-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0416 - mae: 0.0375 - mape: 9224.3594 - val_loss: 0.0540 - val_mae: 0.0503 - val_mape: 8.7703\n",
      "Epoch 4/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0398 - mae: 0.0367 - mape: 9258.6729\n",
      "Epoch 00004: val_loss improved from 0.05401 to 0.04371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 431us/sample - loss: 0.0398 - mae: 0.0367 - mape: 9247.8994 - val_loss: 0.0437 - val_mae: 0.0407 - val_mape: 6.7483\n",
      "Epoch 5/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0336 - mape: 9132.0303  ETA: 4s - \n",
      "Epoch 00005: val_loss did not improve from 0.04371\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0362 - mae: 0.0336 - mape: 9121.3975 - val_loss: 0.0456 - val_mae: 0.0433 - val_mape: 7.2883\n",
      "Epoch 6/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0332 - mape: 9247.4834\n",
      "Epoch 00006: val_loss improved from 0.04371 to 0.03429, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0358 - mae: 0.0332 - mape: 9227.6475 - val_loss: 0.0343 - val_mae: 0.0317 - val_mape: 5.5244\n",
      "Epoch 7/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0334 - mae: 0.0314 - mape: 8832.6689\n",
      "Epoch 00007: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0334 - mae: 0.0314 - mape: 9193.4805 - val_loss: 0.0770 - val_mae: 0.0752 - val_mape: 10.8461\n",
      "Epoch 8/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0309 - mape: 9279.7900\n",
      "Epoch 00008: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0326 - mae: 0.0309 - mape: 9241.6865 - val_loss: 0.0911 - val_mae: 0.0895 - val_mape: 12.6461\n",
      "Epoch 9/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0301 - mape: 9240.0088\n",
      "Epoch 00009: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0315 - mae: 0.0301 - mape: 9211.1299 - val_loss: 0.0497 - val_mae: 0.0483 - val_mape: 7.3868\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0289 - mape: 8889.9570\n",
      "Epoch 00010: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0303 - mae: 0.0289 - mape: 9243.3398 - val_loss: 0.0727 - val_mae: 0.0714 - val_mape: 10.6575\n",
      "Epoch 11/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0290 - mape: 9401.6436\n",
      "Epoch 00011: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0303 - mae: 0.0290 - mape: 9372.2676 - val_loss: 0.0601 - val_mae: 0.0589 - val_mape: 8.9566\n",
      "Epoch 12/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0292 - mae: 0.0280 - mape: 9176.0518\n",
      "Epoch 00012: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0292 - mae: 0.0280 - mape: 9147.3730 - val_loss: 0.0464 - val_mae: 0.0452 - val_mape: 7.1787\n",
      "Epoch 13/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0291 - mae: 0.0279 - mape: 9234.7246\n",
      "Epoch 00013: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0291 - mae: 0.0279 - mape: 9214.9170 - val_loss: 0.0398 - val_mae: 0.0387 - val_mape: 6.3188\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.0278 - mape: 9259.1611\n",
      "Epoch 00014: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0290 - mae: 0.0278 - mape: 9221.1396 - val_loss: 0.0613 - val_mae: 0.0602 - val_mape: 8.9035\n",
      "Epoch 15/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0285 - mae: 0.0274 - mape: 9332.2979\n",
      "Epoch 00015: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0285 - mae: 0.0274 - mape: 9293.9756 - val_loss: 0.0448 - val_mae: 0.0438 - val_mape: 6.8179\n",
      "Epoch 16/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9270.8516- ETA: 0s - loss: 0.0278 - mae: 0.0268 - ma\n",
      "Epoch 00016: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9269.1475 - val_loss: 0.0455 - val_mae: 0.0446 - val_mape: 7.0034\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9212.3809\n",
      "Epoch 00017: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9210.6865 - val_loss: 0.0670 - val_mae: 0.0661 - val_mape: 9.5463\n",
      "Epoch 18/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9337.9092\n",
      "Epoch 00018: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9308.7227 - val_loss: 0.0456 - val_mae: 0.0447 - val_mape: 7.0638\n",
      "Epoch 19/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9224.4463\n",
      "Epoch 00019: val_loss improved from 0.03429 to 0.03249, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0019-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9213.7041 - val_loss: 0.0325 - val_mae: 0.0316 - val_mape: 5.3572\n",
      "Epoch 20/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9302.4258\n",
      "Epoch 00020: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9264.2266 - val_loss: 0.0382 - val_mae: 0.0373 - val_mape: 6.0539\n",
      "Epoch 21/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9244.2197\n",
      "Epoch 00021: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 436us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9206.2666 - val_loss: 0.0409 - val_mae: 0.0400 - val_mape: 6.2748\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0260 - mape: 9348.6699\n",
      "Epoch 00022: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0269 - mae: 0.0260 - mape: 9337.7842 - val_loss: 0.0328 - val_mae: 0.0319 - val_mape: 5.4575\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9254.0684\n",
      "Epoch 00023: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0266 - mae: 0.0257 - mape: 9252.3672 - val_loss: 0.0552 - val_mae: 0.0543 - val_mape: 8.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9298.8604\n",
      "Epoch 00024: val_loss improved from 0.03249 to 0.03114, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0024-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0265 - mae: 0.0257 - mape: 9260.6738 - val_loss: 0.0311 - val_mae: 0.0303 - val_mape: 5.1846\n",
      "Epoch 25/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0254 - mape: 9225.4922\n",
      "Epoch 00025: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0262 - mae: 0.0254 - mape: 9223.7969 - val_loss: 0.0519 - val_mae: 0.0511 - val_mape: 7.9640\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0254 - mape: 9168.1641\n",
      "Epoch 00026: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0263 - mae: 0.0254 - mape: 9139.5078 - val_loss: 0.0324 - val_mae: 0.0315 - val_mape: 5.4029\n",
      "Epoch 27/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0252 - mape: 9292.8506\n",
      "Epoch 00027: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 435us/sample - loss: 0.0261 - mae: 0.0252 - mape: 9282.0293 - val_loss: 0.0368 - val_mae: 0.0359 - val_mape: 5.8316\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0250 - mape: 9257.2148\n",
      "Epoch 00028: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0259 - mae: 0.0250 - mape: 9246.4346 - val_loss: 0.0322 - val_mae: 0.0313 - val_mape: 5.3347\n",
      "Epoch 29/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0249 - mape: 9201.8809\n",
      "Epoch 00029: val_loss improved from 0.03114 to 0.03017, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0257 - mae: 0.0249 - mape: 9173.1152 - val_loss: 0.0302 - val_mae: 0.0293 - val_mape: 5.0982\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0246 - mape: 9224.1299\n",
      "Epoch 00030: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0255 - mae: 0.0246 - mape: 9195.2969 - val_loss: 0.0316 - val_mae: 0.0308 - val_mape: 5.3475\n",
      "Epoch 31/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0247 - mape: 9221.6504\n",
      "Epoch 00031: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0256 - mae: 0.0247 - mape: 9219.9551 - val_loss: 0.0383 - val_mae: 0.0375 - val_mape: 6.0447\n",
      "Epoch 32/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9185.3818\n",
      "Epoch 00032: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0253 - mae: 0.0244 - mape: 9156.6689 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.6061\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9151.5020\n",
      "Epoch 00033: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0252 - mae: 0.0243 - mape: 9104.9512 - val_loss: 0.0435 - val_mae: 0.0426 - val_mape: 6.6821\n",
      "Epoch 34/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0240 - mape: 9209.8779\n",
      "Epoch 00034: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0248 - mae: 0.0240 - mape: 9163.0439 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 5.9090\n",
      "Epoch 35/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0243 - mape: 9190.4863\n",
      "Epoch 00035: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0251 - mae: 0.0243 - mape: 9170.7715 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.4763\n",
      "Epoch 36/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0241 - mape: 9201.8359\n",
      "Epoch 00036: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0250 - mae: 0.0241 - mape: 9182.0967 - val_loss: 0.0393 - val_mae: 0.0384 - val_mape: 6.1283\n",
      "Epoch 37/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0247 - mae: 0.0238 - mape: 9064.8945\n",
      "Epoch 00037: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0247 - mae: 0.0238 - mape: 9063.2285 - val_loss: 0.0322 - val_mae: 0.0314 - val_mape: 5.4603\n",
      "Epoch 38/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0236 - mape: 9153.4014\n",
      "Epoch 00038: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0245 - mae: 0.0236 - mape: 9142.7412 - val_loss: 0.0396 - val_mae: 0.0387 - val_mape: 6.1510\n",
      "Epoch 39/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9212.5186\n",
      "Epoch 00039: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9183.7207 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5752\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0237 - mape: 9298.7363\n",
      "Epoch 00040: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0246 - mae: 0.0237 - mape: 9278.7891 - val_loss: 0.0358 - val_mae: 0.0349 - val_mape: 5.7883\n",
      "Epoch 41/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9290.7383\n",
      "Epoch 00041: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9252.5859 - val_loss: 0.0359 - val_mae: 0.0350 - val_mape: 6.1253\n",
      "Epoch 42/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0244 - mae: 0.0235 - mape: 9100.7422\n",
      "Epoch 00042: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0244 - mae: 0.0235 - mape: 9099.0762 - val_loss: 0.0365 - val_mae: 0.0356 - val_mape: 5.8232\n",
      "Epoch 43/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.0231 - mape: 9206.7373\n",
      "Epoch 00043: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0240 - mae: 0.0231 - mape: 9186.9854 - val_loss: 0.0428 - val_mae: 0.0419 - val_mape: 6.5949\n",
      "Epoch 44/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.0229 - mape: 9111.2773\n",
      "Epoch 00044: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0238 - mae: 0.0229 - mape: 9109.6025 - val_loss: 0.0335 - val_mae: 0.0326 - val_mape: 5.5624\n",
      "Epoch 45/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0229 - mape: 9251.8613\n",
      "Epoch 00045: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0229 - mape: 9250.1602 - val_loss: 0.0383 - val_mae: 0.0374 - val_mape: 6.0213\n",
      "Epoch 46/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0228 - mape: 9249.3652\n",
      "Epoch 00046: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9229.5244 - val_loss: 0.0470 - val_mae: 0.0461 - val_mape: 7.1372\n",
      "Epoch 47/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0228 - mape: 9240.2764\n",
      "Epoch 00047: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9220.4561 - val_loss: 0.0355 - val_mae: 0.0346 - val_mape: 5.8071\n",
      "Epoch 48/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0227 - mape: 9228.7842\n",
      "Epoch 00048: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0236 - mae: 0.0227 - mape: 9227.0869 - val_loss: 0.0335 - val_mae: 0.0327 - val_mape: 5.7051\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0225 - mape: 9308.8867\n",
      "Epoch 00049: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0234 - mae: 0.0225 - mape: 9279.7959 - val_loss: 0.0391 - val_mae: 0.0382 - val_mape: 6.2820\n",
      "Epoch 50/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0223 - mape: 8910.0205\n",
      "Epoch 00050: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0232 - mae: 0.0223 - mape: 9253.5254 - val_loss: 0.0347 - val_mae: 0.0338 - val_mape: 5.7545\n",
      "Epoch 51/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0224 - mape: 9279.0811\n",
      "Epoch 00051: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0233 - mae: 0.0224 - mape: 9277.3750 - val_loss: 0.0390 - val_mae: 0.0381 - val_mape: 6.1803\n",
      "Epoch 52/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0222 - mape: 9221.8008\n",
      "Epoch 00052: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0231 - mae: 0.0222 - mape: 9202.0244 - val_loss: 0.0355 - val_mae: 0.0347 - val_mape: 5.8992\n",
      "Epoch 53/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0218 - mape: 9313.6279\n",
      "Epoch 00053: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0227 - mae: 0.0219 - mape: 9284.5176 - val_loss: 0.0346 - val_mae: 0.0337 - val_mape: 5.8458\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0221 - mape: 9235.9512\n",
      "Epoch 00054: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0230 - mae: 0.0221 - mape: 9207.0791 - val_loss: 0.0399 - val_mae: 0.0391 - val_mape: 6.3575\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.0219 - mape: 9211.0430\n",
      "Epoch 00055: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0228 - mae: 0.0219 - mape: 9191.2910 - val_loss: 0.0397 - val_mae: 0.0388 - val_mape: 6.2600\n",
      "Epoch 56/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0217 - mape: 9251.5020\n",
      "Epoch 00056: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0226 - mae: 0.0217 - mape: 9240.7275 - val_loss: 0.0405 - val_mae: 0.0397 - val_mape: 6.3565\n",
      "Epoch 57/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 9213.3115\n",
      "Epoch 00057: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9166.4385 - val_loss: 0.0343 - val_mae: 0.0334 - val_mape: 5.7472\n",
      "Epoch 58/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0216 - mape: 9254.9170\n",
      "Epoch 00058: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9253.2207 - val_loss: 0.0376 - val_mae: 0.0367 - val_mape: 6.0893\n",
      "Epoch 59/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0220 - mape: 9354.7930\n",
      "Epoch 00059: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0230 - mae: 0.0220 - mape: 9325.5488 - val_loss: 0.0341 - val_mae: 0.0332 - val_mape: 5.7046\n",
      "Epoch 60/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0214 - mape: 9235.1680\n",
      "Epoch 00060: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0222 - mae: 0.0214 - mape: 9188.1836 - val_loss: 0.0366 - val_mae: 0.0358 - val_mape: 5.9825\n",
      "Epoch 61/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0215 - mape: 9196.2959\n",
      "Epoch 00061: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0223 - mae: 0.0215 - mape: 9167.5479 - val_loss: 0.0399 - val_mae: 0.0390 - val_mape: 6.2796\n",
      "Epoch 62/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0213 - mape: 9107.4619\n",
      "Epoch 00062: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0222 - mae: 0.0213 - mape: 9087.9248 - val_loss: 0.0385 - val_mae: 0.0376 - val_mape: 6.1628\n",
      "Epoch 63/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0213 - mape: 9210.2637\n",
      "Epoch 00063: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0221 - mae: 0.0213 - mape: 9172.4463 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5972\n",
      "Epoch 64/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9124.6074\n",
      "Epoch 00064: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9122.9297 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 6.0281\n",
      "Epoch 65/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0212 - mape: 9226.6611\n",
      "Epoch 00065: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0220 - mae: 0.0212 - mape: 9188.7695 - val_loss: 0.0377 - val_mae: 0.0369 - val_mape: 6.2115\n",
      "Epoch 66/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9052.7314\n",
      "Epoch 00066: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9051.0664 - val_loss: 0.0405 - val_mae: 0.0396 - val_mape: 6.4286\n",
      "Epoch 67/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0209 - mape: 9083.4209\n",
      "Epoch 00067: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0218 - mae: 0.0209 - mape: 9072.8418 - val_loss: 0.0365 - val_mae: 0.0357 - val_mape: 6.0204\n",
      "Epoch 68/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9076.5205\n",
      "Epoch 00068: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9039.2510 - val_loss: 0.0342 - val_mae: 0.0334 - val_mape: 5.6559\n",
      "Epoch 69/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0208 - mape: 9033.5391\n",
      "Epoch 00069: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0217 - mae: 0.0208 - mape: 9005.3018 - val_loss: 0.0396 - val_mae: 0.0388 - val_mape: 6.4502\n",
      "Epoch 70/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9120.9199\n",
      "Epoch 00070: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9083.4629 - val_loss: 0.0361 - val_mae: 0.0353 - val_mape: 5.8008\n",
      "Epoch 71/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9097.3145\n",
      "Epoch 00071: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9095.6416 - val_loss: 0.0361 - val_mae: 0.0352 - val_mape: 5.9642\n",
      "Epoch 72/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0206 - mape: 9088.2021\n",
      "Epoch 00072: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0206 - mape: 9086.5312 - val_loss: 0.0380 - val_mae: 0.0372 - val_mape: 6.1542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9105.6396\n",
      "Epoch 00073: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9059.3135 - val_loss: 0.0434 - val_mae: 0.0426 - val_mape: 6.7060\n",
      "Epoch 74/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0205 - mape: 9066.7041\n",
      "Epoch 00074: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0205 - mape: 9065.0361 - val_loss: 0.0346 - val_mae: 0.0338 - val_mape: 5.7416\n",
      "Epoch 75/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0205 - mape: 8978.4404\n",
      "Epoch 00075: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0213 - mae: 0.0205 - mape: 8967.9941 - val_loss: 0.0348 - val_mae: 0.0340 - val_mape: 5.9216\n",
      "Epoch 76/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9057.9648\n",
      "Epoch 00076: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0210 - mae: 0.0202 - mape: 9056.2988 - val_loss: 0.0398 - val_mae: 0.0390 - val_mape: 6.3300\n",
      "Epoch 77/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0203 - mape: 8981.2236\n",
      "Epoch 00077: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0212 - mae: 0.0203 - mape: 8979.5723 - val_loss: 0.0451 - val_mae: 0.0444 - val_mape: 7.0400\n",
      "Epoch 78/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9038.3955\n",
      "Epoch 00078: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0210 - mae: 0.0202 - mape: 8992.4121 - val_loss: 0.0360 - val_mae: 0.0353 - val_mape: 5.9399\n",
      "Epoch 79/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0201 - mape: 8948.2764\n",
      "Epoch 00079: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0209 - mae: 0.0201 - mape: 8920.3037 - val_loss: 0.0350 - val_mae: 0.0342 - val_mape: 5.7982\n",
      "Epoch 80/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0200 - mape: 8990.1953\n",
      "Epoch 00080: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0208 - mae: 0.0200 - mape: 8962.0898 - val_loss: 0.0358 - val_mae: 0.0350 - val_mape: 5.8451\n"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "cW3NWXtXLDA-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 420.56\n",
      "Median Absolute Error (MedAE): 333.06\n",
      "Mean Squared Error (MSE): 313586.16\n",
      "Root Mean Squared Error (RMSE): 559.99\n",
      "Mean Absolute Percentage Error (MAPE): 4.19 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.25 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5'\n",
    "start_epoch= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0228 - mape: 9138.6191- ETA: 3s - loss: 0.023\n",
      "Epoch 00001: val_loss improved from inf to 0.03148, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 56s 2ms/sample - loss: 0.0235 - mae: 0.0228 - mape: 9136.9395 - val_loss: 0.0315 - val_mae: 0.0307 - val_mape: 5.2145\n",
      "Epoch 2/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0225 - mape: 9164.0752\n",
      "Epoch 00002: val_loss improved from 0.03148 to 0.03063, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0232 - mae: 0.0225 - mape: 9162.3906 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1229\n",
      "Epoch 3/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0224 - mape: 9196.9521- ETA: 3s - loss: 0.0\n",
      "Epoch 00003: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0231 - mae: 0.0224 - mape: 9195.2607 - val_loss: 0.0314 - val_mae: 0.0307 - val_mape: 5.2105\n",
      "Epoch 4/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0223 - mape: 9131.6660\n",
      "Epoch 00004: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0230 - mae: 0.0223 - mape: 9121.0312 - val_loss: 0.0310 - val_mae: 0.0304 - val_mape: 5.1898\n",
      "Epoch 5/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.0223 - mape: 9159.3213\n",
      "Epoch 00005: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0229 - mae: 0.0223 - mape: 9148.6533 - val_loss: 0.0321 - val_mae: 0.0314 - val_mape: 5.2817\n",
      "Epoch 6/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9180.6631\n",
      "Epoch 00006: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0228 - mae: 0.0221 - mape: 9178.9824 - val_loss: 0.0309 - val_mae: 0.0303 - val_mape: 5.1679\n",
      "Epoch 7/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9175.2949\n",
      "Epoch 00007: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0227 - mae: 0.0221 - mape: 9173.6084 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.2847\n",
      "Epoch 8/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0220 - mape: 9205.9355\n",
      "Epoch 00008: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0226 - mae: 0.0220 - mape: 9195.2217 - val_loss: 0.0307 - val_mae: 0.0301 - val_mape: 5.2188\n",
      "Epoch 9/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0219 - mape: 9181.9873\n",
      "Epoch 00009: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0225 - mae: 0.0219 - mape: 9180.2988 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.2595\n",
      "Epoch 10/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0218 - mape: 9186.6875\n",
      "Epoch 00010: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0224 - mae: 0.0218 - mape: 9184.9990 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2146\n",
      "Epoch 11/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0217 - mape: 9203.6816\n",
      "Epoch 00011: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0223 - mae: 0.0217 - mape: 9192.9629 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.2609\n",
      "Epoch 12/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9241.5742\n",
      "Epoch 00012: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 9239.8750 - val_loss: 0.0311 - val_mae: 0.0305 - val_mape: 5.2300\n",
      "Epoch 13/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0216 - mape: 9247.9932\n",
      "Epoch 00013: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0216 - mape: 9237.2227 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4177\n",
      "Epoch 14/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0215 - mape: 9297.9727\n",
      "Epoch 00014: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0215 - mape: 9296.2627 - val_loss: 0.0307 - val_mae: 0.0302 - val_mape: 5.2173\n",
      "Epoch 15/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0214 - mape: 9263.5205\n",
      "Epoch 00015: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0220 - mae: 0.0214 - mape: 9261.8164 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2651\n",
      "Epoch 16/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 9198.6846\n",
      "Epoch 00016: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 9196.9941 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.3174\n",
      "Epoch 17/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 9260.9404\n",
      "Epoch 00017: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 9250.1543 - val_loss: 0.0309 - val_mae: 0.0304 - val_mape: 5.2860\n",
      "Epoch 18/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0212 - mape: 9216.3271\n",
      "Epoch 00018: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0212 - mape: 9214.6318 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3829\n",
      "Epoch 19/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0211 - mape: 9211.0586\n",
      "Epoch 00019: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0211 - mape: 9200.3320 - val_loss: 0.0341 - val_mae: 0.0336 - val_mape: 5.5767\n",
      "Epoch 20/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0209 - mape: 9201.0938\n",
      "Epoch 00020: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0215 - mae: 0.0209 - mape: 9190.3779 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3684\n",
      "Epoch 21/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9219.9883\n",
      "Epoch 00021: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0214 - mae: 0.0208 - mape: 9218.2930 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3618\n",
      "Epoch 22/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 9187.7080\n",
      "Epoch 00022: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 9186.0186 - val_loss: 0.0316 - val_mae: 0.0310 - val_mape: 5.3197\n",
      "Epoch 23/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9196.0430\n",
      "Epoch 00023: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0206 - mape: 9194.3525 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2367\n",
      "Epoch 24/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9184.2910\n",
      "Epoch 00024: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 9173.5947 - val_loss: 0.0315 - val_mae: 0.0309 - val_mape: 5.2920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9166.1699\n",
      "Epoch 00025: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0210 - mae: 0.0205 - mape: 9155.4951 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.3492\n",
      "Epoch 26/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9205.6270\n",
      "Epoch 00026: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9203.9385 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2379\n",
      "Epoch 27/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9117.5986\n",
      "Epoch 00027: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9115.9219 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4364\n",
      "Epoch 28/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9204.3340\n",
      "Epoch 00028: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9193.6143 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2468\n",
      "Epoch 29/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9128.1553\n",
      "Epoch 00029: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9126.4766 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3819\n",
      "Epoch 30/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0202 - mape: 9126.3975\n",
      "Epoch 00030: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0207 - mae: 0.0202 - mape: 9124.7197 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.3127\n",
      "Epoch 31/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9127.3574\n",
      "Epoch 00031: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9125.6787 - val_loss: 0.0314 - val_mae: 0.0308 - val_mape: 5.3096\n",
      "Epoch 32/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9139.1289\n",
      "Epoch 00032: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9137.4482 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3143\n",
      "Epoch 33/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0200 - mape: 9129.0029\n",
      "Epoch 00033: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0205 - mae: 0.0200 - mape: 9127.3242 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3023\n",
      "Epoch 34/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9085.1152\n",
      "Epoch 00034: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9083.4443 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3421\n",
      "Epoch 35/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9161.0391\n",
      "Epoch 00035: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9159.3545 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2771\n",
      "Epoch 36/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0198 - mape: 9147.9355\n",
      "Epoch 00036: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0203 - mae: 0.0198 - mape: 9137.2803 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3513\n",
      "Epoch 37/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9170.7051- ETA: 4s - loss: 0. - ETA: 2s - loss: 0.0203\n",
      "Epoch 00037: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9169.0186 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2897\n",
      "Epoch 38/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9186.7959\n",
      "Epoch 00038: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9176.0967 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2613\n",
      "Epoch 39/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0195 - mape: 9121.9736\n",
      "Epoch 00039: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0200 - mae: 0.0195 - mape: 9120.2959 - val_loss: 0.0348 - val_mae: 0.0343 - val_mape: 5.6403\n",
      "Epoch 40/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0194 - mape: 9143.8574\n",
      "Epoch 00040: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0199 - mae: 0.0194 - mape: 9142.1768 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3696\n",
      "Epoch 41/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0192 - mape: 9166.4883\n",
      "Epoch 00041: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0197 - mae: 0.0192 - mape: 9155.8125 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3686\n",
      "Epoch 42/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9157.3496\n",
      "Epoch 00042: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9146.6846 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3299\n",
      "Epoch 43/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9169.7188\n",
      "Epoch 00043: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9159.0391 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2889\n",
      "Epoch 44/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0190 - mape: 9109.2627\n",
      "Epoch 00044: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0195 - mae: 0.0190 - mape: 9107.5879 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3664\n",
      "Epoch 45/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0189 - mape: 9101.7988\n",
      "Epoch 00045: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0195 - mae: 0.0189 - mape: 9100.1250 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4306\n",
      "Epoch 46/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0194 - mae: 0.0189 - mape: 9152.3506\n",
      "Epoch 00046: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0194 - mae: 0.0189 - mape: 9150.6680 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.3450\n",
      "Epoch 47/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0193 - mae: 0.0187 - mape: 9092.9639\n",
      "Epoch 00047: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0193 - mae: 0.0187 - mape: 9091.2920 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3590\n",
      "Epoch 48/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.0186 - mape: 9166.8369\n",
      "Epoch 00048: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0191 - mae: 0.0186 - mape: 9165.1514 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4518\n",
      "Epoch 49/200\n",
      "11776/32614 [=========>....................] - ETA: 32s - loss: 0.0191 - mae: 0.0186 - mape: 7337.6323"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-048f600f12b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 412.44\n",
      "Median Absolute Error (MedAE): 328.08\n",
      "Mean Squared Error (MSE): 301264.17\n",
      "Root Mean Squared Error (RMSE): 548.88\n",
      "Mean Absolute Percentage Error (MAPE): 4.12 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.19 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5'\n",
    "start_epoch= 30+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0218 - mape: 9021.4141\n",
      "Epoch 00001: val_loss improved from inf to 0.03075, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 7s 222us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9155.7246 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1276\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9392.5967\n",
      "Epoch 00002: val_loss improved from 0.03075 to 0.03067, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9142.1660 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1187\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 8595.9766\n",
      "Epoch 00003: val_loss improved from 0.03067 to 0.03054, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9143.0762 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1073\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9385.6611\n",
      "Epoch 00004: val_loss improved from 0.03054 to 0.03050, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9135.4072 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1036\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9377.5986\n",
      "Epoch 00005: val_loss improved from 0.03050 to 0.03046, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9127.5752 - val_loss: 0.0305 - val_mae: 0.0297 - val_mape: 5.0995\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8606.6309\n",
      "Epoch 00006: val_loss improved from 0.03046 to 0.03042, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9137.0928 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0965\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9375.1191\n",
      "Epoch 00007: val_loss improved from 0.03042 to 0.03041, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9125.1416 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9374.5898\n",
      "Epoch 00008: val_loss improved from 0.03041 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9124.6318 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0924\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8210.0459\n",
      "Epoch 00009: val_loss improved from 0.03038 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9126.5254 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0921\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8572.6328  ETA: 1s - loss: 0.0225 - mae: 0.0218 \n",
      "Epoch 00010: val_loss improved from 0.03038 to 0.03035, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9118.1846 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0894\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8207.2930\n",
      "Epoch 00011: val_loss did not improve from 0.03035\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9111.9365 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0895\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9357.2139\n",
      "Epoch 00012: val_loss improved from 0.03035 to 0.03034, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0012-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.7373 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0893\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8967.2090\n",
      "Epoch 00013: val_loss improved from 0.03034 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0859 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0887\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8584.7832\n",
      "Epoch 00014: val_loss improved from 0.03033 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0014-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9115.9170 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9359.8037\n",
      "Epoch 00015: val_loss improved from 0.03033 to 0.03030, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0015-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.2441 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0861\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8970.0029\n",
      "Epoch 00016: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9116.9443 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0868\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8182.7358\n",
      "Epoch 00017: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.9473 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0872\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9360.5244\n",
      "Epoch 00018: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.9551 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8964.7793\n",
      "Epoch 00019: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0879 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8972.5371\n",
      "Epoch 00020: val_loss improved from 0.03030 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0020-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9106.9902 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0856\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8572.1016\n",
      "Epoch 00021: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9105.2520 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0876\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9354.3223\n",
      "Epoch 00022: val_loss improved from 0.03028 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.9277 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0858\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9355.4004- ETA: 1s - loss: 0.0223 - mae: 0.0216 - mape: 9009 - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 869\n",
      "Epoch 00023: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9105.9697 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9358.2070\n",
      "Epoch 00024: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9108.6885 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0860\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8967.9619\n",
      "Epoch 00025: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.4453 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8956.2822\n",
      "Epoch 00026: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.1289 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0879\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9353.0088\n",
      "Epoch 00027: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9103.6240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.5430\n",
      "Epoch 00028: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1504 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0871\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9354.4570\n",
      "Epoch 00029: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.0547 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0886\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.4277\n",
      "Epoch 00030: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.0498 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2637\n",
      "Epoch 00031: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0391 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0890\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.9912\n",
      "Epoch 00032: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.1416 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0896\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8960.5801\n",
      "Epoch 00033: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7783 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0913\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9346.8809\n",
      "Epoch 00034: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9097.6689 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8965.5186\n",
      "Epoch 00035: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8569.2568\n",
      "Epoch 00036: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1602 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0915\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.2012  ETA: 2s - loss: 0.02\n",
      "Epoch 00037: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1436 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0920\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8574.7637\n",
      "Epoch 00038: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.4717 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0902\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.5205\n",
      "Epoch 00039: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.2197 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.8828\n",
      "Epoch 00040: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.5615 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.7637\n",
      "Epoch 00041: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.3633 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0908\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9359.6875\n",
      "Epoch 00042: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0914\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.9414\n",
      "Epoch 00043: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9099.0850 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3818\n",
      "Epoch 00044: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9072 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0957\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2998\n",
      "Epoch 00045: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0811 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8428- ETA: 2s - loss: 0.0223 - mae: 0\n",
      "Epoch 00046: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9570 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0959\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.5625\n",
      "Epoch 00047: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.1621 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0927\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8184\n",
      "Epoch 00048: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.2930 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0955\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8958.4678- ETA: 2s - loss: 0.0224 - mae: 0.0217 - mape: 4847. - ETA: 1s - loss: 0.0222 - mae: 0.0215 \n",
      "Epoch 00049: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.8057 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0937\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8977.0430\n",
      "Epoch 00050: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.9639 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0950\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.6865\n",
      "Epoch 00051: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.2705 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0954\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8964.1064\n",
      "Epoch 00052: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.1123 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0919\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.2607\n",
      "Epoch 00053: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.9170 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0989\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8959.4570\n",
      "Epoch 00054: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.2852 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0969\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.4199- ETA: 2s - loss: 0.0225 - mae: 0\n",
      "Epoch 00055: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.2910 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0990\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6221\n",
      "Epoch 00056: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1025 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0925\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.5264\n",
      "Epoch 00057: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8096 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1009\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8963.5762\n",
      "Epoch 00058: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.3652 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1010\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8584.6045\n",
      "Epoch 00059: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9108.2227 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0963\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.8242\n",
      "Epoch 00060: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.3945 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0996\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3730  ETA: 2s - loss: 0.0\n",
      "Epoch 00061: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8975 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1003\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.2998\n",
      "Epoch 00062: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9113.6562 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0987\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8572.1582\n",
      "Epoch 00063: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.7090 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1015\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6914\n",
      "Epoch 00064: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1013\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.9131\n",
      "Epoch 00065: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.5430 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1008\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9361.4287\n",
      "Epoch 00066: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9111.8457 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1032\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8968.2002\n",
      "Epoch 00067: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.6504 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1028\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.6250\n",
      "Epoch 00068: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7305 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8962.6729- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8456.346\n",
      "Epoch 00069: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9107.0010 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1072\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.2842\n",
      "Epoch 00070: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.8740 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1061\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.1514\n",
      "Epoch 00071: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9108.2100 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1067\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9350.1494\n",
      "Epoch 00072: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9100.8525 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.5088\n",
      "Epoch 00073: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9105.0918 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1070\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8561.8984\n",
      "Epoch 00074: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1073\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.5430\n",
      "Epoch 00075: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.3232 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1101\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.6846\n",
      "Epoch 00076: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.0889 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1018\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.6553\n",
      "Epoch 00077: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.3877 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1131\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.4307\n",
      "Epoch 00078: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.1025 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1066\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8972.5918\n",
      "Epoch 00079: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7490 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.0195\n",
      "Epoch 00080: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.7041 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1059\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.1738\n",
      "Epoch 00081: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.8281 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1086\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9359.2744\n",
      "Epoch 00082: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7461 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1125\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9355.9951\n",
      "Epoch 00083: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.5498 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1091\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.0264\n",
      "Epoch 00084: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9111.4297 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8973.9590\n",
      "Epoch 00085: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.7012 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1117\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8980.7080\n",
      "Epoch 00086: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.9180 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1181\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9362.3105\n",
      "Epoch 00087: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.6982 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1146\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.5547\n",
      "Epoch 00088: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1924 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.5771\n",
      "Epoch 00089: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.1777 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1158\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.7559\n",
      "Epoch 00090: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.4727 - val_loss: 0.0304 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9344.5049\n",
      "Epoch 00091: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9095.3486 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1201\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.3213\n",
      "Epoch 00092: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.6084 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1109\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.4619\n",
      "Epoch 00093: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1074 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9357.2207\n",
      "Epoch 00094: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.7432 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1149\n",
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.8730\n",
      "Epoch 00095: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.5840 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9366.0117\n",
      "Epoch 00096: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9116.2979 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1090\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 7807.1338- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape:\n",
      "Epoch 00097: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9097.1768 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8969.1562\n",
      "Epoch 00098: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9108.1855 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1151\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9350.9482  ETA\n",
      "Epoch 00099: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.6289 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1170\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.7568\n",
      "Epoch 00100: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.3867 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1185\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.9570\n",
      "Epoch 00101: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.5547 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.7695- ETA: 1s - loss: 0.0222 - mae: 0.0215 - ma\n",
      "Epoch 00102: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.2959 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1222\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9360.5537\n",
      "Epoch 00103: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.9805 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1187\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8967.6729\n",
      "Epoch 00104: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1299 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1163\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.5879\n",
      "Epoch 00105: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.3262 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1221\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.9297\n",
      "Epoch 00106: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9114.2559 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1136\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8575.9648\n",
      "Epoch 00107: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.5449 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1218\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.1768\n",
      "Epoch 00108: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9098.9189 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1236\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.6055\n",
      "Epoch 00109: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.2510 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1214\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0214 - mape: 9352.8691\n",
      "Epoch 00110: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0221 - mae: 0.0215 - mape: 9103.5098 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1184\n",
      "Epoch 111/200\n",
      "23552/32614 [====================>.........] - ETA: 0s - loss: 0.0220 - mae: 0.0213 - mape: 9332.0186"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-aa18ff9589c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 411.01\n",
      "Median Absolute Error (MedAE): 326.15\n",
      "Mean Squared Error (MSE): 300114.5\n",
      "Root Mean Squared Error (RMSE): 547.83\n",
      "Mean Absolute Percentage Error (MAPE): 4.11 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.17 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-0022-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
