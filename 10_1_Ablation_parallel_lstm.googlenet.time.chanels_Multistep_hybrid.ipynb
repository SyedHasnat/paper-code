{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tWP4R-BZmaZi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\pyimagesearch')\n",
    "from timeseires.CNN1D.GoogLeNet1D import PC\n",
    "from timeseires.utils.to_split import to_split\n",
    "#from timeseires.callbacks.EpochCheckpoint import EpochCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeseires.callbacks.TrainingMonitor import TrainingMonitor\n",
    "from timeseires.utils.load_only_multi_step import load_only_multi_step\n",
    "from timeseires.utils.multivariate_multi_step import multivariate_multi_step\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD  \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,MaxPooling1D,Concatenate,AveragePooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "import pandas as pd\n",
    "import time, pickle\n",
    "import pydot\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, Reshape, Dense, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten, LeakyReLU, concatenate\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12130, 21), (24259, 21))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tr = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_train.csv'\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values \n",
    "\n",
    "path_v = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_validation.csv'\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "\n",
    "path_te = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_test.csv'\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "#.......................................................................................................\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_scaler.pkl\", 'rb'))\n",
    "df_te.shape,df_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nmYgpwTPnsf4"
   },
   "outputs": [],
   "source": [
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIYjXnV81ZHC",
    "outputId": "4520a0c1-81f1-4cf6-9b17-d15f5ae9c7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.34879446029663086 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X, train_y = multivariate_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24,ahead=0)\n",
    "validation_X, validation_y = multivariate_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24,ahead=0)\n",
    "test_X, test_y = multivariate_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24,ahead=0)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.2447354793548584 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X_load, train_y_load = load_only_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24)\n",
    "validation_X_load, validation_y_load = load_only_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24)\n",
    "test_X_load, test_y_load = load_only_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PbWIXWwSJN3k"
   },
   "outputs": [],
   "source": [
    "#EXP 1 STEPS 24 LOOKBACK 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_goog():   \n",
    "    inputs_combine = Input(shape=(time_steps, num_features))\n",
    "    lstm1   = LSTM(20, return_sequences = True)(inputs_combine)\n",
    "    lstm2   = LSTM(20)(lstm1)\n",
    "    #--------------------------------------------------------\n",
    "    inputs_load = Input(shape=(time_steps, 1))\n",
    "    l = Bidirectional(LSTM(32, return_sequences=True))(inputs_load)\n",
    "    l = Bidirectional(LSTM(32, return_sequences=True))(l)\n",
    "    \n",
    "    c11 = PC.inception_module(inputs_load, 32, 32, 64, 8, 32, \"1a\", 0.01)\n",
    "    c12 = Concatenate(axis=-1)([inputs_load, c11])\n",
    "    \n",
    "    c21 = PC.inception_module(c12, 64, 64, 128, 16, 64, \"2a\", 0.01)\n",
    "    c22 = Concatenate(axis=-1)([ c11, c21])\n",
    "    \n",
    "    c31 = PC.inception_module(c22, 64, 64, 128, 16, 64, \"3a\", 0.01)\n",
    "    #---------------------------------------------------------\n",
    "    y = Concatenate(axis=-1)([inputs_load, l, c21, c31])\n",
    "\n",
    "    #y= PC.conv_module(y, 128, 3, 1)\n",
    "    x = Flatten()(y)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([x, lstm2])\n",
    "    \n",
    "    x = Dense(24, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inputs_load,inputs_combine], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = review_goog()\n",
    "tensorflow.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 24, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_conv (Conv1D)        (None, 24, 32)       64          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_conv (Conv1D)         (None, 24, 8)        16          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_bn (BatchNormalizati (None, 24, 32)       128         1a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_bn (BatchNormalizatio (None, 24, 8)        32          1a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_act (Activation)     (None, 24, 32)       0           1a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_act (Activation)      (None, 24, 8)        0           1a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_conv (Conv1D)          (None, 24, 32)       64          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_conv (Conv1D)        (None, 24, 64)       6208        1a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_conv (Conv1D)         (None, 24, 32)       1312        1a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_bn (BatchNormalization (None, 24, 32)       128         1a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_bn (BatchNormalizati (None, 24, 64)       256         1a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_bn (BatchNormalizatio (None, 24, 32)       128         1a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_act (Activation)       (None, 24, 32)       0           1a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_act (Activation)     (None, 24, 64)       0           1a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_act (Activation)      (None, 24, 32)       0           1a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_mixed (Concatenate)          (None, 24, 128)      0           1a_first_act[0][0]               \n",
      "                                                                 1a_second2_act[0][0]             \n",
      "                                                                 1a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 24, 129)      0           input_2[0][0]                    \n",
      "                                                                 1a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_conv (Conv1D)        (None, 24, 64)       8320        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_conv (Conv1D)         (None, 24, 16)       2080        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_bn (BatchNormalizati (None, 24, 64)       256         2a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_bn (BatchNormalizatio (None, 24, 16)       64          2a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_act (Activation)     (None, 24, 64)       0           2a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_act (Activation)      (None, 24, 16)       0           2a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_conv (Conv1D)          (None, 24, 64)       8320        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_conv (Conv1D)        (None, 24, 128)      24704       2a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_conv (Conv1D)         (None, 24, 64)       5184        2a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_bn (BatchNormalization (None, 24, 64)       256         2a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_bn (BatchNormalizati (None, 24, 128)      512         2a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_bn (BatchNormalizatio (None, 24, 64)       256         2a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_act (Activation)       (None, 24, 64)       0           2a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_act (Activation)     (None, 24, 128)      0           2a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_act (Activation)      (None, 24, 64)       0           2a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_mixed (Concatenate)          (None, 24, 256)      0           2a_first_act[0][0]               \n",
      "                                                                 2a_second2_act[0][0]             \n",
      "                                                                 2a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24, 384)      0           1a_mixed[0][0]                   \n",
      "                                                                 2a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_conv (Conv1D)        (None, 24, 64)       24640       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_conv (Conv1D)         (None, 24, 16)       6160        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_bn (BatchNormalizati (None, 24, 64)       256         3a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_bn (BatchNormalizatio (None, 24, 16)       64          3a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_act (Activation)     (None, 24, 64)       0           3a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_act (Activation)      (None, 24, 16)       0           3a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_conv (Conv1D)          (None, 24, 64)       24640       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_conv (Conv1D)        (None, 24, 128)      24704       3a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_conv (Conv1D)         (None, 24, 64)       5184        3a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_bn (BatchNormalization (None, 24, 64)       256         3a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_bn (BatchNormalizati (None, 24, 128)      512         3a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_bn (BatchNormalizatio (None, 24, 64)       256         3a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 24, 64)       8704        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_act (Activation)       (None, 24, 64)       0           3a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_act (Activation)     (None, 24, 128)      0           3a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_act (Activation)      (None, 24, 64)       0           3a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 64)       24832       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_mixed (Concatenate)          (None, 24, 256)      0           3a_first_act[0][0]               \n",
      "                                                                 3a_second2_act[0][0]             \n",
      "                                                                 3a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 24, 21)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 24, 577)      0           input_2[0][0]                    \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 2a_mixed[0][0]                   \n",
      "                                                                 3a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 24, 20)       3360        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 13848)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20)           3280        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 13868)        0           flatten[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 24)           332856      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 517,992\n",
      "Trainable params: 516,312\n",
      "Non-trainable params: 1,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9rQrz9B0JWvR"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\multistep\\\\1_orignal\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\multistep\\\\1_orignal'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h6NbczAlOpSQ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NkF7jg_XO346"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-rIEZj0PCPF",
    "outputId": "95edda02-84de-4f8a-83a2-b4d53387049d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = review_goog()\n",
    "    opt = Adam(0.001)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 0.0001/2)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-o0qgceiPFdI",
    "outputId": "ebb2c588-e73f-4182-baad-2f4c28f3db95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.1760 - mae: 0.0540 - mape: 2262.5122\n",
      "Epoch 00001: val_loss improved from inf to 0.03669, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0001-loss0.04.h5\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.1760 - mae: 0.0540 - mape: 2262.5122 - val_loss: 0.0367 - val_mae: 0.0359 - val_mape: 17.7322\n",
      "Epoch 2/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0352 - mae: 0.0345 - mape: 2326.0383\n",
      "Epoch 00002: val_loss did not improve from 0.03669\n",
      "2652/2652 [==============================] - 57s 22ms/step - loss: 0.0352 - mae: 0.0345 - mape: 2325.3015 - val_loss: 0.0368 - val_mae: 0.0362 - val_mape: 17.7171\n",
      "Epoch 3/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0341 - mae: 0.0336 - mape: 2340.6323\n",
      "Epoch 00003: val_loss improved from 0.03669 to 0.03655, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0003-loss0.04.h5\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0341 - mae: 0.0336 - mape: 2339.8904 - val_loss: 0.0365 - val_mae: 0.0361 - val_mape: 16.7524\n",
      "Epoch 4/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0335 - mae: 0.0331 - mape: 2323.3779- ETA: 1s - loss: 0.0335 \n",
      "Epoch 00004: val_loss improved from 0.03655 to 0.03417, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0004-loss0.03.h5\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0335 - mae: 0.0331 - mape: 2321.7693 - val_loss: 0.0342 - val_mae: 0.0338 - val_mape: 15.1937\n",
      "Epoch 5/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0329 - mae: 0.0326 - mape: 2318.7959\n",
      "Epoch 00005: val_loss did not improve from 0.03417\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0329 - mae: 0.0326 - mape: 2318.7959 - val_loss: 0.0345 - val_mae: 0.0342 - val_mape: 15.1044\n",
      "Epoch 6/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0324 - mae: 0.0322 - mape: 2331.5776\n",
      "Epoch 00006: val_loss did not improve from 0.03417\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0324 - mae: 0.0322 - mape: 2331.5776 - val_loss: 0.0346 - val_mae: 0.0344 - val_mape: 14.7238\n",
      "Epoch 7/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0319 - mae: 0.0318 - mape: 2217.5645\n",
      "Epoch 00007: val_loss improved from 0.03417 to 0.03362, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0007-loss0.03.h5\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0319 - mae: 0.0318 - mape: 2217.5645 - val_loss: 0.0336 - val_mae: 0.0335 - val_mape: 15.1394\n",
      "Epoch 8/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0314 - mae: 0.0313 - mape: 2225.3564\n",
      "Epoch 00008: val_loss did not improve from 0.03362\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0314 - mae: 0.0313 - mape: 2225.3564 - val_loss: 0.0350 - val_mae: 0.0350 - val_mape: 15.6358\n",
      "Epoch 9/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0309 - mae: 0.0309 - mape: 2224.8503\n",
      "Epoch 00009: val_loss improved from 0.03362 to 0.03240, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0009-loss0.03.h5\n",
      "2652/2652 [==============================] - 57s 21ms/step - loss: 0.0309 - mae: 0.0309 - mape: 2224.1455 - val_loss: 0.0324 - val_mae: 0.0324 - val_mape: 14.2995\n",
      "Epoch 10/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0305 - mae: 0.0305 - mape: 2243.6431\n",
      "Epoch 00010: val_loss improved from 0.03240 to 0.03219, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0010-loss0.03.h5\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0305 - mae: 0.0305 - mape: 2242.0886 - val_loss: 0.0322 - val_mae: 0.0322 - val_mape: 13.8891\n",
      "Epoch 11/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0303 - mae: 0.0303 - mape: 2185.5303\n",
      "Epoch 00011: val_loss improved from 0.03219 to 0.03194, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0011-loss0.03.h5\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0303 - mae: 0.0303 - mape: 2185.5303 - val_loss: 0.0319 - val_mae: 0.0319 - val_mape: 14.0244\n",
      "Epoch 12/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0299 - mae: 0.0299 - mape: 2262.1348- ETA: 3s - loss: 0.0299 - mae: 0.0299 - mape: 2229 - E\n",
      "Epoch 00012: val_loss did not improve from 0.03194\n",
      "2652/2652 [==============================] - 57s 22ms/step - loss: 0.0299 - mae: 0.0299 - mape: 2261.4189 - val_loss: 0.0330 - val_mae: 0.0330 - val_mape: 14.7503\n",
      "Epoch 13/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0296 - mae: 0.0296 - mape: 2182.2498\n",
      "Epoch 00013: val_loss did not improve from 0.03194\n",
      "2652/2652 [==============================] - 57s 22ms/step - loss: 0.0296 - mae: 0.0296 - mape: 2182.2498 - val_loss: 0.0324 - val_mae: 0.0324 - val_mape: 14.4038\n",
      "Epoch 14/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0293 - mae: 0.0293 - mape: 2170.6572\n",
      "Epoch 00014: val_loss did not improve from 0.03194\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0293 - mae: 0.0293 - mape: 2169.9692 - val_loss: 0.0320 - val_mae: 0.0320 - val_mape: 14.4884\n",
      "Epoch 15/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0291 - mae: 0.0291 - mape: 2226.9990\n",
      "Epoch 00015: val_loss did not improve from 0.03194\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0291 - mae: 0.0291 - mape: 2225.4573 - val_loss: 0.0328 - val_mae: 0.0328 - val_mape: 15.0793\n",
      "Epoch 16/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0289 - mae: 0.0289 - mape: 2158.0088\n",
      "Epoch 00016: val_loss improved from 0.03194 to 0.03189, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0016-loss0.03.h5\n",
      "2652/2652 [==============================] - 57s 22ms/step - loss: 0.0289 - mae: 0.0289 - mape: 2157.3250 - val_loss: 0.0319 - val_mae: 0.0319 - val_mape: 14.2091\n",
      "Epoch 17/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0287 - mae: 0.0287 - mape: 2195.1448\n",
      "Epoch 00017: val_loss improved from 0.03189 to 0.03163, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0017-loss0.03.h5\n",
      "2652/2652 [==============================] - 57s 22ms/step - loss: 0.0287 - mae: 0.0287 - mape: 2194.4490 - val_loss: 0.0316 - val_mae: 0.0316 - val_mape: 13.8958\n",
      "Epoch 18/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0285 - mae: 0.0285 - mape: 2182.4639\n",
      "Epoch 00018: val_loss improved from 0.03163 to 0.03155, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0018-loss0.03.h5\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0285 - mae: 0.0285 - mape: 2182.4639 - val_loss: 0.0315 - val_mae: 0.0315 - val_mape: 13.9638\n",
      "Epoch 19/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0284 - mae: 0.0284 - mape: 2107.7449\n",
      "Epoch 00019: val_loss improved from 0.03155 to 0.03100, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0019-loss0.03.h5\n",
      "2652/2652 [==============================] - 57s 21ms/step - loss: 0.0284 - mae: 0.0284 - mape: 2107.7449 - val_loss: 0.0310 - val_mae: 0.0310 - val_mape: 13.4182\n",
      "Epoch 20/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0283 - mae: 0.0283 - mape: 2106.6265\n",
      "Epoch 00020: val_loss did not improve from 0.03100\n",
      "2652/2652 [==============================] - 55s 21ms/step - loss: 0.0283 - mae: 0.0283 - mape: 2105.1682 - val_loss: 0.0316 - val_mae: 0.0316 - val_mape: 13.8696\n",
      "Epoch 21/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0281 - mae: 0.0281 - mape: 2097.0786\n",
      "Epoch 00021: val_loss improved from 0.03100 to 0.03091, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0021-loss0.03.h5\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0281 - mae: 0.0281 - mape: 2097.0786 - val_loss: 0.0309 - val_mae: 0.0309 - val_mape: 13.4072\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0280 - mae: 0.0280 - mape: 2097.8687\n",
      "Epoch 00022: val_loss did not improve from 0.03091\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0280 - mae: 0.0280 - mape: 2096.4150 - val_loss: 0.0310 - val_mae: 0.0310 - val_mape: 13.6897\n",
      "Epoch 23/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0279 - mape: 2116.3359\n",
      "Epoch 00023: val_loss improved from 0.03091 to 0.03077, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0023-loss0.03.h5\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0279 - mae: 0.0279 - mape: 2114.8706 - val_loss: 0.0308 - val_mae: 0.0308 - val_mape: 13.2589\n",
      "Epoch 24/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0278 - mae: 0.0278 - mape: 2149.3643\n",
      "Epoch 00024: val_loss improved from 0.03077 to 0.03059, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0024-loss0.03.h5\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0278 - mae: 0.0278 - mape: 2148.6824 - val_loss: 0.0306 - val_mae: 0.0306 - val_mape: 13.5463\n",
      "Epoch 25/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0277 - mae: 0.0277 - mape: 2071.0164\n",
      "Epoch 00025: val_loss improved from 0.03059 to 0.03041, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0025-loss0.03.h5\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0277 - mae: 0.0277 - mape: 2071.0164 - val_loss: 0.0304 - val_mae: 0.0304 - val_mape: 13.1352\n",
      "Epoch 26/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0276 - mape: 2118.9373- ETA: 0s - loss: 0.0276 - mae: 0.0276 - mape: \n",
      "Epoch 00026: val_loss did not improve from 0.03041\n",
      "2652/2652 [==============================] - 57s 21ms/step - loss: 0.0276 - mae: 0.0276 - mape: 2117.4695 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 13.9435\n",
      "Epoch 27/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0275 - mae: 0.0275 - mape: 2069.3828\n",
      "Epoch 00027: val_loss did not improve from 0.03041\n",
      "2652/2652 [==============================] - 55s 21ms/step - loss: 0.0275 - mae: 0.0275 - mape: 2067.9492 - val_loss: 0.0310 - val_mae: 0.0310 - val_mape: 13.4986\n",
      "Epoch 28/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0274 - mae: 0.0274 - mape: 2026.7452- ETA: 1s - loss: 0.0274 - mae:\n",
      "Epoch 00028: val_loss did not improve from 0.03041\n",
      "2652/2652 [==============================] - 55s 21ms/step - loss: 0.0274 - mae: 0.0274 - mape: 2025.3412 - val_loss: 0.0313 - val_mae: 0.0313 - val_mape: 13.4520\n",
      "Epoch 29/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0273 - mape: 2033.2340\n",
      "Epoch 00029: val_loss did not improve from 0.03041\n",
      "2652/2652 [==============================] - 55s 21ms/step - loss: 0.0273 - mae: 0.0273 - mape: 2032.5923 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 13.6498\n",
      "Epoch 30/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0272 - mape: 2005.6034\n",
      "Epoch 00030: val_loss improved from 0.03041 to 0.03005, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0030-loss0.03.h5\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0272 - mae: 0.0272 - mape: 2004.9675 - val_loss: 0.0301 - val_mae: 0.0301 - val_mape: 13.0353\n",
      "Epoch 31/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0271 - mape: 2019.8008\n",
      "Epoch 00031: val_loss did not improve from 0.03005\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0271 - mae: 0.0271 - mape: 2019.1606 - val_loss: 0.0313 - val_mae: 0.0313 - val_mape: 13.7092\n",
      "Epoch 32/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0270 - mape: 1984.7251\n",
      "Epoch 00032: val_loss did not improve from 0.03005\n",
      "2652/2652 [==============================] - 55s 21ms/step - loss: 0.0270 - mae: 0.0270 - mape: 1983.3505 - val_loss: 0.0311 - val_mae: 0.0311 - val_mape: 13.4557\n",
      "Epoch 33/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0269 - mape: 2023.6077- ET\n",
      "Epoch 00033: val_loss did not improve from 0.03005\n",
      "2652/2652 [==============================] - 55s 21ms/step - loss: 0.0269 - mae: 0.0269 - mape: 2022.9663 - val_loss: 0.0304 - val_mae: 0.0304 - val_mape: 13.2168\n",
      "Epoch 34/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0269 - mape: 1956.0304\n",
      "Epoch 00034: val_loss did not improve from 0.03005\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0269 - mae: 0.0269 - mape: 1955.4103 - val_loss: 0.0306 - val_mae: 0.0306 - val_mape: 13.3546\n",
      "Epoch 35/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0268 - mae: 0.0268 - mape: 1953.3590\n",
      "Epoch 00035: val_loss did not improve from 0.03005\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0268 - mae: 0.0268 - mape: 1952.0065 - val_loss: 0.0302 - val_mae: 0.0302 - val_mape: 12.9955\n",
      "Epoch 36/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0267 - mae: 0.0267 - mape: 1964.2957\n",
      "Epoch 00036: val_loss did not improve from 0.03005\n",
      "2652/2652 [==============================] - 55s 21ms/step - loss: 0.0267 - mae: 0.0267 - mape: 1964.2957 - val_loss: 0.0304 - val_mae: 0.0304 - val_mape: 12.9040\n",
      "Epoch 37/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0266 - mape: 1940.9065\n",
      "Epoch 00037: val_loss did not improve from 0.03005\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0266 - mae: 0.0266 - mape: 1940.2914 - val_loss: 0.0301 - val_mae: 0.0301 - val_mape: 12.6802\n",
      "Epoch 38/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0266 - mae: 0.0266 - mape: 1922.7006\n",
      "Epoch 00038: val_loss did not improve from 0.03005\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0266 - mae: 0.0266 - mape: 1922.7006 - val_loss: 0.0303 - val_mae: 0.0303 - val_mape: 12.9361\n",
      "Epoch 39/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0265 - mae: 0.0265 - mape: 1949.7277\n",
      "Epoch 00039: val_loss did not improve from 0.03005\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0265 - mae: 0.0265 - mape: 1949.7277 - val_loss: 0.0314 - val_mae: 0.0314 - val_mape: 13.6980\n",
      "Epoch 40/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0264 - mape: 1928.2448\n",
      "Epoch 00040: val_loss improved from 0.03005 to 0.02997, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0040-loss0.03.h5\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0264 - mae: 0.0264 - mape: 1926.9089 - val_loss: 0.0300 - val_mae: 0.0300 - val_mape: 12.8066\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oS6zzoUoPL-A",
    "outputId": "ce1a0f3b-6b76-4980-d717-d3542c18a37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 490.25\n",
      "Median Absolute Error (MedAE): 344.8\n",
      "Mean Squared Error (MSE): 464613.29\n",
      "Root Mean Squared Error (RMSE): 681.63\n",
      "Mean Absolute Percentage Error (MAPE): 3.28 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.42 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\1_orignal\\\\E1-cp-0024-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7y39sVKwLMm"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AVN1WhyzuWFl"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\multistep\\\\1_orignal\\\\E2-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\multistep\\\\1_orignal\\\\E1-cp-0025-loss0.03.h5'\n",
    "start_epoch= 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WBeDTA9lu5Qm"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2j6fmkOvA2O",
    "outputId": "7fc1b23b-4d93-4b54-a4df-921c557b9a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E1-cp-0025-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = review_goog()\n",
    "    opt = Adam(0.001)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 0.0001)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rTCJW6IzvLU4",
    "outputId": "f810e1ca-98cc-4de7-a4a3-465e83705c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0268 - mean_absolute_error: 0.0268 - mean_absolute_percentage_error: 2095.7263\n",
      "Epoch 00001: val_loss improved from inf to 0.03036, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E2-cp-0001-loss0.03.h5\n",
      "166/166 [==============================] - 61s 368ms/step - loss: 0.0268 - mean_absolute_error: 0.0268 - mean_absolute_percentage_error: 2095.7263 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304 - val_mean_absolute_percentage_error: 13.2899\n",
      "Epoch 2/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0265 - mean_absolute_error: 0.0265 - mean_absolute_percentage_error: 2099.5098\n",
      "Epoch 00002: val_loss improved from 0.03036 to 0.03028, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E2-cp-0002-loss0.03.h5\n",
      "166/166 [==============================] - 59s 355ms/step - loss: 0.0265 - mean_absolute_error: 0.0265 - mean_absolute_percentage_error: 2099.5098 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.2373\n",
      "Epoch 3/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0264 - mean_absolute_error: 0.0264 - mean_absolute_percentage_error: 2095.4539\n",
      "Epoch 00003: val_loss did not improve from 0.03028\n",
      "166/166 [==============================] - 57s 342ms/step - loss: 0.0264 - mean_absolute_error: 0.0264 - mean_absolute_percentage_error: 2095.4539 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304 - val_mean_absolute_percentage_error: 13.2987\n",
      "Epoch 4/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0264 - mean_absolute_error: 0.0264 - mean_absolute_percentage_error: 2095.6804\n",
      "Epoch 00004: val_loss improved from 0.03028 to 0.03027, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E2-cp-0004-loss0.03.h5\n",
      "166/166 [==============================] - 58s 349ms/step - loss: 0.0264 - mean_absolute_error: 0.0264 - mean_absolute_percentage_error: 2095.6804 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.2243\n",
      "Epoch 5/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2089.6279\n",
      "Epoch 00005: val_loss improved from 0.03027 to 0.03022, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E2-cp-0005-loss0.03.h5\n",
      "166/166 [==============================] - 60s 361ms/step - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2089.6279 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.1731\n",
      "Epoch 6/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2088.4893\n",
      "Epoch 00006: val_loss did not improve from 0.03022\n",
      "166/166 [==============================] - 58s 351ms/step - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2088.4893 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.2084\n",
      "Epoch 7/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2086.3828\n",
      "Epoch 00007: val_loss did not improve from 0.03022\n",
      "166/166 [==============================] - 61s 365ms/step - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2086.3828 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.1402\n",
      "Epoch 8/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2078.8918\n",
      "Epoch 00008: val_loss did not improve from 0.03022\n",
      "166/166 [==============================] - 58s 352ms/step - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2078.8918 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.2180\n",
      "Epoch 9/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2080.9961\n",
      "Epoch 00009: val_loss improved from 0.03022 to 0.03022, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E2-cp-0009-loss0.03.h5\n",
      "166/166 [==============================] - 57s 342ms/step - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2080.9961 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.1270\n",
      "Epoch 10/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2077.4172\n",
      "Epoch 00010: val_loss improved from 0.03022 to 0.03015, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E2-cp-0010-loss0.03.h5\n",
      "166/166 [==============================] - 55s 333ms/step - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2077.4172 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0543\n",
      "Epoch 11/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2074.2852\n",
      "Epoch 00011: val_loss did not improve from 0.03015\n",
      "166/166 [==============================] - 55s 334ms/step - loss: 0.0263 - mean_absolute_error: 0.0263 - mean_absolute_percentage_error: 2074.2852 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1586\n",
      "Epoch 12/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2070.4424\n",
      "Epoch 00012: val_loss did not improve from 0.03015\n",
      "166/166 [==============================] - 58s 351ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2070.4424 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1704\n",
      "Epoch 13/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2067.7036\n",
      "Epoch 00013: val_loss did not improve from 0.03015\n",
      "166/166 [==============================] - 61s 365ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2067.7036 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.2009\n",
      "Epoch 14/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2066.5925\n",
      "Epoch 00014: val_loss did not improve from 0.03015\n",
      "166/166 [==============================] - 57s 344ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2066.5925 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1823\n",
      "Epoch 15/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2068.8303\n",
      "Epoch 00015: val_loss did not improve from 0.03015\n",
      "166/166 [==============================] - 57s 343ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2068.8303 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1640\n",
      "Epoch 16/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2065.9463\n",
      "Epoch 00016: val_loss did not improve from 0.03015\n",
      "166/166 [==============================] - 56s 336ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2065.9463 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.1217\n",
      "Epoch 17/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2058.5273\n",
      "Epoch 00017: val_loss did not improve from 0.03015\n",
      "166/166 [==============================] - 57s 345ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2058.5273 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.1401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2058.5493\n",
      "Epoch 00018: val_loss did not improve from 0.03015\n",
      "166/166 [==============================] - 56s 337ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2058.5493 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0807\n",
      "Epoch 19/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2058.0430\n",
      "Epoch 00019: val_loss did not improve from 0.03015\n",
      "166/166 [==============================] - 56s 335ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2058.0430 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1768\n",
      "Epoch 20/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2059.7075\n",
      "Epoch 00020: val_loss improved from 0.03015 to 0.03012, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\multistep\\1_orignal\\E2-cp-0020-loss0.03.h5\n",
      "166/166 [==============================] - 55s 330ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2059.7075 - val_loss: 0.0301 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 13.0497\n",
      "Epoch 21/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2053.9741\n",
      "Epoch 00021: val_loss did not improve from 0.03012\n",
      "166/166 [==============================] - 55s 330ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2053.9741 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.1376\n",
      "Epoch 22/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2049.3657\n",
      "Epoch 00022: val_loss did not improve from 0.03012\n",
      "166/166 [==============================] - 56s 336ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2049.3657 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.1499\n",
      "Epoch 23/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2053.1360\n",
      "Epoch 00023: val_loss did not improve from 0.03012\n",
      "166/166 [==============================] - 56s 340ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2053.1360 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.1382\n",
      "Epoch 24/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2046.3387\n",
      "Epoch 00024: val_loss did not improve from 0.03012\n",
      "166/166 [==============================] - 59s 357ms/step - loss: 0.0262 - mean_absolute_error: 0.0262 - mean_absolute_percentage_error: 2046.3387 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1455\n",
      "Epoch 25/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2049.5073\n",
      "Epoch 00025: val_loss did not improve from 0.03012\n",
      "166/166 [==============================] - 58s 350ms/step - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2049.5073 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1817\n",
      "Epoch 26/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2047.9395\n",
      "Epoch 00026: val_loss did not improve from 0.03012\n",
      "166/166 [==============================] - 60s 360ms/step - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2047.9395 - val_loss: 0.0301 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 13.0602\n",
      "Epoch 27/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2039.3296\n",
      "Epoch 00027: val_loss did not improve from 0.03012\n",
      "166/166 [==============================] - 56s 334ms/step - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2039.3296 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0792\n",
      "Epoch 28/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2045.0021\n",
      "Epoch 00028: val_loss did not improve from 0.03012\n",
      "166/166 [==============================] - 57s 345ms/step - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2045.0021 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0941\n",
      "Epoch 29/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2043.0768\n",
      "Epoch 00029: val_loss did not improve from 0.03012\n",
      "166/166 [==============================] - 57s 346ms/step - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2043.0768 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0782\n",
      "Epoch 30/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2037.1266\n",
      "Epoch 00030: val_loss did not improve from 0.03012\n",
      "166/166 [==============================] - 58s 347ms/step - loss: 0.0261 - mean_absolute_error: 0.0261 - mean_absolute_percentage_error: 2037.1266 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.1061\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "verbose = 1 #0\n",
    "batch_size = 512\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y_load),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qj8HRsBXvf0M",
    "outputId": "084e6fbf-b9cc-46e8-e4d7-808e9031da87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 491.67\n",
      "Median Absolute Error (MedAE): 336.35\n",
      "Mean Squared Error (MSE): 479457.02\n",
      "Root Mean Squared Error (RMSE): 692.43\n",
      "Mean Absolute Percentage Error (MAPE): 3.28 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.35 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\1_orignal\\\\E2-cp-0004-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAuKvjoOG-fA"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QZI2LOvEw6Qq"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E2-cp-0006-loss0.04.h5'\n",
    "start_epoch= 59+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qPy-9EGRw6eR"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beh0i6Vww6pP",
    "outputId": "41024e1d-abfa-4ec6-a3ed-cf5e5fdef9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E2-cp-0006-loss0.04.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y-9y1xTeYAaB",
    "outputId": "3e59db7d-d1fa-41c7-a91e-cf2063e855da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0231 - mape: 8601.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03575, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0001-loss0.04.h5\n",
      "32614/32614 [==============================] - 7s 210us/sample - loss: 0.0235 - mae: 0.0231 - mape: 8729.3066 - val_loss: 0.0358 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0230 - mape: 8563.2227\n",
      "Epoch 00002: val_loss improved from 0.03575 to 0.03574, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0002-loss0.04.h5\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0235 - mae: 0.0230 - mape: 8710.7080 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1938\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8942.2402\n",
      "Epoch 00003: val_loss improved from 0.03574 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0003-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 115us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8703.8408 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8589.9561\n",
      "Epoch 00004: val_loss improved from 0.03573 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 114us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8711.1074 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1949\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8953.2998\n",
      "Epoch 00005: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8714.5996 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8595.0498\n",
      "Epoch 00006: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8716.8604 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8175.1738\n",
      "Epoch 00007: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8717.2090 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8577.0596\n",
      "Epoch 00008: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8716.0957 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1954\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2109\n",
      "Epoch 00009: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.7314 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2744\n",
      "Epoch 00010: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.3086 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.7139\n",
      "Epoch 00011: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.5303 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.2705\n",
      "Epoch 00012: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8717.8525 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1962\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2910\n",
      "Epoch 00013: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.2773 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8968.6553\n",
      "Epoch 00014: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.5400 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1970\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8580.6602\n",
      "Epoch 00015: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8719.8223 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1967\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8623\n",
      "Epoch 00016: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.7832 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1963\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8877\n",
      "Epoch 00017: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.8203 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8606.8154\n",
      "Epoch 00018: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0547 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2119\n",
      "Epoch 00019: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.2393 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8973.6885\n",
      "Epoch 00020: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8734.4512 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8217.7236  ETA: 2s - loss: 0.0233 -\n",
      "Epoch 00021: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8730.0449 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8967.8350\n",
      "Epoch 00022: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8728.7363 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8965.8945\n",
      "Epoch 00023: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.8516 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1969\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8607.8672\n",
      "Epoch 00024: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0010 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1964\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8588.1279\n",
      "Epoch 00025: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8735.5576 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8230.3066\n",
      "Epoch 00026: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8738.8379 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 27/200\n",
      "13312/32614 [===========>..................] - ETA: 1s - loss: 0.0233 - mae: 0.0229 - mape: 7589.0488"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-05065ff65640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y, \n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9fPE-jZuQ0r",
    "outputId": "aca09565-e9df-4067-d473-32dd8826a738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 541.71\n",
      "Median Absolute Error (MedAE): 449.47\n",
      "Mean Squared Error (MSE): 473684.02\n",
      "Root Mean Squared Error (RMSE): 688.25\n",
      "Mean Absolute Percentage Error (MAPE): 5.4 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.39 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-0004-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7x0O-u0GbLy"
   },
   "source": [
    "# E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rdBroUOqIeW6"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(200, activation='relu')(x)\n",
    " #       x = Dense(10, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AQaM-IuPIXqN"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QwUKEvSfIW09"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NHgO7mJ4IQVp"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TIsLEDoIN0C",
    "outputId": "3ef77929-b334-4953-faab-b938f376c306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpLkyqBmIKIw",
    "outputId": "3d701f4a-270d-4efc-b52a-cd26d12d4dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.2038 - mae: 0.0831 - mape: 10284.1670\n",
      "Epoch 00001: val_loss improved from inf to 0.10131, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0001-loss0.10.h5\n",
      "32614/32614 [==============================] - 19s 581us/sample - loss: 0.2036 - mae: 0.0830 - mape: 10262.1201 - val_loss: 0.1013 - val_mae: 0.0675 - val_mape: 11.9466\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0654 - mae: 0.0492 - mape: 9617.4668\n",
      "Epoch 00002: val_loss improved from 0.10131 to 0.07428, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0002-loss0.07.h5\n",
      "32614/32614 [==============================] - 14s 420us/sample - loss: 0.0654 - mae: 0.0492 - mape: 9596.8438 - val_loss: 0.0743 - val_mae: 0.0671 - val_mape: 11.8373\n",
      "Epoch 3/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0511 - mae: 0.0461 - mape: 9812.7764\n",
      "Epoch 00003: val_loss improved from 0.07428 to 0.06341, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0003-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0510 - mae: 0.0460 - mape: 9762.8730 - val_loss: 0.0634 - val_mae: 0.0599 - val_mape: 10.4427\n",
      "Epoch 4/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0450 - mae: 0.0419 - mape: 9616.9199\n",
      "Epoch 00004: val_loss did not improve from 0.06341\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0450 - mae: 0.0419 - mape: 9596.3027 - val_loss: 0.0706 - val_mae: 0.0680 - val_mape: 11.4199\n",
      "Epoch 5/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0441 - mae: 0.0414 - mape: 9613.5029\n",
      "Epoch 00005: val_loss improved from 0.06341 to 0.05454, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0005-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0441 - mae: 0.0414 - mape: 9611.7354 - val_loss: 0.0545 - val_mae: 0.0519 - val_mape: 9.2312\n",
      "Epoch 6/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0418 - mae: 0.0397 - mape: 9602.1924\n",
      "Epoch 00006: val_loss did not improve from 0.05454\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0418 - mae: 0.0397 - mape: 9572.1885 - val_loss: 0.0587 - val_mae: 0.0570 - val_mape: 9.8649\n",
      "Epoch 7/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0406 - mae: 0.0389 - mape: 9195.3799\n",
      "Epoch 00007: val_loss improved from 0.05454 to 0.04682, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0007-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0406 - mae: 0.0389 - mape: 9554.0049 - val_loss: 0.0468 - val_mae: 0.0450 - val_mape: 8.7847\n",
      "Epoch 8/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0399 - mae: 0.0384 - mape: 9416.9941\n",
      "Epoch 00008: val_loss did not improve from 0.04682\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0399 - mae: 0.0384 - mape: 9396.7998 - val_loss: 0.0475 - val_mae: 0.0462 - val_mape: 8.9737\n",
      "Epoch 9/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0397 - mae: 0.0383 - mape: 9665.3730\n",
      "Epoch 00009: val_loss improved from 0.04682 to 0.04522, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0009-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0397 - mae: 0.0383 - mape: 9644.6445 - val_loss: 0.0452 - val_mae: 0.0439 - val_mape: 8.5116\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0377 - mape: 9573.1260\n",
      "Epoch 00010: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0389 - mae: 0.0377 - mape: 9533.8271 - val_loss: 0.0940 - val_mae: 0.0928 - val_mape: 15.8112\n",
      "Epoch 11/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0390 - mae: 0.0378 - mape: 9537.1748\n",
      "Epoch 00011: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0390 - mae: 0.0378 - mape: 9535.4209 - val_loss: 0.0501 - val_mae: 0.0490 - val_mape: 8.8788\n",
      "Epoch 12/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0381 - mae: 0.0371 - mape: 9512.4170\n",
      "Epoch 00012: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0381 - mae: 0.0371 - mape: 9492.0146 - val_loss: 0.0511 - val_mae: 0.0502 - val_mape: 9.2328\n",
      "Epoch 13/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0377 - mae: 0.0368 - mape: 9480.7461\n",
      "Epoch 00013: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0377 - mae: 0.0368 - mape: 9479.0029 - val_loss: 0.0494 - val_mae: 0.0485 - val_mape: 9.0546\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.0370 - mape: 9640.1309\n",
      "Epoch 00014: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0379 - mae: 0.0370 - mape: 9600.5547 - val_loss: 0.0475 - val_mae: 0.0466 - val_mape: 8.6730\n",
      "Epoch 15/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0375 - mae: 0.0367 - mape: 9567.5752\n",
      "Epoch 00015: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0375 - mae: 0.0367 - mape: 9565.8154 - val_loss: 0.0497 - val_mae: 0.0489 - val_mape: 9.3880\n",
      "Epoch 16/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0374 - mae: 0.0366 - mape: 9551.2051\n",
      "Epoch 00016: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0374 - mae: 0.0366 - mape: 9540.0859 - val_loss: 0.0464 - val_mae: 0.0456 - val_mape: 8.5940\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0362 - mape: 9563.9082\n",
      "Epoch 00017: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0369 - mae: 0.0362 - mape: 9562.1514 - val_loss: 0.0475 - val_mae: 0.0468 - val_mape: 8.6608\n",
      "Epoch 18/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0361 - mape: 9577.6992\n",
      "Epoch 00018: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0368 - mae: 0.0361 - mape: 9566.5488 - val_loss: 0.0627 - val_mae: 0.0620 - val_mape: 10.5263\n",
      "Epoch 19/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0366 - mae: 0.0359 - mape: 8800.0615\n",
      "Epoch 00019: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0366 - mae: 0.0359 - mape: 9530.1943 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 8.7106\n",
      "Epoch 20/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9487.9727\n",
      "Epoch 00020: val_loss improved from 0.04522 to 0.04432, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0020-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9476.9248 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.3836\n",
      "Epoch 21/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9175.1748\n",
      "Epoch 00021: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9548.0811 - val_loss: 0.0444 - val_mae: 0.0438 - val_mape: 8.4635\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0356 - mape: 9438.7158\n",
      "Epoch 00022: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0362 - mae: 0.0356 - mape: 9427.7275 - val_loss: 0.0463 - val_mae: 0.0456 - val_mape: 8.8022\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0363 - mae: 0.0356 - mape: 9494.2676\n",
      "Epoch 00023: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0363 - mae: 0.0356 - mape: 9492.5234 - val_loss: 0.0502 - val_mae: 0.0496 - val_mape: 8.8956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0360 - mae: 0.0354 - mape: 9481.0664\n",
      "Epoch 00024: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0360 - mae: 0.0354 - mape: 9479.3242 - val_loss: 0.0482 - val_mae: 0.0476 - val_mape: 8.8535\n",
      "Epoch 25/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9452.0479\n",
      "Epoch 00025: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9422.5107 - val_loss: 0.0450 - val_mae: 0.0444 - val_mape: 8.3675\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9389.7109\n",
      "Epoch 00026: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9360.3701 - val_loss: 0.0505 - val_mae: 0.0499 - val_mape: 8.9818\n",
      "Epoch 27/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9449.0146\n",
      "Epoch 00027: val_loss improved from 0.04432 to 0.04404, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0027-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9419.4844 - val_loss: 0.0440 - val_mae: 0.0434 - val_mape: 8.4639\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9357.8389\n",
      "Epoch 00028: val_loss improved from 0.04404 to 0.04385, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0028-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9346.9443 - val_loss: 0.0438 - val_mae: 0.0433 - val_mape: 8.3336\n",
      "Epoch 29/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0350 - mape: 9348.0967- ETA: 2s - loss: 0.\n",
      "Epoch 00029: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0356 - mae: 0.0350 - mape: 9337.2129 - val_loss: 0.0453 - val_mae: 0.0447 - val_mape: 8.8223\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0355 - mae: 0.0349 - mape: 9328.2656\n",
      "Epoch 00030: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0355 - mae: 0.0349 - mape: 9299.1152 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.4327\n",
      "Epoch 31/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0348 - mape: 9367.9561\n",
      "Epoch 00031: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0354 - mae: 0.0348 - mape: 9329.5146 - val_loss: 0.0556 - val_mae: 0.0550 - val_mape: 9.5869\n",
      "Epoch 32/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0346 - mae: 0.0341 - mape: 9331.7041\n",
      "Epoch 00032: val_loss improved from 0.04385 to 0.03952, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0032-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0346 - mae: 0.0341 - mape: 9293.3955 - val_loss: 0.0395 - val_mae: 0.0389 - val_mape: 7.3175\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0313 - mae: 0.0307 - mape: 8705.7988\n",
      "Epoch 00033: val_loss did not improve from 0.03952\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0313 - mae: 0.0307 - mape: 9014.9424 - val_loss: 0.0402 - val_mae: 0.0396 - val_mape: 6.9964\n",
      "Epoch 34/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0310 - mae: 0.0304 - mape: 8956.5234\n",
      "Epoch 00034: val_loss improved from 0.03952 to 0.03736, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0034-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 427us/sample - loss: 0.0310 - mae: 0.0304 - mape: 8937.3135 - val_loss: 0.0374 - val_mae: 0.0368 - val_mape: 6.7796\n",
      "Epoch 35/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0308 - mae: 0.0302 - mape: 9005.4199\n",
      "Epoch 00035: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0308 - mae: 0.0302 - mape: 9003.7646 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.5172\n",
      "Epoch 36/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 8986.6465\n",
      "Epoch 00036: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0307 - mae: 0.0301 - mape: 8984.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.7190\n",
      "Epoch 37/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 9036.8193\n",
      "Epoch 00037: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0307 - mae: 0.0301 - mape: 9017.4414 - val_loss: 0.0397 - val_mae: 0.0391 - val_mape: 6.9239\n",
      "Epoch 38/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0304 - mae: 0.0298 - mape: 8871.0625\n",
      "Epoch 00038: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0305 - mae: 0.0299 - mape: 8852.0391 - val_loss: 0.0418 - val_mae: 0.0413 - val_mape: 7.1686\n",
      "Epoch 39/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8882.9082\n",
      "Epoch 00039: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8881.2744 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.0621\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8929.6758\n",
      "Epoch 00040: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8910.5225 - val_loss: 0.0390 - val_mae: 0.0384 - val_mape: 6.7820\n",
      "Epoch 41/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8876.4180\n",
      "Epoch 00041: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8857.3779 - val_loss: 0.0391 - val_mae: 0.0385 - val_mape: 6.9458\n",
      "Epoch 42/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8984.8389\n",
      "Epoch 00042: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8956.7549 - val_loss: 0.0413 - val_mae: 0.0407 - val_mape: 7.6354\n",
      "Epoch 43/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0301 - mae: 0.0295 - mape: 8793.3711\n",
      "Epoch 00043: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0301 - mae: 0.0295 - mape: 8783.1338 - val_loss: 0.0435 - val_mae: 0.0429 - val_mape: 7.4288\n",
      "Epoch 44/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8822.2676- ETA: 6s - loss: 0.0301 - mae: 0.0 - ETA: 1s - loss: 0.0302 - mae:\n",
      "Epoch 00044: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8811.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.9922\n",
      "Epoch 45/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0298 - mae: 0.0292 - mape: 8836.8652\n",
      "Epoch 00045: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0298 - mae: 0.0292 - mape: 8800.5830 - val_loss: 0.0593 - val_mae: 0.0587 - val_mape: 11.2009\n",
      "Epoch 46/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0287 - mae: 0.0281 - mape: 8818.6738\n",
      "Epoch 00046: val_loss improved from 0.03736 to 0.03728, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0046-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0287 - mae: 0.0281 - mape: 8791.1104 - val_loss: 0.0373 - val_mae: 0.0367 - val_mape: 6.3479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0269 - mape: 8903.1064\n",
      "Epoch 00047: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0276 - mae: 0.0269 - mape: 8866.5479 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 7.3343\n",
      "Epoch 48/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0264 - mape: 8938.7715\n",
      "Epoch 00048: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0270 - mae: 0.0264 - mape: 8919.5977 - val_loss: 0.0419 - val_mae: 0.0413 - val_mape: 6.8349\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0265 - mape: 8954.8320\n",
      "Epoch 00049: val_loss improved from 0.03728 to 0.03701, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0049-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0272 - mae: 0.0265 - mape: 8926.8408 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2637\n",
      "Epoch 50/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0263 - mape: 8925.1689\n",
      "Epoch 00050: val_loss did not improve from 0.03701\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0269 - mae: 0.0263 - mape: 8914.7754 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2835\n",
      "Epoch 51/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0262 - mape: 8973.1758\n",
      "Epoch 00051: val_loss improved from 0.03701 to 0.03644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0051-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0269 - mae: 0.0262 - mape: 8936.3379 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3367\n",
      "Epoch 52/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0267 - mae: 0.0260 - mape: 8984.7070\n",
      "Epoch 00052: val_loss improved from 0.03644 to 0.03603, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0267 - mae: 0.0260 - mape: 8947.8154 - val_loss: 0.0360 - val_mae: 0.0354 - val_mape: 6.2870\n",
      "Epoch 53/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.0259 - mape: 8895.4316\n",
      "Epoch 00053: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0265 - mae: 0.0259 - mape: 8893.7969 - val_loss: 0.0433 - val_mae: 0.0426 - val_mape: 6.9243\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0258 - mape: 8529.0977\n",
      "Epoch 00054: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0264 - mae: 0.0258 - mape: 8883.7363 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.3396\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8902.2764\n",
      "Epoch 00055: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8883.1807 - val_loss: 0.0377 - val_mae: 0.0371 - val_mape: 6.5213\n",
      "Epoch 56/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8834.2520\n",
      "Epoch 00056: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8806.6387 - val_loss: 0.0378 - val_mae: 0.0372 - val_mape: 6.4096\n",
      "Epoch 57/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0255 - mape: 8899.8281\n",
      "Epoch 00057: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0261 - mae: 0.0255 - mape: 8889.4648 - val_loss: 0.0365 - val_mae: 0.0358 - val_mape: 6.4844\n",
      "Epoch 58/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0257 - mape: 8935.8350\n",
      "Epoch 00058: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0263 - mae: 0.0257 - mape: 8899.1426 - val_loss: 0.0389 - val_mae: 0.0382 - val_mape: 6.9754\n",
      "Epoch 59/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0255 - mape: 8985.3438\n",
      "Epoch 00059: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0262 - mae: 0.0255 - mape: 8966.0693 - val_loss: 0.0448 - val_mae: 0.0442 - val_mape: 7.1970\n",
      "Epoch 60/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0254 - mape: 8698.2939\n",
      "Epoch 00060: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0261 - mae: 0.0254 - mape: 9026.8086 - val_loss: 0.0371 - val_mae: 0.0365 - val_mape: 6.4728\n",
      "Epoch 61/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0256 - mape: 8944.7266\n",
      "Epoch 00061: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0262 - mae: 0.0256 - mape: 8943.0820 - val_loss: 0.0376 - val_mae: 0.0369 - val_mape: 6.4816\n",
      "Epoch 62/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0253 - mape: 8861.0674\n",
      "Epoch 00062: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0259 - mae: 0.0253 - mape: 8833.3691 - val_loss: 0.0376 - val_mae: 0.0370 - val_mape: 6.7046\n",
      "Epoch 63/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0252 - mape: 8937.2539\n",
      "Epoch 00063: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0259 - mae: 0.0252 - mape: 8909.3174 - val_loss: 0.0441 - val_mae: 0.0434 - val_mape: 7.0393\n",
      "Epoch 64/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0251 - mape: 8836.8535\n",
      "Epoch 00064: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0257 - mae: 0.0251 - mape: 8826.5635 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.1966\n",
      "Epoch 65/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8864.3887\n",
      "Epoch 00065: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8845.3740 - val_loss: 0.0403 - val_mae: 0.0396 - val_mape: 6.6836\n",
      "Epoch 66/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0251 - mape: 8864.2441\n",
      "Epoch 00066: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0258 - mae: 0.0251 - mape: 8853.9238 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.2001\n",
      "Epoch 67/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8919.5137- ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8391\n",
      "Epoch 00067: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0256 - mae: 0.0249 - mape: 8900.3799 - val_loss: 0.0435 - val_mae: 0.0428 - val_mape: 6.9965\n",
      "Epoch 68/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0249 - mape: 8921.0537\n",
      "Epoch 00068: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0255 - mae: 0.0249 - mape: 8910.6650 - val_loss: 0.0368 - val_mae: 0.0361 - val_mape: 6.4016\n",
      "Epoch 69/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8904.1221\n",
      "Epoch 00069: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8885.0215 - val_loss: 0.0366 - val_mae: 0.0359 - val_mape: 6.4782\n",
      "Epoch 70/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0248 - mape: 8509.3799\n",
      "Epoch 00070: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0255 - mae: 0.0248 - mape: 8848.4863 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0247 - mape: 8894.0742\n",
      "Epoch 00071: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0254 - mae: 0.0247 - mape: 8874.9961 - val_loss: 0.0447 - val_mae: 0.0440 - val_mape: 7.2170\n",
      "Epoch 72/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0246 - mape: 8940.8789\n",
      "Epoch 00072: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0253 - mae: 0.0246 - mape: 8921.6992 - val_loss: 0.0399 - val_mae: 0.0393 - val_mape: 6.6394\n",
      "Epoch 73/80\n",
      "31840/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0246 - mape: 8697.8066"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d15c63a445f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BNTzwnub2Eh-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 577.97\n",
      "Median Absolute Error (MedAE): 474.07\n",
      "Mean Squared Error (MSE): 554533.54\n",
      "Root Mean Squared Error (RMSE): 744.67\n",
      "Mean Absolute Percentage Error (MAPE): 5.86 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.58 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tYU2gaBI4Yl"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fz9kU3b6IvyO"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5'\n",
    "start_epoch= 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2bMbPx8RJKL4"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5T4Ho3YJJKTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "14t41QwoJKZ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0241 - mae: 0.0235 - mape: 8787.5186\n",
      "Epoch 00001: val_loss improved from inf to 0.03371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 57s 2ms/sample - loss: 0.0241 - mae: 0.0235 - mape: 8785.9023 - val_loss: 0.0337 - val_mae: 0.0331 - val_mape: 5.6219\n",
      "Epoch 2/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8818.7939\n",
      "Epoch 00002: val_loss improved from 0.03371 to 0.03268, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 8808.5244 - val_loss: 0.0327 - val_mae: 0.0321 - val_mape: 5.5019\n",
      "Epoch 3/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 8832.4248\n",
      "Epoch 00003: val_loss improved from 0.03268 to 0.03238, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 8830.8018 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4341\n",
      "Epoch 4/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 8807.1309\n",
      "Epoch 00004: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 8805.5117 - val_loss: 0.0330 - val_mae: 0.0324 - val_mape: 5.4931\n",
      "Epoch 5/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0212 - mape: 8834.0068\n",
      "Epoch 00005: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0212 - mape: 8832.3828 - val_loss: 0.0333 - val_mae: 0.0328 - val_mape: 5.5382\n",
      "Epoch 6/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8771.0771\n",
      "Epoch 00006: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8769.4648 - val_loss: 0.0326 - val_mae: 0.0321 - val_mape: 5.4661\n",
      "Epoch 7/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8793.5381\n",
      "Epoch 00007: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8783.2979 - val_loss: 0.0325 - val_mae: 0.0320 - val_mape: 5.4550\n",
      "Epoch 8/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8746.4922\n",
      "Epoch 00008: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8744.8848 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.4740\n",
      "Epoch 9/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8799.6211\n",
      "Epoch 00009: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8789.3721 - val_loss: 0.0332 - val_mae: 0.0327 - val_mape: 5.5137\n",
      "Epoch 10/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8783.9893\n",
      "Epoch 00010: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8782.3740 - val_loss: 0.0327 - val_mae: 0.0322 - val_mape: 5.4856\n",
      "Epoch 11/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8814.5137\n",
      "Epoch 00011: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8804.2490 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.4683\n",
      "Epoch 12/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0208 - mape: 8829.3320\n",
      "Epoch 00012: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0213 - mae: 0.0208 - mape: 8819.0498 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4762\n",
      "Epoch 13/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8766.2891\n",
      "Epoch 00013: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8764.6777 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.6098\n",
      "Epoch 14/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0208 - mape: 8807.6914\n",
      "Epoch 00014: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0208 - mape: 8797.4355 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4454\n",
      "Epoch 15/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8763.3545\n",
      "Epoch 00015: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8761.7432 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4502\n",
      "Epoch 16/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0207 - mape: 8800.9727\n",
      "Epoch 00016: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0207 - mape: 8790.7227 - val_loss: 0.0336 - val_mae: 0.0331 - val_mape: 5.5508\n",
      "Epoch 17/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8805.7637\n",
      "Epoch 00017: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 8804.1465 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5136\n",
      "Epoch 18/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8785.8975\n",
      "Epoch 00018: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8775.6660 - val_loss: 0.0324 - val_mae: 0.0320 - val_mape: 5.4366\n",
      "Epoch 19/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8766.3740\n",
      "Epoch 00019: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8756.1650 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5053\n",
      "Epoch 20/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8869.6514- ETA: 1s - loss: 0.0209 - mae: 0.0205\n",
      "Epoch 00020: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8859.3301 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5394\n",
      "Epoch 21/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8719.9990\n",
      "Epoch 00021: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8718.3955 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.4912\n",
      "Epoch 22/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8816.5059\n",
      "Epoch 00022: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8814.8848 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5017\n",
      "Epoch 23/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8851.2988\n",
      "Epoch 00023: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8840.9902 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5257\n",
      "Epoch 24/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8849.9473\n",
      "Epoch 00024: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8839.6475 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8417.9062\n",
      "Epoch 00025: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8762.0635 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5165\n",
      "Epoch 26/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8781.5459\n",
      "Epoch 00026: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8771.3203 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5305\n",
      "Epoch 27/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8805.2725\n",
      "Epoch 00027: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8795.0176 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5373\n",
      "Epoch 28/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8814.1201\n",
      "Epoch 00028: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8803.8555 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5812\n",
      "Epoch 29/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8835.2373\n",
      "Epoch 00029: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8833.6133 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.5439\n",
      "Epoch 30/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8756.9062\n",
      "Epoch 00030: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8746.7100 - val_loss: 0.0335 - val_mae: 0.0331 - val_mape: 5.5782\n",
      "Epoch 31/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8848.8691\n",
      "Epoch 00031: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8847.2422 - val_loss: 0.0345 - val_mae: 0.0342 - val_mape: 5.6903\n",
      "Epoch 32/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8786.9775\n",
      "Epoch 00032: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8785.3623 - val_loss: 0.0348 - val_mae: 0.0344 - val_mape: 5.7179\n",
      "Epoch 33/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8790.1523\n",
      "Epoch 00033: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8779.9150 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5215\n",
      "Epoch 34/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8812.7725\n",
      "Epoch 00034: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8811.1523 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5479\n",
      "Epoch 35/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8778.3477-\n",
      "Epoch 00035: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8776.7334 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5697\n",
      "Epoch 36/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8766.7051\n",
      "Epoch 00036: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8756.4961 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.6073\n",
      "Epoch 37/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0200 - mape: 8769.1572\n",
      "Epoch 00037: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0200 - mape: 8767.5449 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5441\n",
      "Epoch 38/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0201 - mape: 8747.3877\n",
      "Epoch 00038: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0201 - mape: 8745.7793 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5797\n",
      "Epoch 39/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0200 - mape: 8836.6064\n",
      "Epoch 00039: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0200 - mape: 8834.9814 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5334\n",
      "Epoch 40/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8814.8398\n",
      "Epoch 00040: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8804.5742 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5985\n",
      "Epoch 41/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8839.9746\n",
      "Epoch 00041: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8829.6797 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5734\n",
      "Epoch 42/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0198 - mape: 8831.3848\n",
      "Epoch 00042: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0202 - mae: 0.0198 - mape: 8829.7607 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5997\n",
      "Epoch 43/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0201 - mae: 0.0198 - mape: 8783.8525\n",
      "Epoch 00043: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0201 - mae: 0.0198 - mape: 8782.2383 - val_loss: 0.0337 - val_mae: 0.0334 - val_mape: 5.6261\n",
      "Epoch 44/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0197 - mape: 8822.6426\n",
      "Epoch 00044: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0197 - mape: 8821.0205 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.6350\n",
      "Epoch 45/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8898.9219\n",
      "Epoch 00045: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8888.5576 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5640\n",
      "Epoch 46/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8848.5547\n",
      "Epoch 00046: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8846.9277 - val_loss: 0.0326 - val_mae: 0.0323 - val_mape: 5.5130\n",
      "Epoch 47/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8915.1211\n",
      "Epoch 00047: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8904.7383 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5152\n",
      "Epoch 48/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8884.6523\n",
      "Epoch 00048: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8874.3057 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5568\n",
      "Epoch 49/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0195 - mape: 8901.6914\n",
      "Epoch 00049: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0195 - mape: 8891.3252 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8912.0732\n",
      "Epoch 00050: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8901.6934 - val_loss: 0.0326 - val_mae: 0.0322 - val_mape: 5.5117\n",
      "Epoch 51/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0194 - mape: 8897.9141\n",
      "Epoch 00051: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0194 - mape: 8896.2773 - val_loss: 0.0339 - val_mae: 0.0336 - val_mape: 5.6620\n",
      "Epoch 52/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8820.4697\n",
      "Epoch 00052: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8810.1973 - val_loss: 0.0336 - val_mae: 0.0333 - val_mape: 5.6221\n",
      "Epoch 53/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8908.1738\n",
      "Epoch 00053: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8906.5361 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5511\n",
      "Epoch 54/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8871.0322\n",
      "Epoch 00054: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8869.4014 - val_loss: 0.0333 - val_mae: 0.0330 - val_mape: 5.5471\n",
      "Epoch 55/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8876.8662\n",
      "Epoch 00055: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8866.5283 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5071\n",
      "Epoch 56/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8917.7432- ETA: 2s - loss: 0.0196 - ma\n",
      "Epoch 00056: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8916.1035 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.5076\n",
      "Epoch 57/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8886.0176\n",
      "Epoch 00057: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8884.3838 - val_loss: 0.0330 - val_mae: 0.0327 - val_mape: 5.6687\n",
      "Epoch 58/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8843.5547\n",
      "Epoch 00058: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0195 - mae: 0.0192 - mape: 8833.2559 - val_loss: 0.0340 - val_mae: 0.0337 - val_mape: 5.6366\n",
      "Epoch 59/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8897.1895\n",
      "Epoch 00059: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8886.8271 - val_loss: 0.0344 - val_mae: 0.0340 - val_mape: 5.7448\n",
      "Epoch 60/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8878.5566\n",
      "Epoch 00060: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0192 - mape: 8868.2227 - val_loss: 0.0338 - val_mae: 0.0334 - val_mape: 5.6100\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.92\n",
      "Median Absolute Error (MedAE): 341.57\n",
      "Mean Squared Error (MSE): 329961.29\n",
      "Root Mean Squared Error (RMSE): 574.42\n",
      "Mean Absolute Percentage Error (MAPE): 4.31 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.33 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPZ0OMjDJhnO"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UZgZB020JqhL"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5'\n",
    "start_epoch= 53+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ssm3plQeJn6F"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "u-3UhfYuJlge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "9b_pJN2ZJKff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9133.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03257, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 9s 283us/sample - loss: 0.0214 - mae: 0.0208 - mape: 8890.3867 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4449\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8733.1416\n",
      "Epoch 00002: val_loss improved from 0.03257 to 0.03256, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8869.8643 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4437\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9107.9521\n",
      "Epoch 00003: val_loss improved from 0.03256 to 0.03242, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8865.1064 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8707.8369\n",
      "Epoch 00004: val_loss improved from 0.03242 to 0.03233, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8851.4385 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4285\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9089.2246\n",
      "Epoch 00005: val_loss improved from 0.03233 to 0.03228, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8846.9043 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8326.9922\n",
      "Epoch 00006: val_loss improved from 0.03228 to 0.03226, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8841.4912 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8705.7549\n",
      "Epoch 00007: val_loss improved from 0.03226 to 0.03223, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8839.4424 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4216\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9068.8096\n",
      "Epoch 00008: val_loss improved from 0.03223 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8827.0107 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4208\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8693.2275  ETA: 1s - loss: 0.0213 - mae: 0.020\n",
      "Epoch 00009: val_loss improved from 0.03222 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8828.1250 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8320.9189\n",
      "Epoch 00010: val_loss improved from 0.03222 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8823.5527 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8678.0664\n",
      "Epoch 00011: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8818.6221 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9058.5664\n",
      "Epoch 00012: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8817.0547 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.8018\n",
      "Epoch 00013: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8819.4121 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9055.2490\n",
      "Epoch 00014: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8813.7979 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9056.6045\n",
      "Epoch 00015: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8815.1211 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4180\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9046.7842\n",
      "Epoch 00016: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8805.6162 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.0303\n",
      "Epoch 00017: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8811.1191 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4184\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8288.7080\n",
      "Epoch 00018: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8806.9727 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9051.8076\n",
      "Epoch 00019: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.4658 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8302.6875\n",
      "Epoch 00020: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.8389 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4196\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.1914\n",
      "Epoch 00021: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8803.5537 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8289.9629\n",
      "Epoch 00022: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8808.2617 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.5645\n",
      "Epoch 00023: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8807.9199 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8657.6855\n",
      "Epoch 00024: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.0996 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8660.9229- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00025: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4287 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4194\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.9697\n",
      "Epoch 00026: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.0215 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4190\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.5674- ETA: 1s - loss: 0.0211 - mae: 0.0205\n",
      "Epoch 00027: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.4775 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.4648\n",
      "Epoch 00028: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4198\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8674.4756\n",
      "Epoch 00029: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.5635 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7906.3037\n",
      "Epoch 00030: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.4629 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.0000\n",
      "Epoch 00031: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.7725 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4202\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9045.3105\n",
      "Epoch 00032: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.1299 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.4668  ETA: 2s - loss: 0.0211 - m\n",
      "Epoch 00033: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.4404 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8285.9170\n",
      "Epoch 00034: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.4795 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4204\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.9082\n",
      "Epoch 00035: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6709 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8681.3047- ETA: 2s - loss: 0.0210 - mae: 0.\n",
      "Epoch 00036: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.3027 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1055- ETA: 2s - loss: 0.0211 - mae: \n",
      "Epoch 00037: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.6387 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.3789\n",
      "Epoch 00038: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4212\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7911.5845\n",
      "Epoch 00039: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.6006 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4207\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7988\n",
      "Epoch 00040: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.7549 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4209\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.4160\n",
      "Epoch 00041: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8797.7119 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4218\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.8086\n",
      "Epoch 00042: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.4561 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9040.8076- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8763.4\n",
      "Epoch 00043: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.7510 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8671.4062\n",
      "Epoch 00044: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9043.8984\n",
      "Epoch 00045: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.7754 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.5801\n",
      "Epoch 00046: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4297 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4199\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.4580\n",
      "Epoch 00047: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.2539 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4226\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8656.3926\n",
      "Epoch 00048: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8795.1895 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4227\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.8350\n",
      "Epoch 00049: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6064 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9053.7900\n",
      "Epoch 00050: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8812.3799 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4222\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.2178\n",
      "Epoch 00051: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.1719 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4229\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.2744\n",
      "Epoch 00052: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.0586 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4240\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7900\n",
      "Epoch 00053: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.5859 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8664.6738\n",
      "Epoch 00054: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.6367 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4230\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8658.2246\n",
      "Epoch 00055: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.2617 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4231\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9039.0703\n",
      "Epoch 00056: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.0605 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.2510\n",
      "Epoch 00057: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.1816 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4238\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.6846\n",
      "Epoch 00058: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.4893 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4235\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.6758\n",
      "Epoch 00059: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4250\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.4639\n",
      "Epoch 00060: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.2031 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.5479\n",
      "Epoch 00061: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8801.4482 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.7363\n",
      "Epoch 00062: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.5029 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1113- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7955. - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8069\n",
      "Epoch 00063: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8803.5488 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.6602\n",
      "Epoch 00064: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8809.3477 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8292.0371\n",
      "Epoch 00065: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8800.8613 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4243\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8661.9170\n",
      "Epoch 00066: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.5088 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4233\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.0117\n",
      "Epoch 00067: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.7666 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.6230- ETA: 2s - loss: 0.0212 - mae:\n",
      "Epoch 00068: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.3867 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4254\n",
      "Epoch 69/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8285.9189\n",
      "Epoch 00069: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.3457 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4237\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.5889\n",
      "Epoch 00070: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.3418 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.3438\n",
      "Epoch 00071: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8803.1846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4249\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.1455\n",
      "Epoch 00072: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.9678 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8665.8174\n",
      "Epoch 00073: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.1689 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.3008\n",
      "Epoch 00074: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.6846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.2236\n",
      "Epoch 00075: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.9883 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4261\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.5234  ETA: 2s - loss: 0.0209 - \n",
      "Epoch 00076: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4239\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9045.4512\n",
      "Epoch 00077: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.2715 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4265\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.2568\n",
      "Epoch 00078: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0527 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4256\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9055.4580\n",
      "Epoch 00079: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.0088 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4271\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9054.0859\n",
      "Epoch 00080: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8812.6738 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9052.4775\n",
      "Epoch 00081: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.1074 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4263\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.4385\n",
      "Epoch 00082: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.1035 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.2695\n",
      "Epoch 00083: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.9912 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8680.7773\n",
      "Epoch 00084: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8681.9316\n",
      "Epoch 00085: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.9023 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4258\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.9326\n",
      "Epoch 00086: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.5898 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.4629\n",
      "Epoch 00087: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.1172 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4259\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8666.3701\n",
      "Epoch 00088: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2188 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8664.1152\n",
      "Epoch 00089: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.5791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4258\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.7578\n",
      "Epoch 00090: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4267\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8674.5449\n",
      "Epoch 00091: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.6641 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4264\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.3330\n",
      "Epoch 00092: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.8057 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8680.4805\n",
      "Epoch 00093: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8807.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4268\n",
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8669.4492\n",
      "Epoch 00094: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0020 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8670.0195\n",
      "Epoch 00095: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.6592 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.8623\n",
      "Epoch 00096: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8808.5645 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4274\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8297.3535\n",
      "Epoch 00097: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8814.3789 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.8828\n",
      "Epoch 00098: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8803.7236 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8692.5449\n",
      "Epoch 00099: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8821.6729 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4253\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.0117  ETA: 2s - loss: 0.0212 - mae: 0.0207 - mape - ETA: 1s - loss: 0.0211 - mae: 0.0206 - mape: 13501.83 - ETA: 1s - loss: 0.0211 - mae: 0.0\n",
      "Epoch 00100: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.5918 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8291.2773\n",
      "Epoch 00101: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.7295 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4274\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.1973\n",
      "Epoch 00102: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.7637 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4269\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.2109\n",
      "Epoch 00103: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.1260 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.0146\n",
      "Epoch 00104: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.8604 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8309.9893\n",
      "Epoch 00105: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.9658 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8300.4561\n",
      "Epoch 00106: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.9160 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8687.0166\n",
      "Epoch 00107: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8820.6562 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4257\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.9922  ETA: 1s - loss: 0.021\n",
      "Epoch 00108: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8818.7070 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7918.0552\n",
      "Epoch 00109: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8806.1182 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4266\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9054.2461\n",
      "Epoch 00110: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8812.8340 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 111/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.2012\n",
      "Epoch 00111: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8809.0146 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 112/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.0439\n",
      "Epoch 00112: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.5566 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 113/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8305.8838\n",
      "Epoch 00113: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.7695 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4286\n",
      "Epoch 114/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9051.7646\n",
      "Epoch 00114: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.4189 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4268\n",
      "Epoch 115/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8671.9346\n",
      "Epoch 00115: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.5635 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4283\n",
      "Epoch 116/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.4092  ETA: 2s - loss: \n",
      "Epoch 00116: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.9219 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 117/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8685.8936- ETA: 1s - loss: 0.0209 - mae: 0.02\n",
      "Epoch 00117: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8817.3330 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 118/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8302.0400\n",
      "Epoch 00118: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8812.6797 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4707\n",
      "Epoch 00119: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.0967 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 120/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8304.8125\n",
      "Epoch 00120: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8818.3047 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4265\n",
      "Epoch 121/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9058.0273\n",
      "Epoch 00121: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5293 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4270\n",
      "Epoch 122/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8682.1084  ETA: 1s - loss: 0.0208 - mae:\n",
      "Epoch 00122: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.7100 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 123/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9050.8926\n",
      "Epoch 00123: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.5742 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4287\n",
      "Epoch 124/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.2852- ETA: 2s - loss: 0.0209 - mae:\n",
      "Epoch 00124: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.8037 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4277\n",
      "Epoch 125/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.9121\n",
      "Epoch 00125: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.4541 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 126/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7553.2466\n",
      "Epoch 00126: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.8301 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 127/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.8857\n",
      "Epoch 00127: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.4443 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 128/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.3145\n",
      "Epoch 00128: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.7705 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 129/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9062.5234\n",
      "Epoch 00129: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8820.9062 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 130/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8310.6270\n",
      "Epoch 00130: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.6572 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4276\n",
      "Epoch 131/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8676.7832\n",
      "Epoch 00131: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8813.8525 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 132/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.6250\n",
      "Epoch 00132: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1289 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 133/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.2627\n",
      "Epoch 00133: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.7959 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 134/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9385\n",
      "Epoch 00134: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.0283 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 135/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9990\n",
      "Epoch 00135: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.8438 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 136/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9050.7119- ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8224.\n",
      "Epoch 00136: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.3926 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 137/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9065.7979\n",
      "Epoch 00137: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.0713 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 138/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.0449\n",
      "Epoch 00138: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.1436 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4327\n",
      "Epoch 139/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.4678\n",
      "Epoch 00139: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.9834 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 140/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.8877\n",
      "Epoch 00140: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.4102 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 141/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.1895\n",
      "Epoch 00141: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.6465 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4293\n",
      "Epoch 142/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9068.6006\n",
      "Epoch 00142: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8826.8496 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4311\n",
      "Epoch 143/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.5254\n",
      "Epoch 00143: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.0791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8303.7139\n",
      "Epoch 00144: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.9434 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 145/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9057.6035\n",
      "Epoch 00145: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1113 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 146/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8295.9824\n",
      "Epoch 00146: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 147/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8299.4932- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00147: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.4980 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 148/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8677.7822\n",
      "Epoch 00148: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.8496 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 149/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4521\n",
      "Epoch 00149: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8807.3672 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4317\n",
      "Epoch 150/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8687.6270\n",
      "Epoch 00150: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.2520 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 151/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7550.1221\n",
      "Epoch 00151: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5537 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4318\n",
      "Epoch 152/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8665.8770\n",
      "Epoch 00152: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.2012 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4307\n",
      "Epoch 153/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8305.3477\n",
      "Epoch 00153: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.7168 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 154/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8664.9102\n",
      "Epoch 00154: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8803.5449 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 155/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.1807\n",
      "Epoch 00155: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.1172 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4323\n",
      "Epoch 156/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9061.2051\n",
      "Epoch 00156: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8819.6104 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 157/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9058.3799\n",
      "Epoch 00157: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.8613 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4316\n",
      "Epoch 158/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9053.1445\n",
      "Epoch 00158: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8811.7646 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 159/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.0205\n",
      "Epoch 00159: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8813.5781 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 160/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8673.6445\n",
      "Epoch 00160: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8812.8994 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4306\n",
      "Epoch 161/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.0654\n",
      "Epoch 00161: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8804.1348 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 162/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8678.7412\n",
      "Epoch 00162: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8817.2510 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 163/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9058.5049  ETA: 1s - loss: 0.0209 - mae: 0.0204 - map\n",
      "Epoch 00163: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.9990 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4288\n",
      "Epoch 164/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8671.3730\n",
      "Epoch 00164: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8807.5947 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 165/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8304.8350\n",
      "Epoch 00165: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8825.1436 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4294\n",
      "Epoch 166/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7930.7822\n",
      "Epoch 00166: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.1318 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 167/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8676.1992  ETA: 1s - loss: 0.0209 - mae: 0.0203 - mape:  - ETA: 1s - loss: 0.0208 - mae: 0.0202 - mape: - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9\n",
      "Epoch 00167: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.6123 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8682.7363\n",
      "Epoch 00168: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.8896 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 169/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9065.5938\n",
      "Epoch 00169: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8823.9004 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 170/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9051.8340\n",
      "Epoch 00170: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.4756 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 171/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8678.6602- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8967.800\n",
      "Epoch 00171: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8815.0049 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 172/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.9189\n",
      "Epoch 00172: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.3682 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 173/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9055.6904\n",
      "Epoch 00173: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2402 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 174/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.9043- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8588.017\n",
      "Epoch 00174: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.3857 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 175/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.7666\n",
      "Epoch 00175: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3809 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 176/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8673.9277\n",
      "Epoch 00176: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.3027 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4343\n",
      "Epoch 177/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.8848\n",
      "Epoch 00177: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2305 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4321\n",
      "Epoch 178/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9054.9736  ETA: 1s - loss: 0.0208 - mae: 0.0203 \n",
      "Epoch 00178: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.5439 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4330\n",
      "Epoch 179/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8675.4736\n",
      "Epoch 00179: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8809.1797 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4327\n",
      "Epoch 180/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9061.8086\n",
      "Epoch 00180: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2021 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4329\n",
      "Epoch 181/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9060.0869\n",
      "Epoch 00181: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5107 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4319\n",
      "Epoch 182/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.6191\n",
      "Epoch 00182: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5000 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 183/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.7178\n",
      "Epoch 00183: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2656 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4324\n",
      "Epoch 184/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8291.2705\n",
      "Epoch 00184: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.1533 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4331\n",
      "Epoch 185/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.7324\n",
      "Epoch 00185: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.8145 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 186/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.2549\n",
      "Epoch 00186: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8817.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4305\n",
      "Epoch 187/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.2168\n",
      "Epoch 00187: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8819.0938 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4315\n",
      "Epoch 188/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.7939\n",
      "Epoch 00188: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.2949 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 189/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.9834\n",
      "Epoch 00189: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.4678 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 190/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.2012\n",
      "Epoch 00190: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7588 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4332\n",
      "Epoch 191/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8679.1182\n",
      "Epoch 00191: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.3721 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4325\n",
      "Epoch 192/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8303.0508\n",
      "Epoch 00192: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7334 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 193/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8677.2891\n",
      "Epoch 00193: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.0742 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4315\n",
      "Epoch 194/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9051.3916\n",
      "Epoch 00194: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.0596 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4353\n",
      "Epoch 195/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 7911.0254\n",
      "Epoch 00195: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.6299 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 196/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9053.1436\n",
      "Epoch 00196: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.7627 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4345\n",
      "Epoch 197/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9056.7646- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 73\n",
      "Epoch 00197: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.2822 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 198/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8293.9717\n",
      "Epoch 00198: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3867 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4346\n",
      "Epoch 199/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.1162\n",
      "Epoch 00199: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.7646 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4328\n",
      "Epoch 200/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.9668- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 796\n",
      "Epoch 00200: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8808.9336 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4344\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.57\n",
      "Median Absolute Error (MedAE): 338.68\n",
      "Mean Squared Error (MSE): 332645.96\n",
      "Root Mean Squared Error (RMSE): 576.75\n",
      "Mean Absolute Percentage Error (MAPE): 4.3 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.3 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-0013-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aW5pp6LD-1"
   },
   "source": [
    "# E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "j9RoOmzuLgHR"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B2H5iBwQLfhj"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "k06fUuK7LerZ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mB2kXVVCLRBA"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sizKJbygLNBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Q6uDSkmdLJlR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.1973 - mae: 0.0794 - mape: 9692.2637\n",
      "Epoch 00001: val_loss improved from inf to 0.10644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0001-loss0.11.h5\n",
      "32614/32614 [==============================] - 19s 584us/sample - loss: 0.1972 - mae: 0.0794 - mape: 9680.9932 - val_loss: 0.1064 - val_mae: 0.0766 - val_mape: 12.9892\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0557 - mae: 0.0421 - mape: 9249.9512\n",
      "Epoch 00002: val_loss improved from 0.10644 to 0.05840, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0002-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0556 - mae: 0.0420 - mape: 9230.1133 - val_loss: 0.0584 - val_mae: 0.0523 - val_mape: 8.7301\n",
      "Epoch 3/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0416 - mae: 0.0375 - mape: 9262.3770\n",
      "Epoch 00003: val_loss improved from 0.05840 to 0.05401, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0003-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0416 - mae: 0.0375 - mape: 9224.3594 - val_loss: 0.0540 - val_mae: 0.0503 - val_mape: 8.7703\n",
      "Epoch 4/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0398 - mae: 0.0367 - mape: 9258.6729\n",
      "Epoch 00004: val_loss improved from 0.05401 to 0.04371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 431us/sample - loss: 0.0398 - mae: 0.0367 - mape: 9247.8994 - val_loss: 0.0437 - val_mae: 0.0407 - val_mape: 6.7483\n",
      "Epoch 5/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0336 - mape: 9132.0303  ETA: 4s - \n",
      "Epoch 00005: val_loss did not improve from 0.04371\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0362 - mae: 0.0336 - mape: 9121.3975 - val_loss: 0.0456 - val_mae: 0.0433 - val_mape: 7.2883\n",
      "Epoch 6/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0332 - mape: 9247.4834\n",
      "Epoch 00006: val_loss improved from 0.04371 to 0.03429, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0358 - mae: 0.0332 - mape: 9227.6475 - val_loss: 0.0343 - val_mae: 0.0317 - val_mape: 5.5244\n",
      "Epoch 7/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0334 - mae: 0.0314 - mape: 8832.6689\n",
      "Epoch 00007: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0334 - mae: 0.0314 - mape: 9193.4805 - val_loss: 0.0770 - val_mae: 0.0752 - val_mape: 10.8461\n",
      "Epoch 8/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0309 - mape: 9279.7900\n",
      "Epoch 00008: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0326 - mae: 0.0309 - mape: 9241.6865 - val_loss: 0.0911 - val_mae: 0.0895 - val_mape: 12.6461\n",
      "Epoch 9/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0301 - mape: 9240.0088\n",
      "Epoch 00009: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0315 - mae: 0.0301 - mape: 9211.1299 - val_loss: 0.0497 - val_mae: 0.0483 - val_mape: 7.3868\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0289 - mape: 8889.9570\n",
      "Epoch 00010: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0303 - mae: 0.0289 - mape: 9243.3398 - val_loss: 0.0727 - val_mae: 0.0714 - val_mape: 10.6575\n",
      "Epoch 11/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0290 - mape: 9401.6436\n",
      "Epoch 00011: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0303 - mae: 0.0290 - mape: 9372.2676 - val_loss: 0.0601 - val_mae: 0.0589 - val_mape: 8.9566\n",
      "Epoch 12/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0292 - mae: 0.0280 - mape: 9176.0518\n",
      "Epoch 00012: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0292 - mae: 0.0280 - mape: 9147.3730 - val_loss: 0.0464 - val_mae: 0.0452 - val_mape: 7.1787\n",
      "Epoch 13/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0291 - mae: 0.0279 - mape: 9234.7246\n",
      "Epoch 00013: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0291 - mae: 0.0279 - mape: 9214.9170 - val_loss: 0.0398 - val_mae: 0.0387 - val_mape: 6.3188\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.0278 - mape: 9259.1611\n",
      "Epoch 00014: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0290 - mae: 0.0278 - mape: 9221.1396 - val_loss: 0.0613 - val_mae: 0.0602 - val_mape: 8.9035\n",
      "Epoch 15/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0285 - mae: 0.0274 - mape: 9332.2979\n",
      "Epoch 00015: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0285 - mae: 0.0274 - mape: 9293.9756 - val_loss: 0.0448 - val_mae: 0.0438 - val_mape: 6.8179\n",
      "Epoch 16/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9270.8516- ETA: 0s - loss: 0.0278 - mae: 0.0268 - ma\n",
      "Epoch 00016: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9269.1475 - val_loss: 0.0455 - val_mae: 0.0446 - val_mape: 7.0034\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9212.3809\n",
      "Epoch 00017: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9210.6865 - val_loss: 0.0670 - val_mae: 0.0661 - val_mape: 9.5463\n",
      "Epoch 18/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9337.9092\n",
      "Epoch 00018: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9308.7227 - val_loss: 0.0456 - val_mae: 0.0447 - val_mape: 7.0638\n",
      "Epoch 19/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9224.4463\n",
      "Epoch 00019: val_loss improved from 0.03429 to 0.03249, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0019-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9213.7041 - val_loss: 0.0325 - val_mae: 0.0316 - val_mape: 5.3572\n",
      "Epoch 20/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9302.4258\n",
      "Epoch 00020: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9264.2266 - val_loss: 0.0382 - val_mae: 0.0373 - val_mape: 6.0539\n",
      "Epoch 21/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9244.2197\n",
      "Epoch 00021: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 436us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9206.2666 - val_loss: 0.0409 - val_mae: 0.0400 - val_mape: 6.2748\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0260 - mape: 9348.6699\n",
      "Epoch 00022: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0269 - mae: 0.0260 - mape: 9337.7842 - val_loss: 0.0328 - val_mae: 0.0319 - val_mape: 5.4575\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9254.0684\n",
      "Epoch 00023: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0266 - mae: 0.0257 - mape: 9252.3672 - val_loss: 0.0552 - val_mae: 0.0543 - val_mape: 8.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9298.8604\n",
      "Epoch 00024: val_loss improved from 0.03249 to 0.03114, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0024-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0265 - mae: 0.0257 - mape: 9260.6738 - val_loss: 0.0311 - val_mae: 0.0303 - val_mape: 5.1846\n",
      "Epoch 25/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0254 - mape: 9225.4922\n",
      "Epoch 00025: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0262 - mae: 0.0254 - mape: 9223.7969 - val_loss: 0.0519 - val_mae: 0.0511 - val_mape: 7.9640\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0254 - mape: 9168.1641\n",
      "Epoch 00026: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0263 - mae: 0.0254 - mape: 9139.5078 - val_loss: 0.0324 - val_mae: 0.0315 - val_mape: 5.4029\n",
      "Epoch 27/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0252 - mape: 9292.8506\n",
      "Epoch 00027: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 435us/sample - loss: 0.0261 - mae: 0.0252 - mape: 9282.0293 - val_loss: 0.0368 - val_mae: 0.0359 - val_mape: 5.8316\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0250 - mape: 9257.2148\n",
      "Epoch 00028: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0259 - mae: 0.0250 - mape: 9246.4346 - val_loss: 0.0322 - val_mae: 0.0313 - val_mape: 5.3347\n",
      "Epoch 29/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0249 - mape: 9201.8809\n",
      "Epoch 00029: val_loss improved from 0.03114 to 0.03017, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0257 - mae: 0.0249 - mape: 9173.1152 - val_loss: 0.0302 - val_mae: 0.0293 - val_mape: 5.0982\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0246 - mape: 9224.1299\n",
      "Epoch 00030: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0255 - mae: 0.0246 - mape: 9195.2969 - val_loss: 0.0316 - val_mae: 0.0308 - val_mape: 5.3475\n",
      "Epoch 31/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0247 - mape: 9221.6504\n",
      "Epoch 00031: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0256 - mae: 0.0247 - mape: 9219.9551 - val_loss: 0.0383 - val_mae: 0.0375 - val_mape: 6.0447\n",
      "Epoch 32/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9185.3818\n",
      "Epoch 00032: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0253 - mae: 0.0244 - mape: 9156.6689 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.6061\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9151.5020\n",
      "Epoch 00033: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0252 - mae: 0.0243 - mape: 9104.9512 - val_loss: 0.0435 - val_mae: 0.0426 - val_mape: 6.6821\n",
      "Epoch 34/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0240 - mape: 9209.8779\n",
      "Epoch 00034: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0248 - mae: 0.0240 - mape: 9163.0439 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 5.9090\n",
      "Epoch 35/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0243 - mape: 9190.4863\n",
      "Epoch 00035: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0251 - mae: 0.0243 - mape: 9170.7715 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.4763\n",
      "Epoch 36/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0241 - mape: 9201.8359\n",
      "Epoch 00036: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0250 - mae: 0.0241 - mape: 9182.0967 - val_loss: 0.0393 - val_mae: 0.0384 - val_mape: 6.1283\n",
      "Epoch 37/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0247 - mae: 0.0238 - mape: 9064.8945\n",
      "Epoch 00037: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0247 - mae: 0.0238 - mape: 9063.2285 - val_loss: 0.0322 - val_mae: 0.0314 - val_mape: 5.4603\n",
      "Epoch 38/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0236 - mape: 9153.4014\n",
      "Epoch 00038: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0245 - mae: 0.0236 - mape: 9142.7412 - val_loss: 0.0396 - val_mae: 0.0387 - val_mape: 6.1510\n",
      "Epoch 39/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9212.5186\n",
      "Epoch 00039: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9183.7207 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5752\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0237 - mape: 9298.7363\n",
      "Epoch 00040: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0246 - mae: 0.0237 - mape: 9278.7891 - val_loss: 0.0358 - val_mae: 0.0349 - val_mape: 5.7883\n",
      "Epoch 41/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9290.7383\n",
      "Epoch 00041: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9252.5859 - val_loss: 0.0359 - val_mae: 0.0350 - val_mape: 6.1253\n",
      "Epoch 42/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0244 - mae: 0.0235 - mape: 9100.7422\n",
      "Epoch 00042: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0244 - mae: 0.0235 - mape: 9099.0762 - val_loss: 0.0365 - val_mae: 0.0356 - val_mape: 5.8232\n",
      "Epoch 43/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.0231 - mape: 9206.7373\n",
      "Epoch 00043: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0240 - mae: 0.0231 - mape: 9186.9854 - val_loss: 0.0428 - val_mae: 0.0419 - val_mape: 6.5949\n",
      "Epoch 44/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.0229 - mape: 9111.2773\n",
      "Epoch 00044: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0238 - mae: 0.0229 - mape: 9109.6025 - val_loss: 0.0335 - val_mae: 0.0326 - val_mape: 5.5624\n",
      "Epoch 45/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0229 - mape: 9251.8613\n",
      "Epoch 00045: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0229 - mape: 9250.1602 - val_loss: 0.0383 - val_mae: 0.0374 - val_mape: 6.0213\n",
      "Epoch 46/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0228 - mape: 9249.3652\n",
      "Epoch 00046: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9229.5244 - val_loss: 0.0470 - val_mae: 0.0461 - val_mape: 7.1372\n",
      "Epoch 47/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0228 - mape: 9240.2764\n",
      "Epoch 00047: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9220.4561 - val_loss: 0.0355 - val_mae: 0.0346 - val_mape: 5.8071\n",
      "Epoch 48/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0227 - mape: 9228.7842\n",
      "Epoch 00048: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0236 - mae: 0.0227 - mape: 9227.0869 - val_loss: 0.0335 - val_mae: 0.0327 - val_mape: 5.7051\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0225 - mape: 9308.8867\n",
      "Epoch 00049: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0234 - mae: 0.0225 - mape: 9279.7959 - val_loss: 0.0391 - val_mae: 0.0382 - val_mape: 6.2820\n",
      "Epoch 50/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0223 - mape: 8910.0205\n",
      "Epoch 00050: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0232 - mae: 0.0223 - mape: 9253.5254 - val_loss: 0.0347 - val_mae: 0.0338 - val_mape: 5.7545\n",
      "Epoch 51/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0224 - mape: 9279.0811\n",
      "Epoch 00051: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0233 - mae: 0.0224 - mape: 9277.3750 - val_loss: 0.0390 - val_mae: 0.0381 - val_mape: 6.1803\n",
      "Epoch 52/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0222 - mape: 9221.8008\n",
      "Epoch 00052: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0231 - mae: 0.0222 - mape: 9202.0244 - val_loss: 0.0355 - val_mae: 0.0347 - val_mape: 5.8992\n",
      "Epoch 53/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0218 - mape: 9313.6279\n",
      "Epoch 00053: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0227 - mae: 0.0219 - mape: 9284.5176 - val_loss: 0.0346 - val_mae: 0.0337 - val_mape: 5.8458\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0221 - mape: 9235.9512\n",
      "Epoch 00054: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0230 - mae: 0.0221 - mape: 9207.0791 - val_loss: 0.0399 - val_mae: 0.0391 - val_mape: 6.3575\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.0219 - mape: 9211.0430\n",
      "Epoch 00055: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0228 - mae: 0.0219 - mape: 9191.2910 - val_loss: 0.0397 - val_mae: 0.0388 - val_mape: 6.2600\n",
      "Epoch 56/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0217 - mape: 9251.5020\n",
      "Epoch 00056: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0226 - mae: 0.0217 - mape: 9240.7275 - val_loss: 0.0405 - val_mae: 0.0397 - val_mape: 6.3565\n",
      "Epoch 57/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 9213.3115\n",
      "Epoch 00057: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9166.4385 - val_loss: 0.0343 - val_mae: 0.0334 - val_mape: 5.7472\n",
      "Epoch 58/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0216 - mape: 9254.9170\n",
      "Epoch 00058: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9253.2207 - val_loss: 0.0376 - val_mae: 0.0367 - val_mape: 6.0893\n",
      "Epoch 59/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0220 - mape: 9354.7930\n",
      "Epoch 00059: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0230 - mae: 0.0220 - mape: 9325.5488 - val_loss: 0.0341 - val_mae: 0.0332 - val_mape: 5.7046\n",
      "Epoch 60/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0214 - mape: 9235.1680\n",
      "Epoch 00060: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0222 - mae: 0.0214 - mape: 9188.1836 - val_loss: 0.0366 - val_mae: 0.0358 - val_mape: 5.9825\n",
      "Epoch 61/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0215 - mape: 9196.2959\n",
      "Epoch 00061: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0223 - mae: 0.0215 - mape: 9167.5479 - val_loss: 0.0399 - val_mae: 0.0390 - val_mape: 6.2796\n",
      "Epoch 62/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0213 - mape: 9107.4619\n",
      "Epoch 00062: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0222 - mae: 0.0213 - mape: 9087.9248 - val_loss: 0.0385 - val_mae: 0.0376 - val_mape: 6.1628\n",
      "Epoch 63/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0213 - mape: 9210.2637\n",
      "Epoch 00063: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0221 - mae: 0.0213 - mape: 9172.4463 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5972\n",
      "Epoch 64/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9124.6074\n",
      "Epoch 00064: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9122.9297 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 6.0281\n",
      "Epoch 65/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0212 - mape: 9226.6611\n",
      "Epoch 00065: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0220 - mae: 0.0212 - mape: 9188.7695 - val_loss: 0.0377 - val_mae: 0.0369 - val_mape: 6.2115\n",
      "Epoch 66/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9052.7314\n",
      "Epoch 00066: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9051.0664 - val_loss: 0.0405 - val_mae: 0.0396 - val_mape: 6.4286\n",
      "Epoch 67/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0209 - mape: 9083.4209\n",
      "Epoch 00067: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0218 - mae: 0.0209 - mape: 9072.8418 - val_loss: 0.0365 - val_mae: 0.0357 - val_mape: 6.0204\n",
      "Epoch 68/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9076.5205\n",
      "Epoch 00068: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9039.2510 - val_loss: 0.0342 - val_mae: 0.0334 - val_mape: 5.6559\n",
      "Epoch 69/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0208 - mape: 9033.5391\n",
      "Epoch 00069: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0217 - mae: 0.0208 - mape: 9005.3018 - val_loss: 0.0396 - val_mae: 0.0388 - val_mape: 6.4502\n",
      "Epoch 70/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9120.9199\n",
      "Epoch 00070: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9083.4629 - val_loss: 0.0361 - val_mae: 0.0353 - val_mape: 5.8008\n",
      "Epoch 71/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9097.3145\n",
      "Epoch 00071: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9095.6416 - val_loss: 0.0361 - val_mae: 0.0352 - val_mape: 5.9642\n",
      "Epoch 72/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0206 - mape: 9088.2021\n",
      "Epoch 00072: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0206 - mape: 9086.5312 - val_loss: 0.0380 - val_mae: 0.0372 - val_mape: 6.1542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9105.6396\n",
      "Epoch 00073: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9059.3135 - val_loss: 0.0434 - val_mae: 0.0426 - val_mape: 6.7060\n",
      "Epoch 74/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0205 - mape: 9066.7041\n",
      "Epoch 00074: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0205 - mape: 9065.0361 - val_loss: 0.0346 - val_mae: 0.0338 - val_mape: 5.7416\n",
      "Epoch 75/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0205 - mape: 8978.4404\n",
      "Epoch 00075: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0213 - mae: 0.0205 - mape: 8967.9941 - val_loss: 0.0348 - val_mae: 0.0340 - val_mape: 5.9216\n",
      "Epoch 76/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9057.9648\n",
      "Epoch 00076: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0210 - mae: 0.0202 - mape: 9056.2988 - val_loss: 0.0398 - val_mae: 0.0390 - val_mape: 6.3300\n",
      "Epoch 77/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0203 - mape: 8981.2236\n",
      "Epoch 00077: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0212 - mae: 0.0203 - mape: 8979.5723 - val_loss: 0.0451 - val_mae: 0.0444 - val_mape: 7.0400\n",
      "Epoch 78/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9038.3955\n",
      "Epoch 00078: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0210 - mae: 0.0202 - mape: 8992.4121 - val_loss: 0.0360 - val_mae: 0.0353 - val_mape: 5.9399\n",
      "Epoch 79/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0201 - mape: 8948.2764\n",
      "Epoch 00079: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0209 - mae: 0.0201 - mape: 8920.3037 - val_loss: 0.0350 - val_mae: 0.0342 - val_mape: 5.7982\n",
      "Epoch 80/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0200 - mape: 8990.1953\n",
      "Epoch 00080: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0208 - mae: 0.0200 - mape: 8962.0898 - val_loss: 0.0358 - val_mae: 0.0350 - val_mape: 5.8451\n"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "cW3NWXtXLDA-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 420.56\n",
      "Median Absolute Error (MedAE): 333.06\n",
      "Mean Squared Error (MSE): 313586.16\n",
      "Root Mean Squared Error (RMSE): 559.99\n",
      "Mean Absolute Percentage Error (MAPE): 4.19 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.25 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5'\n",
    "start_epoch= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0228 - mape: 9138.6191- ETA: 3s - loss: 0.023\n",
      "Epoch 00001: val_loss improved from inf to 0.03148, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 56s 2ms/sample - loss: 0.0235 - mae: 0.0228 - mape: 9136.9395 - val_loss: 0.0315 - val_mae: 0.0307 - val_mape: 5.2145\n",
      "Epoch 2/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0225 - mape: 9164.0752\n",
      "Epoch 00002: val_loss improved from 0.03148 to 0.03063, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0232 - mae: 0.0225 - mape: 9162.3906 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1229\n",
      "Epoch 3/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0224 - mape: 9196.9521- ETA: 3s - loss: 0.0\n",
      "Epoch 00003: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0231 - mae: 0.0224 - mape: 9195.2607 - val_loss: 0.0314 - val_mae: 0.0307 - val_mape: 5.2105\n",
      "Epoch 4/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0223 - mape: 9131.6660\n",
      "Epoch 00004: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0230 - mae: 0.0223 - mape: 9121.0312 - val_loss: 0.0310 - val_mae: 0.0304 - val_mape: 5.1898\n",
      "Epoch 5/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.0223 - mape: 9159.3213\n",
      "Epoch 00005: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0229 - mae: 0.0223 - mape: 9148.6533 - val_loss: 0.0321 - val_mae: 0.0314 - val_mape: 5.2817\n",
      "Epoch 6/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9180.6631\n",
      "Epoch 00006: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0228 - mae: 0.0221 - mape: 9178.9824 - val_loss: 0.0309 - val_mae: 0.0303 - val_mape: 5.1679\n",
      "Epoch 7/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9175.2949\n",
      "Epoch 00007: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0227 - mae: 0.0221 - mape: 9173.6084 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.2847\n",
      "Epoch 8/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0220 - mape: 9205.9355\n",
      "Epoch 00008: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0226 - mae: 0.0220 - mape: 9195.2217 - val_loss: 0.0307 - val_mae: 0.0301 - val_mape: 5.2188\n",
      "Epoch 9/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0219 - mape: 9181.9873\n",
      "Epoch 00009: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0225 - mae: 0.0219 - mape: 9180.2988 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.2595\n",
      "Epoch 10/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0218 - mape: 9186.6875\n",
      "Epoch 00010: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0224 - mae: 0.0218 - mape: 9184.9990 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2146\n",
      "Epoch 11/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0217 - mape: 9203.6816\n",
      "Epoch 00011: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0223 - mae: 0.0217 - mape: 9192.9629 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.2609\n",
      "Epoch 12/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9241.5742\n",
      "Epoch 00012: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 9239.8750 - val_loss: 0.0311 - val_mae: 0.0305 - val_mape: 5.2300\n",
      "Epoch 13/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0216 - mape: 9247.9932\n",
      "Epoch 00013: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0216 - mape: 9237.2227 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4177\n",
      "Epoch 14/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0215 - mape: 9297.9727\n",
      "Epoch 00014: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0215 - mape: 9296.2627 - val_loss: 0.0307 - val_mae: 0.0302 - val_mape: 5.2173\n",
      "Epoch 15/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0214 - mape: 9263.5205\n",
      "Epoch 00015: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0220 - mae: 0.0214 - mape: 9261.8164 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2651\n",
      "Epoch 16/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 9198.6846\n",
      "Epoch 00016: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 9196.9941 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.3174\n",
      "Epoch 17/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 9260.9404\n",
      "Epoch 00017: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 9250.1543 - val_loss: 0.0309 - val_mae: 0.0304 - val_mape: 5.2860\n",
      "Epoch 18/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0212 - mape: 9216.3271\n",
      "Epoch 00018: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0212 - mape: 9214.6318 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3829\n",
      "Epoch 19/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0211 - mape: 9211.0586\n",
      "Epoch 00019: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0211 - mape: 9200.3320 - val_loss: 0.0341 - val_mae: 0.0336 - val_mape: 5.5767\n",
      "Epoch 20/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0209 - mape: 9201.0938\n",
      "Epoch 00020: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0215 - mae: 0.0209 - mape: 9190.3779 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3684\n",
      "Epoch 21/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9219.9883\n",
      "Epoch 00021: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0214 - mae: 0.0208 - mape: 9218.2930 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3618\n",
      "Epoch 22/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 9187.7080\n",
      "Epoch 00022: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 9186.0186 - val_loss: 0.0316 - val_mae: 0.0310 - val_mape: 5.3197\n",
      "Epoch 23/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9196.0430\n",
      "Epoch 00023: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0206 - mape: 9194.3525 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2367\n",
      "Epoch 24/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9184.2910\n",
      "Epoch 00024: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 9173.5947 - val_loss: 0.0315 - val_mae: 0.0309 - val_mape: 5.2920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9166.1699\n",
      "Epoch 00025: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0210 - mae: 0.0205 - mape: 9155.4951 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.3492\n",
      "Epoch 26/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9205.6270\n",
      "Epoch 00026: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9203.9385 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2379\n",
      "Epoch 27/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9117.5986\n",
      "Epoch 00027: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9115.9219 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4364\n",
      "Epoch 28/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9204.3340\n",
      "Epoch 00028: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9193.6143 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2468\n",
      "Epoch 29/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9128.1553\n",
      "Epoch 00029: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9126.4766 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3819\n",
      "Epoch 30/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0202 - mape: 9126.3975\n",
      "Epoch 00030: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0207 - mae: 0.0202 - mape: 9124.7197 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.3127\n",
      "Epoch 31/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9127.3574\n",
      "Epoch 00031: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9125.6787 - val_loss: 0.0314 - val_mae: 0.0308 - val_mape: 5.3096\n",
      "Epoch 32/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9139.1289\n",
      "Epoch 00032: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9137.4482 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3143\n",
      "Epoch 33/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0200 - mape: 9129.0029\n",
      "Epoch 00033: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0205 - mae: 0.0200 - mape: 9127.3242 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3023\n",
      "Epoch 34/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9085.1152\n",
      "Epoch 00034: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9083.4443 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3421\n",
      "Epoch 35/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9161.0391\n",
      "Epoch 00035: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9159.3545 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2771\n",
      "Epoch 36/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0198 - mape: 9147.9355\n",
      "Epoch 00036: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0203 - mae: 0.0198 - mape: 9137.2803 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3513\n",
      "Epoch 37/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9170.7051- ETA: 4s - loss: 0. - ETA: 2s - loss: 0.0203\n",
      "Epoch 00037: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9169.0186 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2897\n",
      "Epoch 38/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9186.7959\n",
      "Epoch 00038: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9176.0967 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2613\n",
      "Epoch 39/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0195 - mape: 9121.9736\n",
      "Epoch 00039: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0200 - mae: 0.0195 - mape: 9120.2959 - val_loss: 0.0348 - val_mae: 0.0343 - val_mape: 5.6403\n",
      "Epoch 40/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0194 - mape: 9143.8574\n",
      "Epoch 00040: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0199 - mae: 0.0194 - mape: 9142.1768 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3696\n",
      "Epoch 41/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0192 - mape: 9166.4883\n",
      "Epoch 00041: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0197 - mae: 0.0192 - mape: 9155.8125 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3686\n",
      "Epoch 42/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9157.3496\n",
      "Epoch 00042: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9146.6846 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3299\n",
      "Epoch 43/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9169.7188\n",
      "Epoch 00043: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9159.0391 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2889\n",
      "Epoch 44/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0190 - mape: 9109.2627\n",
      "Epoch 00044: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0195 - mae: 0.0190 - mape: 9107.5879 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3664\n",
      "Epoch 45/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0189 - mape: 9101.7988\n",
      "Epoch 00045: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0195 - mae: 0.0189 - mape: 9100.1250 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4306\n",
      "Epoch 46/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0194 - mae: 0.0189 - mape: 9152.3506\n",
      "Epoch 00046: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0194 - mae: 0.0189 - mape: 9150.6680 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.3450\n",
      "Epoch 47/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0193 - mae: 0.0187 - mape: 9092.9639\n",
      "Epoch 00047: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0193 - mae: 0.0187 - mape: 9091.2920 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3590\n",
      "Epoch 48/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.0186 - mape: 9166.8369\n",
      "Epoch 00048: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0191 - mae: 0.0186 - mape: 9165.1514 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4518\n",
      "Epoch 49/200\n",
      "11776/32614 [=========>....................] - ETA: 32s - loss: 0.0191 - mae: 0.0186 - mape: 7337.6323"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-048f600f12b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 412.44\n",
      "Median Absolute Error (MedAE): 328.08\n",
      "Mean Squared Error (MSE): 301264.17\n",
      "Root Mean Squared Error (RMSE): 548.88\n",
      "Mean Absolute Percentage Error (MAPE): 4.12 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.19 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5'\n",
    "start_epoch= 30+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0218 - mape: 9021.4141\n",
      "Epoch 00001: val_loss improved from inf to 0.03075, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 7s 222us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9155.7246 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1276\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9392.5967\n",
      "Epoch 00002: val_loss improved from 0.03075 to 0.03067, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9142.1660 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1187\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 8595.9766\n",
      "Epoch 00003: val_loss improved from 0.03067 to 0.03054, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9143.0762 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1073\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9385.6611\n",
      "Epoch 00004: val_loss improved from 0.03054 to 0.03050, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9135.4072 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1036\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9377.5986\n",
      "Epoch 00005: val_loss improved from 0.03050 to 0.03046, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9127.5752 - val_loss: 0.0305 - val_mae: 0.0297 - val_mape: 5.0995\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8606.6309\n",
      "Epoch 00006: val_loss improved from 0.03046 to 0.03042, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9137.0928 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0965\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9375.1191\n",
      "Epoch 00007: val_loss improved from 0.03042 to 0.03041, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9125.1416 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9374.5898\n",
      "Epoch 00008: val_loss improved from 0.03041 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9124.6318 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0924\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8210.0459\n",
      "Epoch 00009: val_loss improved from 0.03038 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9126.5254 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0921\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8572.6328  ETA: 1s - loss: 0.0225 - mae: 0.0218 \n",
      "Epoch 00010: val_loss improved from 0.03038 to 0.03035, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9118.1846 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0894\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8207.2930\n",
      "Epoch 00011: val_loss did not improve from 0.03035\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9111.9365 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0895\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9357.2139\n",
      "Epoch 00012: val_loss improved from 0.03035 to 0.03034, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0012-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.7373 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0893\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8967.2090\n",
      "Epoch 00013: val_loss improved from 0.03034 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0859 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0887\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8584.7832\n",
      "Epoch 00014: val_loss improved from 0.03033 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0014-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9115.9170 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9359.8037\n",
      "Epoch 00015: val_loss improved from 0.03033 to 0.03030, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0015-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.2441 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0861\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8970.0029\n",
      "Epoch 00016: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9116.9443 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0868\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8182.7358\n",
      "Epoch 00017: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.9473 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0872\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9360.5244\n",
      "Epoch 00018: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.9551 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8964.7793\n",
      "Epoch 00019: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0879 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8972.5371\n",
      "Epoch 00020: val_loss improved from 0.03030 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0020-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9106.9902 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0856\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8572.1016\n",
      "Epoch 00021: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9105.2520 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0876\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9354.3223\n",
      "Epoch 00022: val_loss improved from 0.03028 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.9277 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0858\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9355.4004- ETA: 1s - loss: 0.0223 - mae: 0.0216 - mape: 9009 - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 869\n",
      "Epoch 00023: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9105.9697 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9358.2070\n",
      "Epoch 00024: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9108.6885 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0860\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8967.9619\n",
      "Epoch 00025: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.4453 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8956.2822\n",
      "Epoch 00026: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.1289 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0879\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9353.0088\n",
      "Epoch 00027: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9103.6240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.5430\n",
      "Epoch 00028: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1504 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0871\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9354.4570\n",
      "Epoch 00029: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.0547 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0886\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.4277\n",
      "Epoch 00030: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.0498 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2637\n",
      "Epoch 00031: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0391 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0890\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.9912\n",
      "Epoch 00032: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.1416 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0896\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8960.5801\n",
      "Epoch 00033: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7783 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0913\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9346.8809\n",
      "Epoch 00034: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9097.6689 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8965.5186\n",
      "Epoch 00035: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8569.2568\n",
      "Epoch 00036: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1602 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0915\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.2012  ETA: 2s - loss: 0.02\n",
      "Epoch 00037: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1436 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0920\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8574.7637\n",
      "Epoch 00038: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.4717 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0902\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.5205\n",
      "Epoch 00039: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.2197 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.8828\n",
      "Epoch 00040: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.5615 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.7637\n",
      "Epoch 00041: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.3633 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0908\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9359.6875\n",
      "Epoch 00042: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0914\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.9414\n",
      "Epoch 00043: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9099.0850 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3818\n",
      "Epoch 00044: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9072 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0957\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2998\n",
      "Epoch 00045: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0811 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8428- ETA: 2s - loss: 0.0223 - mae: 0\n",
      "Epoch 00046: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9570 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0959\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.5625\n",
      "Epoch 00047: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.1621 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0927\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8184\n",
      "Epoch 00048: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.2930 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0955\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8958.4678- ETA: 2s - loss: 0.0224 - mae: 0.0217 - mape: 4847. - ETA: 1s - loss: 0.0222 - mae: 0.0215 \n",
      "Epoch 00049: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.8057 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0937\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8977.0430\n",
      "Epoch 00050: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.9639 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0950\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.6865\n",
      "Epoch 00051: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.2705 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0954\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8964.1064\n",
      "Epoch 00052: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.1123 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0919\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.2607\n",
      "Epoch 00053: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.9170 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0989\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8959.4570\n",
      "Epoch 00054: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.2852 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0969\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.4199- ETA: 2s - loss: 0.0225 - mae: 0\n",
      "Epoch 00055: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.2910 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0990\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6221\n",
      "Epoch 00056: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1025 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0925\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.5264\n",
      "Epoch 00057: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8096 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1009\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8963.5762\n",
      "Epoch 00058: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.3652 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1010\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8584.6045\n",
      "Epoch 00059: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9108.2227 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0963\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.8242\n",
      "Epoch 00060: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.3945 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0996\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3730  ETA: 2s - loss: 0.0\n",
      "Epoch 00061: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8975 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1003\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.2998\n",
      "Epoch 00062: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9113.6562 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0987\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8572.1582\n",
      "Epoch 00063: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.7090 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1015\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6914\n",
      "Epoch 00064: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1013\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.9131\n",
      "Epoch 00065: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.5430 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1008\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9361.4287\n",
      "Epoch 00066: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9111.8457 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1032\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8968.2002\n",
      "Epoch 00067: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.6504 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1028\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.6250\n",
      "Epoch 00068: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7305 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8962.6729- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8456.346\n",
      "Epoch 00069: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9107.0010 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1072\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.2842\n",
      "Epoch 00070: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.8740 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1061\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.1514\n",
      "Epoch 00071: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9108.2100 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1067\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9350.1494\n",
      "Epoch 00072: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9100.8525 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.5088\n",
      "Epoch 00073: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9105.0918 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1070\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8561.8984\n",
      "Epoch 00074: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1073\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.5430\n",
      "Epoch 00075: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.3232 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1101\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.6846\n",
      "Epoch 00076: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.0889 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1018\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.6553\n",
      "Epoch 00077: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.3877 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1131\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.4307\n",
      "Epoch 00078: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.1025 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1066\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8972.5918\n",
      "Epoch 00079: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7490 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.0195\n",
      "Epoch 00080: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.7041 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1059\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.1738\n",
      "Epoch 00081: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.8281 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1086\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9359.2744\n",
      "Epoch 00082: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7461 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1125\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9355.9951\n",
      "Epoch 00083: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.5498 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1091\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.0264\n",
      "Epoch 00084: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9111.4297 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8973.9590\n",
      "Epoch 00085: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.7012 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1117\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8980.7080\n",
      "Epoch 00086: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.9180 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1181\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9362.3105\n",
      "Epoch 00087: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.6982 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1146\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.5547\n",
      "Epoch 00088: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1924 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.5771\n",
      "Epoch 00089: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.1777 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1158\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.7559\n",
      "Epoch 00090: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.4727 - val_loss: 0.0304 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9344.5049\n",
      "Epoch 00091: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9095.3486 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1201\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.3213\n",
      "Epoch 00092: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.6084 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1109\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.4619\n",
      "Epoch 00093: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1074 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9357.2207\n",
      "Epoch 00094: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.7432 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1149\n",
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.8730\n",
      "Epoch 00095: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.5840 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9366.0117\n",
      "Epoch 00096: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9116.2979 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1090\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 7807.1338- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape:\n",
      "Epoch 00097: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9097.1768 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8969.1562\n",
      "Epoch 00098: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9108.1855 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1151\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9350.9482  ETA\n",
      "Epoch 00099: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.6289 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1170\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.7568\n",
      "Epoch 00100: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.3867 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1185\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.9570\n",
      "Epoch 00101: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.5547 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.7695- ETA: 1s - loss: 0.0222 - mae: 0.0215 - ma\n",
      "Epoch 00102: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.2959 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1222\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9360.5537\n",
      "Epoch 00103: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.9805 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1187\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8967.6729\n",
      "Epoch 00104: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1299 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1163\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.5879\n",
      "Epoch 00105: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.3262 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1221\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.9297\n",
      "Epoch 00106: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9114.2559 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1136\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8575.9648\n",
      "Epoch 00107: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.5449 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1218\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.1768\n",
      "Epoch 00108: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9098.9189 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1236\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.6055\n",
      "Epoch 00109: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.2510 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1214\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0214 - mape: 9352.8691\n",
      "Epoch 00110: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0221 - mae: 0.0215 - mape: 9103.5098 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1184\n",
      "Epoch 111/200\n",
      "23552/32614 [====================>.........] - ETA: 0s - loss: 0.0220 - mae: 0.0213 - mape: 9332.0186"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-aa18ff9589c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 411.01\n",
      "Median Absolute Error (MedAE): 326.15\n",
      "Mean Squared Error (MSE): 300114.5\n",
      "Root Mean Squared Error (RMSE): 547.83\n",
      "Mean Absolute Percentage Error (MAPE): 4.11 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.17 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-0022-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
