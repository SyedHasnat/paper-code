{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tWP4R-BZmaZi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\pyimagesearch')\n",
    "from timeseires.CNN1D.GoogLeNet1D import PC\n",
    "from timeseires.utils.to_split import to_split\n",
    "#from timeseires.callbacks.EpochCheckpoint import EpochCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeseires.callbacks.TrainingMonitor import TrainingMonitor\n",
    "from timeseires.utils.load_only_multi_step import load_only_multi_step\n",
    "from timeseires.utils.multivariate_multi_step import multivariate_multi_step\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD  \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,MaxPooling1D,Concatenate,AveragePooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "import pandas as pd\n",
    "import time, pickle\n",
    "import pydot\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, Reshape, Dense, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten, LeakyReLU, concatenate\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12130, 21), (24259, 21))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tr = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_train.csv'\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values \n",
    "\n",
    "path_v = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_validation.csv'\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "\n",
    "path_te = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_test.csv'\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "#.......................................................................................................\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_scaler.pkl\", 'rb'))\n",
    "df_te.shape,df_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nmYgpwTPnsf4"
   },
   "outputs": [],
   "source": [
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIYjXnV81ZHC",
    "outputId": "4520a0c1-81f1-4cf6-9b17-d15f5ae9c7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.37502574920654297 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X, train_y = multivariate_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24,ahead=0)\n",
    "validation_X, validation_y = multivariate_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24,ahead=0)\n",
    "test_X, test_y = multivariate_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24,ahead=0)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.2557990550994873 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X_load, train_y_load = load_only_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24)\n",
    "validation_X_load, validation_y_load = load_only_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24)\n",
    "test_X_load, test_y_load = load_only_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PbWIXWwSJN3k"
   },
   "outputs": [],
   "source": [
    "#EXP 1 STEPS 24 LOOKBACK 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_goog():   \n",
    "    inputs_combine = Input(shape=(time_steps, num_features))\n",
    "    lstm1   = LSTM(20, return_sequences = True)(inputs_combine)\n",
    "    lstm2   = LSTM(20, return_sequences = True)(lstm1)\n",
    "    #--------------------------------------------------------\n",
    "    inputs_load = Input(shape=(time_steps, 1))\n",
    "    l = Bidirectional(LSTM(48, return_sequences=True))(inputs_load)\n",
    "    c = PC.inception_module(inputs_load, 32, 32, 64, 8, 32, \"2a\")\n",
    "    c = PC.inception_module(c, 64, 64, 128, 16, 64, \"3a\")\n",
    "    c = PC.inception_module(c, 64, 92, 72, 8, 48, \"4a\")\n",
    "    #---------------------------------------------------------\n",
    "    y = Concatenate(axis=-1)([inputs_load, l, c, lstm2, inputs_combine])\n",
    "    y = PC.inception_module(y, 64, 92, 72, 8, 48, \"4o\")\n",
    "    #y= PC.conv_module(y, 128, 3, 1)\n",
    "    x = Flatten()(y)\n",
    "    x = Dense(128, activation=\"selu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(24, activation=\"linear\")(x)\n",
    "    model = Model(inputs=[inputs_load,inputs_combine], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = review_goog()\n",
    "tensorflow.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_3:0' shape=(None, 24, 1) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input(shape=(time_steps, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9rQrz9B0JWvR"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\1\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\1'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "h6NbczAlOpSQ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NkF7jg_XO346"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-rIEZj0PCPF",
    "outputId": "95edda02-84de-4f8a-83a2-b4d53387049d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = review_goog()\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84859, 24, 21)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-o0qgceiPFdI",
    "outputId": "ebb2c588-e73f-4182-baad-2f4c28f3db95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.2452 - mae: 0.1081 - mape: 9028.1611\n",
      "Epoch 00001: val_loss improved from inf to 0.06210, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0001-loss0.06.h5\n",
      "2652/2652 [==============================] - 67s 25ms/step - loss: 0.2451 - mae: 0.1081 - mape: 9021.8955 - val_loss: 0.0621 - val_mae: 0.0416 - val_mape: 20.4628\n",
      "Epoch 2/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0529 - mae: 0.0430 - mape: 2225.2983\n",
      "Epoch 00002: val_loss improved from 0.06210 to 0.04180, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0002-loss0.04.h5\n",
      "2652/2652 [==============================] - 64s 24ms/step - loss: 0.0529 - mae: 0.0431 - mape: 2223.7627 - val_loss: 0.0418 - val_mae: 0.0383 - val_mape: 17.3342\n",
      "Epoch 3/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0444 - mae: 0.0408 - mape: 2460.7734\n",
      "Epoch 00003: val_loss improved from 0.04180 to 0.03922, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0003-loss0.04.h5\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0444 - mae: 0.0408 - mape: 2459.9941 - val_loss: 0.0392 - val_mae: 0.0371 - val_mape: 15.7520\n",
      "Epoch 4/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0422 - mae: 0.0395 - mape: 2178.4690\n",
      "Epoch 00004: val_loss did not improve from 0.03922\n",
      "2652/2652 [==============================] - 62s 24ms/step - loss: 0.0422 - mae: 0.0395 - mape: 2177.7791 - val_loss: 0.0394 - val_mae: 0.0372 - val_mape: 16.8963\n",
      "Epoch 5/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0407 - mae: 0.0389 - mape: 2259.4995\n",
      "Epoch 00005: val_loss improved from 0.03922 to 0.03795, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0005-loss0.04.h5\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0407 - mae: 0.0389 - mape: 2259.4995 - val_loss: 0.0379 - val_mae: 0.0363 - val_mape: 16.9394\n",
      "Epoch 6/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0400 - mae: 0.0384 - mape: 2338.8152\n",
      "Epoch 00006: val_loss did not improve from 0.03795\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0400 - mae: 0.0384 - mape: 2337.1975 - val_loss: 0.0444 - val_mae: 0.0430 - val_mape: 19.6756\n",
      "Epoch 7/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0403 - mae: 0.0386 - mape: 2182.4663\n",
      "Epoch 00007: val_loss did not improve from 0.03795\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0403 - mae: 0.0386 - mape: 2181.7754 - val_loss: 0.0389 - val_mae: 0.0374 - val_mape: 16.6912\n",
      "Epoch 8/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0391 - mae: 0.0377 - mape: 2217.0764\n",
      "Epoch 00008: val_loss improved from 0.03795 to 0.03710, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0008-loss0.04.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0391 - mae: 0.0377 - mape: 2217.0764 - val_loss: 0.0371 - val_mae: 0.0358 - val_mape: 14.9850\n",
      "Epoch 9/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0391 - mae: 0.0378 - mape: 2375.4348\n",
      "Epoch 00009: val_loss improved from 0.03710 to 0.03567, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0009-loss0.04.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0391 - mae: 0.0378 - mape: 2375.4348 - val_loss: 0.0357 - val_mae: 0.0344 - val_mape: 14.6769\n",
      "Epoch 10/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0384 - mae: 0.0372 - mape: 2250.9019\n",
      "Epoch 00010: val_loss did not improve from 0.03567\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0384 - mae: 0.0372 - mape: 2250.1890 - val_loss: 0.0367 - val_mae: 0.0356 - val_mape: 16.6735\n",
      "Epoch 11/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0384 - mae: 0.0373 - mape: 2383.9309\n",
      "Epoch 00011: val_loss did not improve from 0.03567\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0384 - mae: 0.0373 - mape: 2383.1763 - val_loss: 0.0388 - val_mae: 0.0373 - val_mape: 16.4988\n",
      "Epoch 12/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0383 - mae: 0.0370 - mape: 2299.8801\n",
      "Epoch 00012: val_loss improved from 0.03567 to 0.03565, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0012-loss0.04.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0383 - mae: 0.0370 - mape: 2299.1519 - val_loss: 0.0356 - val_mae: 0.0345 - val_mape: 14.3793\n",
      "Epoch 13/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0380 - mae: 0.0368 - mape: 2272.4771\n",
      "Epoch 00013: val_loss did not improve from 0.03565\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0380 - mae: 0.0368 - mape: 2271.7578 - val_loss: 0.0359 - val_mae: 0.0347 - val_mape: 15.5128\n",
      "Epoch 14/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0375 - mae: 0.0364 - mape: 2125.0037\n",
      "Epoch 00014: val_loss improved from 0.03565 to 0.03532, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0014-loss0.04.h5\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0375 - mae: 0.0364 - mape: 2123.5339 - val_loss: 0.0353 - val_mae: 0.0343 - val_mape: 14.4172\n",
      "Epoch 15/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0382 - mae: 0.0372 - mape: 2293.7898\n",
      "Epoch 00015: val_loss did not improve from 0.03532\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0382 - mae: 0.0372 - mape: 2293.0654 - val_loss: 0.0945 - val_mae: 0.0932 - val_mape: 48.7159\n",
      "Epoch 16/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0376 - mae: 0.0365 - mape: 2219.2214\n",
      "Epoch 00016: val_loss did not improve from 0.03532\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0376 - mae: 0.0365 - mape: 2217.6853 - val_loss: 0.0364 - val_mae: 0.0352 - val_mape: 15.4625\n",
      "Epoch 17/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0371 - mae: 0.0360 - mape: 2345.3096\n",
      "Epoch 00017: val_loss improved from 0.03532 to 0.03498, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0017-loss0.03.h5\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0371 - mae: 0.0360 - mape: 2343.6863 - val_loss: 0.0350 - val_mae: 0.0339 - val_mape: 14.3421\n",
      "Epoch 18/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0371 - mae: 0.0361 - mape: 2180.7014\n",
      "Epoch 00018: val_loss did not improve from 0.03498\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0371 - mae: 0.0361 - mape: 2180.0112 - val_loss: 0.0354 - val_mae: 0.0344 - val_mape: 15.4168\n",
      "Epoch 19/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0359 - mape: 2309.9880\n",
      "Epoch 00019: val_loss did not improve from 0.03498\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0368 - mae: 0.0359 - mape: 2308.3896 - val_loss: 0.0363 - val_mae: 0.0354 - val_mape: 15.1097\n",
      "Epoch 20/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0366 - mae: 0.0357 - mape: 2197.3435\n",
      "Epoch 00020: val_loss improved from 0.03498 to 0.03379, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0020-loss0.03.h5\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0366 - mae: 0.0357 - mape: 2195.8225 - val_loss: 0.0338 - val_mae: 0.0329 - val_mape: 14.1551\n",
      "Epoch 21/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0363 - mae: 0.0355 - mape: 2224.5273- ETA: 4s - loss: 0.0363 - mae: 0.0355 -  - ETA:  - ETA: 0s - loss: 0.0363 - mae: 0.0355 - map\n",
      "Epoch 00021: val_loss did not improve from 0.03379\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0363 - mae: 0.0355 - mape: 2224.5273 - val_loss: 0.0361 - val_mae: 0.0352 - val_mape: 15.1973\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0363 - mae: 0.0354 - mape: 2199.8848\n",
      "Epoch 00022: val_loss did not improve from 0.03379\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0363 - mae: 0.0354 - mape: 2199.1887 - val_loss: 0.0360 - val_mae: 0.0351 - val_mape: 15.6689\n",
      "Epoch 23/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0354 - mape: 2201.3867\n",
      "Epoch 00023: val_loss did not improve from 0.03379\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0362 - mae: 0.0354 - mape: 2199.8645 - val_loss: 0.0344 - val_mae: 0.0335 - val_mape: 13.9075\n",
      "Epoch 24/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0355 - mape: 2184.8818\n",
      "Epoch 00024: val_loss did not improve from 0.03379\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0364 - mae: 0.0355 - mape: 2184.1904 - val_loss: 0.0340 - val_mae: 0.0332 - val_mape: 13.8372\n",
      "Epoch 25/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0360 - mae: 0.0352 - mape: 2095.9180\n",
      "Epoch 00025: val_loss did not improve from 0.03379\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0360 - mae: 0.0352 - mape: 2095.9180 - val_loss: 0.0363 - val_mae: 0.0354 - val_mape: 15.9462\n",
      "Epoch 26/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0351 - mape: 2088.9463\n",
      "Epoch 00026: val_loss did not improve from 0.03379\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0359 - mae: 0.0350 - mape: 2087.5015 - val_loss: 0.0386 - val_mae: 0.0379 - val_mape: 17.4978\n",
      "Epoch 27/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0361 - mae: 0.0352 - mape: 2230.1235\n",
      "Epoch 00027: val_loss did not improve from 0.03379\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0361 - mae: 0.0352 - mape: 2230.1235 - val_loss: 0.0370 - val_mae: 0.0362 - val_mape: 17.9876\n",
      "Epoch 28/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0357 - mae: 0.0349 - mape: 2168.4988\n",
      "Epoch 00028: val_loss did not improve from 0.03379\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0357 - mae: 0.0349 - mape: 2166.9990 - val_loss: 0.0353 - val_mae: 0.0345 - val_mape: 15.4613\n",
      "Epoch 29/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0347 - mape: 2171.2385\n",
      "Epoch 00029: val_loss did not improve from 0.03379\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0356 - mae: 0.0347 - mape: 2169.7363 - val_loss: 0.0364 - val_mae: 0.0356 - val_mape: 14.8833\n",
      "Epoch 30/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0350 - mape: 2167.4529\n",
      "Epoch 00030: val_loss did not improve from 0.03379\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0359 - mae: 0.0350 - mape: 2166.7664 - val_loss: 0.0344 - val_mae: 0.0336 - val_mape: 14.3302\n",
      "Epoch 31/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0345 - mape: 2292.4551\n",
      "Epoch 00031: val_loss improved from 0.03379 to 0.03371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0031-loss0.03.h5\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0353 - mae: 0.0345 - mape: 2290.8689 - val_loss: 0.0337 - val_mae: 0.0329 - val_mape: 14.0452\n",
      "Epoch 32/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0352 - mae: 0.0344 - mape: 2157.7712\n",
      "Epoch 00032: val_loss did not improve from 0.03371\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0352 - mae: 0.0344 - mape: 2157.7712 - val_loss: 0.0359 - val_mae: 0.0352 - val_mape: 15.7668\n",
      "Epoch 33/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0357 - mae: 0.0349 - mape: 2121.4102\n",
      "Epoch 00033: val_loss did not improve from 0.03371\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0357 - mae: 0.0349 - mape: 2119.9434 - val_loss: 0.0353 - val_mae: 0.0346 - val_mape: 15.6029\n",
      "Epoch 34/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0355 - mae: 0.0347 - mape: 2033.0978\n",
      "Epoch 00034: val_loss did not improve from 0.03371\n",
      "2652/2652 [==============================] - 64s 24ms/step - loss: 0.0355 - mae: 0.0347 - mape: 2033.0978 - val_loss: 0.0339 - val_mae: 0.0331 - val_mape: 15.0253\n",
      "Epoch 35/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0352 - mae: 0.0345 - mape: 2133.8213\n",
      "Epoch 00035: val_loss did not improve from 0.03371\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0352 - mae: 0.0345 - mape: 2132.3457 - val_loss: 0.0375 - val_mae: 0.0366 - val_mape: 17.2631\n",
      "Epoch 36/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0349 - mae: 0.0341 - mape: 2105.6719\n",
      "Epoch 00036: val_loss improved from 0.03371 to 0.03353, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0036-loss0.03.h5\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0349 - mae: 0.0341 - mape: 2215.0195 - val_loss: 0.0335 - val_mae: 0.0328 - val_mape: 13.7400\n",
      "Epoch 37/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0347 - mae: 0.0340 - mape: 1994.3242\n",
      "Epoch 00037: val_loss did not improve from 0.03353\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0347 - mae: 0.0340 - mape: 1993.6927 - val_loss: 0.0348 - val_mae: 0.0340 - val_mape: 14.7318\n",
      "Epoch 38/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0347 - mae: 0.0340 - mape: 2072.4692\n",
      "Epoch 00038: val_loss did not improve from 0.03353\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0347 - mae: 0.0340 - mape: 2098.2136 - val_loss: 0.0363 - val_mae: 0.0355 - val_mape: 16.3099\n",
      "Epoch 39/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0345 - mae: 0.0338 - mape: 2262.7690\n",
      "Epoch 00039: val_loss improved from 0.03353 to 0.03252, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0039-loss0.03.h5\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0345 - mae: 0.0338 - mape: 2261.2024 - val_loss: 0.0325 - val_mae: 0.0318 - val_mape: 13.3072\n",
      "Epoch 40/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0346 - mae: 0.0339 - mape: 2124.9304\n",
      "Epoch 00040: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0346 - mae: 0.0339 - mape: 2124.9304 - val_loss: 0.0348 - val_mae: 0.0341 - val_mape: 15.1976\n",
      "Epoch 41/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0345 - mae: 0.0337 - mape: 1946.7124\n",
      "Epoch 00041: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0345 - mae: 0.0337 - mape: 1945.3668 - val_loss: 0.0357 - val_mae: 0.0350 - val_mape: 15.2454\n",
      "Epoch 42/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0344 - mae: 0.0337 - mape: 2014.4956\n",
      "Epoch 00042: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0344 - mae: 0.0337 - mape: 2014.4956 - val_loss: 0.0353 - val_mae: 0.0346 - val_mape: 15.8589\n",
      "Epoch 43/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0342 - mae: 0.0335 - mape: 2077.6458\n",
      "Epoch 00043: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0342 - mae: 0.0335 - mape: 2076.2083 - val_loss: 0.0344 - val_mae: 0.0337 - val_mape: 14.7941\n",
      "Epoch 44/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0341 - mae: 0.0334 - mape: 1821.0502\n",
      "Epoch 00044: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0341 - mae: 0.0334 - mape: 1819.7913 - val_loss: 0.0352 - val_mae: 0.0345 - val_mape: 16.2743\n",
      "Epoch 45/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0343 - mae: 0.0336 - mape: 1855.0439\n",
      "Epoch 00045: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0343 - mae: 0.0336 - mape: 1853.7610 - val_loss: 0.0342 - val_mae: 0.0335 - val_mape: 14.7589\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0341 - mae: 0.0334 - mape: 1978.4005\n",
      "Epoch 00046: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0341 - mae: 0.0334 - mape: 1978.4005 - val_loss: 0.0337 - val_mae: 0.0330 - val_mape: 14.2114\n",
      "Epoch 47/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0340 - mae: 0.0333 - mape: 1901.1636\n",
      "Epoch 00047: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0340 - mae: 0.0333 - mape: 1900.5616 - val_loss: 0.0338 - val_mae: 0.0332 - val_mape: 14.4146\n",
      "Epoch 48/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0339 - mae: 0.0333 - mape: 1870.1544\n",
      "Epoch 00048: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0339 - mae: 0.0333 - mape: 1869.5636 - val_loss: 0.0335 - val_mae: 0.0329 - val_mape: 14.7345\n",
      "Epoch 49/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0343 - mae: 0.0337 - mape: 1927.2888\n",
      "Epoch 00049: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0343 - mae: 0.0337 - mape: 1925.9562 - val_loss: 0.0338 - val_mae: 0.0331 - val_mape: 13.6380\n",
      "Epoch 50/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0338 - mae: 0.0332 - mape: 1773.9580\n",
      "Epoch 00050: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0338 - mae: 0.0332 - mape: 1773.9580 - val_loss: 0.0369 - val_mae: 0.0363 - val_mape: 16.7424\n",
      "Epoch 51/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0340 - mae: 0.0333 - mape: 1843.9121\n",
      "Epoch 00051: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0340 - mae: 0.0333 - mape: 1843.3282 - val_loss: 0.0348 - val_mae: 0.0342 - val_mape: 15.4706\n",
      "Epoch 52/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0337 - mae: 0.0330 - mape: 1807.5991\n",
      "Epoch 00052: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0337 - mae: 0.0330 - mape: 1807.5991 - val_loss: 0.0343 - val_mae: 0.0337 - val_mape: 14.9629\n",
      "Epoch 53/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0350 - mae: 0.0343 - mape: 1925.8740\n",
      "Epoch 00053: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0350 - mae: 0.0343 - mape: 1925.2640 - val_loss: 0.0517 - val_mae: 0.0510 - val_mape: 26.0987\n",
      "Epoch 54/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0340 - mae: 0.0334 - mape: 1912.9570\n",
      "Epoch 00054: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0340 - mae: 0.0334 - mape: 1912.3517 - val_loss: 0.0385 - val_mae: 0.0379 - val_mape: 18.5445\n",
      "Epoch 55/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0338 - mae: 0.0332 - mape: 1894.4038\n",
      "Epoch 00055: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0338 - mae: 0.0332 - mape: 1894.4038 - val_loss: 0.0342 - val_mae: 0.0336 - val_mape: 14.3898\n",
      "Epoch 56/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0339 - mae: 0.0334 - mape: 1902.9136\n",
      "Epoch 00056: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0339 - mae: 0.0334 - mape: 1902.3114 - val_loss: 0.0343 - val_mae: 0.0338 - val_mape: 14.6245\n",
      "Epoch 57/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0335 - mae: 0.0330 - mape: 1652.4073\n",
      "Epoch 00057: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0335 - mae: 0.0330 - mape: 1651.2651 - val_loss: 0.0352 - val_mae: 0.0347 - val_mape: 15.8902\n",
      "Epoch 58/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0335 - mae: 0.0330 - mape: 2005.5109\n",
      "Epoch 00058: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0335 - mae: 0.0330 - mape: 2005.5109 - val_loss: 0.0347 - val_mae: 0.0342 - val_mape: 15.7111\n",
      "Epoch 59/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0334 - mae: 0.0329 - mape: 1932.3574\n",
      "Epoch 00059: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0334 - mae: 0.0329 - mape: 1931.7462 - val_loss: 0.0365 - val_mae: 0.0360 - val_mape: 16.3652\n",
      "Epoch 60/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0333 - mae: 0.0328 - mape: 2040.6298\n",
      "Epoch 00060: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0333 - mae: 0.0328 - mape: 2039.2190 - val_loss: 0.0350 - val_mae: 0.0345 - val_mape: 15.6681\n",
      "Epoch 61/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0334 - mae: 0.0329 - mape: 1990.8466- ETA: 0s - loss: 0.0334 - mae: 0.0329 - mape: \n",
      "Epoch 00061: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0334 - mae: 0.0329 - mape: 1989.4702 - val_loss: 0.0339 - val_mae: 0.0334 - val_mape: 14.4315\n",
      "Epoch 62/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0333 - mae: 0.0328 - mape: 1837.7147\n",
      "Epoch 00062: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0333 - mae: 0.0328 - mape: 1836.4434 - val_loss: 0.0336 - val_mae: 0.0331 - val_mape: 13.3540\n",
      "Epoch 63/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0344 - mae: 0.0339 - mape: 1988.0712\n",
      "Epoch 00063: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0344 - mae: 0.0339 - mape: 1986.6960 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 13.8933\n",
      "Epoch 64/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0335 - mae: 0.0331 - mape: 2036.4929\n",
      "Epoch 00064: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0335 - mae: 0.0331 - mape: 2036.4929 - val_loss: 0.0343 - val_mae: 0.0340 - val_mape: 15.5951\n",
      "Epoch 65/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0333 - mae: 0.0330 - mape: 1813.5228\n",
      "Epoch 00065: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0333 - mae: 0.0330 - mape: 1812.9489 - val_loss: 0.0340 - val_mae: 0.0337 - val_mape: 14.6235\n",
      "Epoch 66/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0333 - mae: 0.0329 - mape: 1966.1401\n",
      "Epoch 00066: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0333 - mae: 0.0329 - mape: 1964.7808 - val_loss: 0.0332 - val_mae: 0.0328 - val_mape: 13.4966\n",
      "Epoch 67/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0332 - mae: 0.0329 - mape: 2029.9615\n",
      "Epoch 00067: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0332 - mae: 0.0329 - mape: 2029.3186 - val_loss: 0.0348 - val_mae: 0.0345 - val_mape: 15.8780\n",
      "Epoch 68/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0331 - mae: 0.0327 - mape: 1718.6437\n",
      "Epoch 00068: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0331 - mae: 0.0327 - mape: 1718.6437 - val_loss: 0.0334 - val_mae: 0.0331 - val_mape: 13.9807\n",
      "Epoch 69/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0330 - mae: 0.0327 - mape: 1873.3156\n",
      "Epoch 00069: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0330 - mae: 0.0327 - mape: 1872.7229 - val_loss: 0.0355 - val_mae: 0.0352 - val_mape: 16.0984\n",
      "Epoch 70/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0330 - mae: 0.0327 - mape: 1850.1714\n",
      "Epoch 00070: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0330 - mae: 0.0327 - mape: 1849.5862 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 13.9348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0329 - mae: 0.0326 - mape: 1894.7063\n",
      "Epoch 00071: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0329 - mae: 0.0326 - mape: 1894.1069 - val_loss: 0.0343 - val_mae: 0.0340 - val_mape: 15.9578\n",
      "Epoch 72/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0328 - mae: 0.0325 - mape: 1876.4985\n",
      "Epoch 00072: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0328 - mae: 0.0325 - mape: 1875.9047 - val_loss: 0.0344 - val_mae: 0.0340 - val_mape: 16.4198\n",
      "Epoch 73/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0328 - mae: 0.0324 - mape: 1975.8430\n",
      "Epoch 00073: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0328 - mae: 0.0324 - mape: 1975.8430 - val_loss: 0.0343 - val_mae: 0.0340 - val_mape: 14.9122\n",
      "Epoch 74/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0327 - mae: 0.0324 - mape: 1839.1985\n",
      "Epoch 00074: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0327 - mae: 0.0324 - mape: 1837.9261 - val_loss: 0.0334 - val_mae: 0.0331 - val_mape: 14.0931\n",
      "Epoch 75/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0323 - mape: 1817.8824\n",
      "Epoch 00075: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0326 - mae: 0.0323 - mape: 1816.6265 - val_loss: 0.0353 - val_mae: 0.0350 - val_mape: 17.0005\n",
      "Epoch 76/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0326 - mae: 0.0323 - mape: 1957.4822\n",
      "Epoch 00076: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0326 - mae: 0.0323 - mape: 1957.4822 - val_loss: 0.0364 - val_mae: 0.0360 - val_mape: 17.3415\n",
      "Epoch 77/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0322 - mape: 1737.0222\n",
      "Epoch 00077: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0326 - mae: 0.0322 - mape: 1853.9258 - val_loss: 0.0360 - val_mae: 0.0357 - val_mape: 17.3632\n",
      "Epoch 78/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0325 - mae: 0.0322 - mape: 1757.6398\n",
      "Epoch 00078: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0325 - mae: 0.0322 - mape: 1757.0835 - val_loss: 0.0332 - val_mae: 0.0329 - val_mape: 13.9589\n",
      "Epoch 79/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0324 - mae: 0.0321 - mape: 1924.8325\n",
      "Epoch 00079: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0324 - mae: 0.0321 - mape: 1924.2236 - val_loss: 0.0379 - val_mae: 0.0375 - val_mape: 19.1476\n",
      "Epoch 80/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0324 - mae: 0.0321 - mape: 1732.3042\n",
      "Epoch 00080: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0324 - mae: 0.0321 - mape: 1731.1063 - val_loss: 0.0329 - val_mae: 0.0326 - val_mape: 13.5321\n",
      "Epoch 81/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0323 - mae: 0.0320 - mape: 1801.5730\n",
      "Epoch 00081: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0323 - mae: 0.0320 - mape: 1801.5730 - val_loss: 0.0333 - val_mae: 0.0330 - val_mape: 13.6103\n",
      "Epoch 82/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0323 - mae: 0.0320 - mape: 1736.0481\n",
      "Epoch 00082: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0323 - mae: 0.0320 - mape: 1735.4985 - val_loss: 0.0340 - val_mae: 0.0337 - val_mape: 15.0097\n",
      "Epoch 83/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0323 - mae: 0.0319 - mape: 1761.9017\n",
      "Epoch 00083: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0323 - mae: 0.0319 - mape: 1760.6833 - val_loss: 0.0332 - val_mae: 0.0329 - val_mape: 14.7626\n",
      "Epoch 84/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0322 - mae: 0.0319 - mape: 1932.1099\n",
      "Epoch 00084: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0322 - mae: 0.0319 - mape: 1932.1099 - val_loss: 0.0340 - val_mae: 0.0337 - val_mape: 14.3352\n",
      "Epoch 85/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0322 - mae: 0.0318 - mape: 1715.5781\n",
      "Epoch 00085: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0322 - mae: 0.0318 - mape: 1715.0392 - val_loss: 0.0334 - val_mae: 0.0331 - val_mape: 14.8487\n",
      "Epoch 86/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0321 - mae: 0.0317 - mape: 1774.6715\n",
      "Epoch 00086: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0321 - mae: 0.0317 - mape: 1773.4453 - val_loss: 0.0329 - val_mae: 0.0326 - val_mape: 13.7231\n",
      "Epoch 87/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0321 - mae: 0.0318 - mape: 1757.5398\n",
      "Epoch 00087: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0321 - mae: 0.0318 - mape: 1756.9845 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 14.6555\n",
      "Epoch 88/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0321 - mae: 0.0318 - mape: 1744.5068\n",
      "Epoch 00088: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0321 - mae: 0.0318 - mape: 1743.3005 - val_loss: 0.0349 - val_mae: 0.0345 - val_mape: 14.3752\n",
      "Epoch 89/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0320 - mae: 0.0317 - mape: 1755.4526\n",
      "Epoch 00089: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0320 - mae: 0.0317 - mape: 1754.8971 - val_loss: 0.0332 - val_mae: 0.0329 - val_mape: 13.7359\n",
      "Epoch 90/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0319 - mae: 0.0316 - mape: 1650.1378\n",
      "Epoch 00090: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0319 - mae: 0.0316 - mape: 1649.6165 - val_loss: 0.0349 - val_mae: 0.0346 - val_mape: 15.0671\n",
      "Epoch 91/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0319 - mae: 0.0315 - mape: 1634.1571\n",
      "Epoch 00091: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0319 - mae: 0.0315 - mape: 1633.6404 - val_loss: 0.0370 - val_mae: 0.0366 - val_mape: 18.3108\n",
      "Epoch 92/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0319 - mae: 0.0316 - mape: 1728.6049\n",
      "Epoch 00092: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0319 - mae: 0.0316 - mape: 1728.6049 - val_loss: 0.0369 - val_mae: 0.0366 - val_mape: 17.8069\n",
      "Epoch 93/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0319 - mae: 0.0316 - mape: 1558.2859\n",
      "Epoch 00093: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0319 - mae: 0.0316 - mape: 1557.2085 - val_loss: 0.0348 - val_mae: 0.0345 - val_mape: 15.2404\n",
      "Epoch 94/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0318 - mae: 0.0315 - mape: 1726.8746\n",
      "Epoch 00094: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0318 - mae: 0.0315 - mape: 1726.3278 - val_loss: 0.0332 - val_mae: 0.0329 - val_mape: 14.0920\n",
      "Epoch 95/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0317 - mae: 0.0314 - mape: 1564.5992- ETA: 1s - loss: 0.0317 - mae: 0.031\n",
      "Epoch 00095: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0317 - mae: 0.0314 - mape: 1564.5992 - val_loss: 0.0329 - val_mae: 0.0326 - val_mape: 13.4345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0317 - mae: 0.0314 - mape: 1537.8278\n",
      "Epoch 00096: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0317 - mae: 0.0314 - mape: 1536.7650 - val_loss: 0.0337 - val_mae: 0.0334 - val_mape: 14.2337\n",
      "Epoch 97/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0317 - mae: 0.0314 - mape: 1539.3015\n",
      "Epoch 00097: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0317 - mae: 0.0314 - mape: 1538.8151 - val_loss: 0.0337 - val_mae: 0.0334 - val_mape: 14.5710\n",
      "Epoch 98/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0316 - mae: 0.0313 - mape: 1439.6587\n",
      "Epoch 00098: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0316 - mae: 0.0313 - mape: 1438.6650 - val_loss: 0.0362 - val_mae: 0.0359 - val_mape: 18.2853\n",
      "Epoch 99/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0316 - mae: 0.0313 - mape: 1499.3873\n",
      "Epoch 00099: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0316 - mae: 0.0313 - mape: 1499.3873 - val_loss: 0.0344 - val_mae: 0.0341 - val_mape: 15.6683\n",
      "Epoch 100/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0316 - mae: 0.0313 - mape: 1466.7148\n",
      "Epoch 00100: val_loss did not improve from 0.03252\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0316 - mae: 0.0313 - mape: 1466.7148 - val_loss: 0.0360 - val_mae: 0.0357 - val_mape: 15.8255\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y_load),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oS6zzoUoPL-A",
    "outputId": "ce1a0f3b-6b76-4980-d717-d3542c18a37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 505.54\n",
      "Median Absolute Error (MedAE): 364.19\n",
      "Mean Squared Error (MSE): 486154.93\n",
      "Root Mean Squared Error (RMSE): 697.25\n",
      "Mean Absolute Percentage Error (MAPE): 3.38 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.54 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\1\\\\E1-cp-0039-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7y39sVKwLMm"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "AVN1WhyzuWFl"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\1\\\\E2-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\1\\\\E1-cp-0039-loss0.03.h5'\n",
    "start_epoch= 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WBeDTA9lu5Qm"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2j6fmkOvA2O",
    "outputId": "7fc1b23b-4d93-4b54-a4df-921c557b9a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E1-cp-0039-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rTCJW6IzvLU4",
    "outputId": "f810e1ca-98cc-4de7-a4a3-465e83705c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0318 - mean_absolute_percentage_error: 1961.4301\n",
      "Epoch 00001: val_loss improved from inf to 0.03258, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E2-cp-0001-loss0.03.h5\n",
      "2652/2652 [==============================] - 68s 26ms/step - loss: 0.0325 - mean_absolute_error: 0.0318 - mean_absolute_percentage_error: 1960.0726 - val_loss: 0.0326 - val_mean_absolute_error: 0.0320 - val_mean_absolute_percentage_error: 13.5374\n",
      "Epoch 2/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0323 - mean_absolute_error: 0.0317 - mean_absolute_percentage_error: 1944.7821- ETA: 3s - loss: 0.0323 - mean_absolute_error: 0.0317 - mean_a - ETA: 1s - loss: 0.0323 - mean_absolute_error: 0.0317 - mean_absolute_percenta\n",
      "Epoch 00002: val_loss improved from 0.03258 to 0.03212, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E2-cp-0002-loss0.03.h5\n",
      "2652/2652 [==============================] - 63s 24ms/step - loss: 0.0323 - mean_absolute_error: 0.0317 - mean_absolute_percentage_error: 1944.1660 - val_loss: 0.0321 - val_mean_absolute_error: 0.0316 - val_mean_absolute_percentage_error: 13.4861\n",
      "Epoch 3/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0323 - mean_absolute_error: 0.0318 - mean_absolute_percentage_error: 1790.9147\n",
      "Epoch 00003: val_loss did not improve from 0.03212\n",
      "2652/2652 [==============================] - 65s 25ms/step - loss: 0.0323 - mean_absolute_error: 0.0318 - mean_absolute_percentage_error: 1789.6761 - val_loss: 0.0330 - val_mean_absolute_error: 0.0325 - val_mean_absolute_percentage_error: 13.8916\n",
      "Epoch 4/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0315 - mean_absolute_percentage_error: 1980.7269\n",
      "Epoch 00004: val_loss improved from 0.03212 to 0.03202, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E2-cp-0004-loss0.03.h5\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0320 - mean_absolute_error: 0.0315 - mean_absolute_percentage_error: 1980.0995 - val_loss: 0.0320 - val_mean_absolute_error: 0.0315 - val_mean_absolute_percentage_error: 13.3417\n",
      "Epoch 5/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0315 - mean_absolute_percentage_error: 1996.4910\n",
      "Epoch 00005: val_loss did not improve from 0.03202\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0320 - mean_absolute_error: 0.0315 - mean_absolute_percentage_error: 1996.4910 - val_loss: 0.0324 - val_mean_absolute_error: 0.0320 - val_mean_absolute_percentage_error: 13.7224\n",
      "Epoch 6/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0323 - mean_absolute_error: 0.0319 - mean_absolute_percentage_error: 1955.1659\n",
      "Epoch 00006: val_loss did not improve from 0.03202\n",
      "2652/2652 [==============================] - 63s 24ms/step - loss: 0.0323 - mean_absolute_error: 0.0319 - mean_absolute_percentage_error: 1955.1659 - val_loss: 0.0328 - val_mean_absolute_error: 0.0323 - val_mean_absolute_percentage_error: 14.1452\n",
      "Epoch 7/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.0316 - mean_absolute_percentage_error: 1818.6163\n",
      "Epoch 00007: val_loss did not improve from 0.03202\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0321 - mean_absolute_error: 0.0316 - mean_absolute_percentage_error: 1818.6163 - val_loss: 0.0324 - val_mean_absolute_error: 0.0320 - val_mean_absolute_percentage_error: 13.7306\n",
      "Epoch 8/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0315 - mean_absolute_percentage_error: 1854.9263\n",
      "Epoch 00008: val_loss did not improve from 0.03202\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0319 - mean_absolute_error: 0.0315 - mean_absolute_percentage_error: 1854.9263 - val_loss: 0.0321 - val_mean_absolute_error: 0.0317 - val_mean_absolute_percentage_error: 13.5565\n",
      "Epoch 9/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0315 - mean_absolute_percentage_error: 1926.642 - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0315 - mean_absolute_percentage_error: 1924.5868\n",
      "Epoch 00009: val_loss did not improve from 0.03202\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0320 - mean_absolute_error: 0.0315 - mean_absolute_percentage_error: 1924.5868 - val_loss: 0.0322 - val_mean_absolute_error: 0.0318 - val_mean_absolute_percentage_error: 13.9110\n",
      "Epoch 10/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0314 - mean_absolute_percentage_error: 1990.4785- ETA: 2s - loss: 0.0318 - mean_absolute_error: 0.\n",
      "Epoch 00010: val_loss improved from 0.03202 to 0.03160, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\1\\E2-cp-0010-loss0.03.h5\n",
      "2652/2652 [==============================] - 64s 24ms/step - loss: 0.0318 - mean_absolute_error: 0.0314 - mean_absolute_percentage_error: 1989.1006 - val_loss: 0.0316 - val_mean_absolute_error: 0.0312 - val_mean_absolute_percentage_error: 13.2918\n",
      "Epoch 11/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0313 - mean_absolute_percentage_error: 1878.0000\n",
      "Epoch 00011: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0317 - mean_absolute_error: 0.0313 - mean_absolute_percentage_error: 1877.4053 - val_loss: 0.0325 - val_mean_absolute_error: 0.0321 - val_mean_absolute_percentage_error: 13.9500\n",
      "Epoch 12/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 1942.9498\n",
      "Epoch 00012: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0316 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 1942.9498 - val_loss: 0.0320 - val_mean_absolute_error: 0.0317 - val_mean_absolute_percentage_error: 13.2880\n",
      "Epoch 13/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0313 - mean_absolute_percentage_error: 1909.3093\n",
      "Epoch 00013: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0316 - mean_absolute_error: 0.0313 - mean_absolute_percentage_error: 1909.3093 - val_loss: 0.0319 - val_mean_absolute_error: 0.0315 - val_mean_absolute_percentage_error: 13.1213\n",
      "Epoch 14/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0316 - mean_absolute_percentage_error: 1923.1332- ETA: 2s - loss: 0.0320 - mean_absolute_error: 0.0316 \n",
      "Epoch 00014: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 64s 24ms/step - loss: 0.0319 - mean_absolute_error: 0.0316 - mean_absolute_percentage_error: 1921.8038 - val_loss: 0.0323 - val_mean_absolute_error: 0.0320 - val_mean_absolute_percentage_error: 13.8868\n",
      "Epoch 15/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0314 - mean_absolute_percentage_error: 1934.4530\n",
      "Epoch 00015: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0318 - mean_absolute_error: 0.0314 - mean_absolute_percentage_error: 1934.4530 - val_loss: 0.0321 - val_mean_absolute_error: 0.0317 - val_mean_absolute_percentage_error: 13.5547\n",
      "Epoch 16/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 1946.0688\n",
      "Epoch 00016: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 63s 24ms/step - loss: 0.0315 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 1944.7224 - val_loss: 0.0321 - val_mean_absolute_error: 0.0317 - val_mean_absolute_percentage_error: 13.2908\n",
      "Epoch 17/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0314 - mean_absolute_percentage_error: 1835.2778\n",
      "Epoch 00017: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0317 - mean_absolute_error: 0.0314 - mean_absolute_percentage_error: 1834.6965 - val_loss: 0.0322 - val_mean_absolute_error: 0.0319 - val_mean_absolute_percentage_error: 13.6646\n",
      "Epoch 18/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0313 - mean_absolute_percentage_error: 1826.1394\n",
      "Epoch 00018: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0317 - mean_absolute_error: 0.0313 - mean_absolute_percentage_error: 1826.1394 - val_loss: 0.0317 - val_mean_absolute_error: 0.0314 - val_mean_absolute_percentage_error: 13.1360\n",
      "Epoch 19/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1892.3973\n",
      "Epoch 00019: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0315 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1891.0880 - val_loss: 0.0320 - val_mean_absolute_error: 0.0317 - val_mean_absolute_percentage_error: 13.6383\n",
      "Epoch 20/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1932.3990\n",
      "Epoch 00020: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0315 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1931.0623 - val_loss: 0.0321 - val_mean_absolute_error: 0.0317 - val_mean_absolute_percentage_error: 13.5863\n",
      "Epoch 21/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 1904.2407\n",
      "Epoch 00021: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0315 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 1902.9243 - val_loss: 0.0320 - val_mean_absolute_error: 0.0317 - val_mean_absolute_percentage_error: 13.7173\n",
      "Epoch 22/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1775.8527\n",
      "Epoch 00022: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0315 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1775.2910 - val_loss: 0.0318 - val_mean_absolute_error: 0.0315 - val_mean_absolute_percentage_error: 13.3309\n",
      "Epoch 23/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1829.7349\n",
      "Epoch 00023: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0314 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1828.4690 - val_loss: 0.0316 - val_mean_absolute_error: 0.0313 - val_mean_absolute_percentage_error: 13.1674\n",
      "Epoch 24/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1942.1045\n",
      "Epoch 00024: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0314 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1941.4897 - val_loss: 0.0316 - val_mean_absolute_error: 0.0313 - val_mean_absolute_percentage_error: 13.2993\n",
      "Epoch 25/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1889.3702\n",
      "Epoch 00025: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 63s 24ms/step - loss: 0.0314 - mean_absolute_error: 0.0311 - mean_absolute_percentage_error: 1889.3702 - val_loss: 0.0319 - val_mean_absolute_error: 0.0316 - val_mean_absolute_percentage_error: 13.7085\n",
      "Epoch 26/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0316 - mean_absolute_percentage_error: 1938.0973\n",
      "Epoch 00026: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 62s 24ms/step - loss: 0.0319 - mean_absolute_error: 0.0316 - mean_absolute_percentage_error: 1937.4839 - val_loss: 0.0322 - val_mean_absolute_error: 0.0319 - val_mean_absolute_percentage_error: 13.6817\n",
      "Epoch 27/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 1951.5792\n",
      "Epoch 00027: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0315 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 1950.9607 - val_loss: 0.0320 - val_mean_absolute_error: 0.0317 - val_mean_absolute_percentage_error: 13.5230\n",
      "Epoch 28/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0313 - mean_absolute_error: 0.0310 - mean_absolute_percentage_error: 1841.2952\n",
      "Epoch 00028: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0313 - mean_absolute_error: 0.0310 - mean_absolute_percentage_error: 1841.2952 - val_loss: 0.0327 - val_mean_absolute_error: 0.0323 - val_mean_absolute_percentage_error: 14.0490\n",
      "Epoch 29/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 1790.9200\n",
      "Epoch 00029: val_loss did not improve from 0.03160\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0315 - mean_absolute_error: 0.0312 - mean_absolute_percentage_error: 1790.3536 - val_loss: 0.0318 - val_mean_absolute_error: 0.0315 - val_mean_absolute_percentage_error: 13.3903\n",
      "Epoch 30/60\n",
      " 452/2652 [====>.........................] - ETA: 51s - loss: 0.0311 - mean_absolute_error: 0.0308 - mean_absolute_percentage_error: 1535.0608"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2752\\4001373639.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalidation_X_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_X\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y_load\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y_load),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qj8HRsBXvf0M",
    "outputId": "084e6fbf-b9cc-46e8-e4d7-808e9031da87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 495.96\n",
      "Median Absolute Error (MedAE): 347.2\n",
      "Mean Squared Error (MSE): 479322.68\n",
      "Root Mean Squared Error (RMSE): 692.33\n",
      "Mean Absolute Percentage Error (MAPE): 3.31 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.42 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\1\\\\E2-cp-0010-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAuKvjoOG-fA"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QZI2LOvEw6Qq"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E2-cp-0006-loss0.04.h5'\n",
    "start_epoch= 59+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qPy-9EGRw6eR"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beh0i6Vww6pP",
    "outputId": "41024e1d-abfa-4ec6-a3ed-cf5e5fdef9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E2-cp-0006-loss0.04.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y-9y1xTeYAaB",
    "outputId": "3e59db7d-d1fa-41c7-a91e-cf2063e855da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0231 - mape: 8601.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03575, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0001-loss0.04.h5\n",
      "32614/32614 [==============================] - 7s 210us/sample - loss: 0.0235 - mae: 0.0231 - mape: 8729.3066 - val_loss: 0.0358 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0230 - mape: 8563.2227\n",
      "Epoch 00002: val_loss improved from 0.03575 to 0.03574, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0002-loss0.04.h5\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0235 - mae: 0.0230 - mape: 8710.7080 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1938\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8942.2402\n",
      "Epoch 00003: val_loss improved from 0.03574 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0003-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 115us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8703.8408 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8589.9561\n",
      "Epoch 00004: val_loss improved from 0.03573 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 114us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8711.1074 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1949\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8953.2998\n",
      "Epoch 00005: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8714.5996 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8595.0498\n",
      "Epoch 00006: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8716.8604 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8175.1738\n",
      "Epoch 00007: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8717.2090 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8577.0596\n",
      "Epoch 00008: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8716.0957 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1954\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2109\n",
      "Epoch 00009: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.7314 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2744\n",
      "Epoch 00010: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.3086 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.7139\n",
      "Epoch 00011: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.5303 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.2705\n",
      "Epoch 00012: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8717.8525 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1962\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2910\n",
      "Epoch 00013: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.2773 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8968.6553\n",
      "Epoch 00014: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.5400 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1970\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8580.6602\n",
      "Epoch 00015: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8719.8223 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1967\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8623\n",
      "Epoch 00016: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.7832 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1963\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8877\n",
      "Epoch 00017: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.8203 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8606.8154\n",
      "Epoch 00018: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0547 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2119\n",
      "Epoch 00019: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.2393 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8973.6885\n",
      "Epoch 00020: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8734.4512 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8217.7236  ETA: 2s - loss: 0.0233 -\n",
      "Epoch 00021: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8730.0449 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8967.8350\n",
      "Epoch 00022: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8728.7363 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8965.8945\n",
      "Epoch 00023: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.8516 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1969\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8607.8672\n",
      "Epoch 00024: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0010 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1964\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8588.1279\n",
      "Epoch 00025: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8735.5576 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8230.3066\n",
      "Epoch 00026: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8738.8379 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 27/200\n",
      "13312/32614 [===========>..................] - ETA: 1s - loss: 0.0233 - mae: 0.0229 - mape: 7589.0488"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-05065ff65640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y, \n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9fPE-jZuQ0r",
    "outputId": "aca09565-e9df-4067-d473-32dd8826a738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 541.71\n",
      "Median Absolute Error (MedAE): 449.47\n",
      "Mean Squared Error (MSE): 473684.02\n",
      "Root Mean Squared Error (RMSE): 688.25\n",
      "Mean Absolute Percentage Error (MAPE): 5.4 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.39 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-0004-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7x0O-u0GbLy"
   },
   "source": [
    "# E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rdBroUOqIeW6"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(200, activation='relu')(x)\n",
    " #       x = Dense(10, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AQaM-IuPIXqN"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QwUKEvSfIW09"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NHgO7mJ4IQVp"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TIsLEDoIN0C",
    "outputId": "3ef77929-b334-4953-faab-b938f376c306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpLkyqBmIKIw",
    "outputId": "3d701f4a-270d-4efc-b52a-cd26d12d4dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.2038 - mae: 0.0831 - mape: 10284.1670\n",
      "Epoch 00001: val_loss improved from inf to 0.10131, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0001-loss0.10.h5\n",
      "32614/32614 [==============================] - 19s 581us/sample - loss: 0.2036 - mae: 0.0830 - mape: 10262.1201 - val_loss: 0.1013 - val_mae: 0.0675 - val_mape: 11.9466\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0654 - mae: 0.0492 - mape: 9617.4668\n",
      "Epoch 00002: val_loss improved from 0.10131 to 0.07428, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0002-loss0.07.h5\n",
      "32614/32614 [==============================] - 14s 420us/sample - loss: 0.0654 - mae: 0.0492 - mape: 9596.8438 - val_loss: 0.0743 - val_mae: 0.0671 - val_mape: 11.8373\n",
      "Epoch 3/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0511 - mae: 0.0461 - mape: 9812.7764\n",
      "Epoch 00003: val_loss improved from 0.07428 to 0.06341, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0003-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0510 - mae: 0.0460 - mape: 9762.8730 - val_loss: 0.0634 - val_mae: 0.0599 - val_mape: 10.4427\n",
      "Epoch 4/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0450 - mae: 0.0419 - mape: 9616.9199\n",
      "Epoch 00004: val_loss did not improve from 0.06341\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0450 - mae: 0.0419 - mape: 9596.3027 - val_loss: 0.0706 - val_mae: 0.0680 - val_mape: 11.4199\n",
      "Epoch 5/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0441 - mae: 0.0414 - mape: 9613.5029\n",
      "Epoch 00005: val_loss improved from 0.06341 to 0.05454, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0005-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0441 - mae: 0.0414 - mape: 9611.7354 - val_loss: 0.0545 - val_mae: 0.0519 - val_mape: 9.2312\n",
      "Epoch 6/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0418 - mae: 0.0397 - mape: 9602.1924\n",
      "Epoch 00006: val_loss did not improve from 0.05454\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0418 - mae: 0.0397 - mape: 9572.1885 - val_loss: 0.0587 - val_mae: 0.0570 - val_mape: 9.8649\n",
      "Epoch 7/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0406 - mae: 0.0389 - mape: 9195.3799\n",
      "Epoch 00007: val_loss improved from 0.05454 to 0.04682, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0007-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0406 - mae: 0.0389 - mape: 9554.0049 - val_loss: 0.0468 - val_mae: 0.0450 - val_mape: 8.7847\n",
      "Epoch 8/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0399 - mae: 0.0384 - mape: 9416.9941\n",
      "Epoch 00008: val_loss did not improve from 0.04682\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0399 - mae: 0.0384 - mape: 9396.7998 - val_loss: 0.0475 - val_mae: 0.0462 - val_mape: 8.9737\n",
      "Epoch 9/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0397 - mae: 0.0383 - mape: 9665.3730\n",
      "Epoch 00009: val_loss improved from 0.04682 to 0.04522, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0009-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0397 - mae: 0.0383 - mape: 9644.6445 - val_loss: 0.0452 - val_mae: 0.0439 - val_mape: 8.5116\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0377 - mape: 9573.1260\n",
      "Epoch 00010: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0389 - mae: 0.0377 - mape: 9533.8271 - val_loss: 0.0940 - val_mae: 0.0928 - val_mape: 15.8112\n",
      "Epoch 11/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0390 - mae: 0.0378 - mape: 9537.1748\n",
      "Epoch 00011: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0390 - mae: 0.0378 - mape: 9535.4209 - val_loss: 0.0501 - val_mae: 0.0490 - val_mape: 8.8788\n",
      "Epoch 12/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0381 - mae: 0.0371 - mape: 9512.4170\n",
      "Epoch 00012: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0381 - mae: 0.0371 - mape: 9492.0146 - val_loss: 0.0511 - val_mae: 0.0502 - val_mape: 9.2328\n",
      "Epoch 13/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0377 - mae: 0.0368 - mape: 9480.7461\n",
      "Epoch 00013: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0377 - mae: 0.0368 - mape: 9479.0029 - val_loss: 0.0494 - val_mae: 0.0485 - val_mape: 9.0546\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.0370 - mape: 9640.1309\n",
      "Epoch 00014: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0379 - mae: 0.0370 - mape: 9600.5547 - val_loss: 0.0475 - val_mae: 0.0466 - val_mape: 8.6730\n",
      "Epoch 15/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0375 - mae: 0.0367 - mape: 9567.5752\n",
      "Epoch 00015: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0375 - mae: 0.0367 - mape: 9565.8154 - val_loss: 0.0497 - val_mae: 0.0489 - val_mape: 9.3880\n",
      "Epoch 16/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0374 - mae: 0.0366 - mape: 9551.2051\n",
      "Epoch 00016: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0374 - mae: 0.0366 - mape: 9540.0859 - val_loss: 0.0464 - val_mae: 0.0456 - val_mape: 8.5940\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0362 - mape: 9563.9082\n",
      "Epoch 00017: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0369 - mae: 0.0362 - mape: 9562.1514 - val_loss: 0.0475 - val_mae: 0.0468 - val_mape: 8.6608\n",
      "Epoch 18/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0361 - mape: 9577.6992\n",
      "Epoch 00018: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0368 - mae: 0.0361 - mape: 9566.5488 - val_loss: 0.0627 - val_mae: 0.0620 - val_mape: 10.5263\n",
      "Epoch 19/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0366 - mae: 0.0359 - mape: 8800.0615\n",
      "Epoch 00019: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0366 - mae: 0.0359 - mape: 9530.1943 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 8.7106\n",
      "Epoch 20/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9487.9727\n",
      "Epoch 00020: val_loss improved from 0.04522 to 0.04432, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0020-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9476.9248 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.3836\n",
      "Epoch 21/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9175.1748\n",
      "Epoch 00021: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9548.0811 - val_loss: 0.0444 - val_mae: 0.0438 - val_mape: 8.4635\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0356 - mape: 9438.7158\n",
      "Epoch 00022: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0362 - mae: 0.0356 - mape: 9427.7275 - val_loss: 0.0463 - val_mae: 0.0456 - val_mape: 8.8022\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0363 - mae: 0.0356 - mape: 9494.2676\n",
      "Epoch 00023: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0363 - mae: 0.0356 - mape: 9492.5234 - val_loss: 0.0502 - val_mae: 0.0496 - val_mape: 8.8956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0360 - mae: 0.0354 - mape: 9481.0664\n",
      "Epoch 00024: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0360 - mae: 0.0354 - mape: 9479.3242 - val_loss: 0.0482 - val_mae: 0.0476 - val_mape: 8.8535\n",
      "Epoch 25/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9452.0479\n",
      "Epoch 00025: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9422.5107 - val_loss: 0.0450 - val_mae: 0.0444 - val_mape: 8.3675\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9389.7109\n",
      "Epoch 00026: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9360.3701 - val_loss: 0.0505 - val_mae: 0.0499 - val_mape: 8.9818\n",
      "Epoch 27/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9449.0146\n",
      "Epoch 00027: val_loss improved from 0.04432 to 0.04404, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0027-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9419.4844 - val_loss: 0.0440 - val_mae: 0.0434 - val_mape: 8.4639\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9357.8389\n",
      "Epoch 00028: val_loss improved from 0.04404 to 0.04385, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0028-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9346.9443 - val_loss: 0.0438 - val_mae: 0.0433 - val_mape: 8.3336\n",
      "Epoch 29/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0350 - mape: 9348.0967- ETA: 2s - loss: 0.\n",
      "Epoch 00029: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0356 - mae: 0.0350 - mape: 9337.2129 - val_loss: 0.0453 - val_mae: 0.0447 - val_mape: 8.8223\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0355 - mae: 0.0349 - mape: 9328.2656\n",
      "Epoch 00030: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0355 - mae: 0.0349 - mape: 9299.1152 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.4327\n",
      "Epoch 31/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0348 - mape: 9367.9561\n",
      "Epoch 00031: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0354 - mae: 0.0348 - mape: 9329.5146 - val_loss: 0.0556 - val_mae: 0.0550 - val_mape: 9.5869\n",
      "Epoch 32/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0346 - mae: 0.0341 - mape: 9331.7041\n",
      "Epoch 00032: val_loss improved from 0.04385 to 0.03952, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0032-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0346 - mae: 0.0341 - mape: 9293.3955 - val_loss: 0.0395 - val_mae: 0.0389 - val_mape: 7.3175\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0313 - mae: 0.0307 - mape: 8705.7988\n",
      "Epoch 00033: val_loss did not improve from 0.03952\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0313 - mae: 0.0307 - mape: 9014.9424 - val_loss: 0.0402 - val_mae: 0.0396 - val_mape: 6.9964\n",
      "Epoch 34/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0310 - mae: 0.0304 - mape: 8956.5234\n",
      "Epoch 00034: val_loss improved from 0.03952 to 0.03736, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0034-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 427us/sample - loss: 0.0310 - mae: 0.0304 - mape: 8937.3135 - val_loss: 0.0374 - val_mae: 0.0368 - val_mape: 6.7796\n",
      "Epoch 35/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0308 - mae: 0.0302 - mape: 9005.4199\n",
      "Epoch 00035: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0308 - mae: 0.0302 - mape: 9003.7646 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.5172\n",
      "Epoch 36/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 8986.6465\n",
      "Epoch 00036: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0307 - mae: 0.0301 - mape: 8984.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.7190\n",
      "Epoch 37/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 9036.8193\n",
      "Epoch 00037: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0307 - mae: 0.0301 - mape: 9017.4414 - val_loss: 0.0397 - val_mae: 0.0391 - val_mape: 6.9239\n",
      "Epoch 38/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0304 - mae: 0.0298 - mape: 8871.0625\n",
      "Epoch 00038: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0305 - mae: 0.0299 - mape: 8852.0391 - val_loss: 0.0418 - val_mae: 0.0413 - val_mape: 7.1686\n",
      "Epoch 39/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8882.9082\n",
      "Epoch 00039: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8881.2744 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.0621\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8929.6758\n",
      "Epoch 00040: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8910.5225 - val_loss: 0.0390 - val_mae: 0.0384 - val_mape: 6.7820\n",
      "Epoch 41/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8876.4180\n",
      "Epoch 00041: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8857.3779 - val_loss: 0.0391 - val_mae: 0.0385 - val_mape: 6.9458\n",
      "Epoch 42/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8984.8389\n",
      "Epoch 00042: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8956.7549 - val_loss: 0.0413 - val_mae: 0.0407 - val_mape: 7.6354\n",
      "Epoch 43/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0301 - mae: 0.0295 - mape: 8793.3711\n",
      "Epoch 00043: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0301 - mae: 0.0295 - mape: 8783.1338 - val_loss: 0.0435 - val_mae: 0.0429 - val_mape: 7.4288\n",
      "Epoch 44/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8822.2676- ETA: 6s - loss: 0.0301 - mae: 0.0 - ETA: 1s - loss: 0.0302 - mae:\n",
      "Epoch 00044: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8811.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.9922\n",
      "Epoch 45/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0298 - mae: 0.0292 - mape: 8836.8652\n",
      "Epoch 00045: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0298 - mae: 0.0292 - mape: 8800.5830 - val_loss: 0.0593 - val_mae: 0.0587 - val_mape: 11.2009\n",
      "Epoch 46/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0287 - mae: 0.0281 - mape: 8818.6738\n",
      "Epoch 00046: val_loss improved from 0.03736 to 0.03728, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0046-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0287 - mae: 0.0281 - mape: 8791.1104 - val_loss: 0.0373 - val_mae: 0.0367 - val_mape: 6.3479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0269 - mape: 8903.1064\n",
      "Epoch 00047: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0276 - mae: 0.0269 - mape: 8866.5479 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 7.3343\n",
      "Epoch 48/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0264 - mape: 8938.7715\n",
      "Epoch 00048: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0270 - mae: 0.0264 - mape: 8919.5977 - val_loss: 0.0419 - val_mae: 0.0413 - val_mape: 6.8349\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0265 - mape: 8954.8320\n",
      "Epoch 00049: val_loss improved from 0.03728 to 0.03701, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0049-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0272 - mae: 0.0265 - mape: 8926.8408 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2637\n",
      "Epoch 50/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0263 - mape: 8925.1689\n",
      "Epoch 00050: val_loss did not improve from 0.03701\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0269 - mae: 0.0263 - mape: 8914.7754 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2835\n",
      "Epoch 51/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0262 - mape: 8973.1758\n",
      "Epoch 00051: val_loss improved from 0.03701 to 0.03644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0051-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0269 - mae: 0.0262 - mape: 8936.3379 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3367\n",
      "Epoch 52/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0267 - mae: 0.0260 - mape: 8984.7070\n",
      "Epoch 00052: val_loss improved from 0.03644 to 0.03603, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0267 - mae: 0.0260 - mape: 8947.8154 - val_loss: 0.0360 - val_mae: 0.0354 - val_mape: 6.2870\n",
      "Epoch 53/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.0259 - mape: 8895.4316\n",
      "Epoch 00053: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0265 - mae: 0.0259 - mape: 8893.7969 - val_loss: 0.0433 - val_mae: 0.0426 - val_mape: 6.9243\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0258 - mape: 8529.0977\n",
      "Epoch 00054: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0264 - mae: 0.0258 - mape: 8883.7363 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.3396\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8902.2764\n",
      "Epoch 00055: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8883.1807 - val_loss: 0.0377 - val_mae: 0.0371 - val_mape: 6.5213\n",
      "Epoch 56/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8834.2520\n",
      "Epoch 00056: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8806.6387 - val_loss: 0.0378 - val_mae: 0.0372 - val_mape: 6.4096\n",
      "Epoch 57/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0255 - mape: 8899.8281\n",
      "Epoch 00057: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0261 - mae: 0.0255 - mape: 8889.4648 - val_loss: 0.0365 - val_mae: 0.0358 - val_mape: 6.4844\n",
      "Epoch 58/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0257 - mape: 8935.8350\n",
      "Epoch 00058: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0263 - mae: 0.0257 - mape: 8899.1426 - val_loss: 0.0389 - val_mae: 0.0382 - val_mape: 6.9754\n",
      "Epoch 59/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0255 - mape: 8985.3438\n",
      "Epoch 00059: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0262 - mae: 0.0255 - mape: 8966.0693 - val_loss: 0.0448 - val_mae: 0.0442 - val_mape: 7.1970\n",
      "Epoch 60/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0254 - mape: 8698.2939\n",
      "Epoch 00060: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0261 - mae: 0.0254 - mape: 9026.8086 - val_loss: 0.0371 - val_mae: 0.0365 - val_mape: 6.4728\n",
      "Epoch 61/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0256 - mape: 8944.7266\n",
      "Epoch 00061: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0262 - mae: 0.0256 - mape: 8943.0820 - val_loss: 0.0376 - val_mae: 0.0369 - val_mape: 6.4816\n",
      "Epoch 62/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0253 - mape: 8861.0674\n",
      "Epoch 00062: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0259 - mae: 0.0253 - mape: 8833.3691 - val_loss: 0.0376 - val_mae: 0.0370 - val_mape: 6.7046\n",
      "Epoch 63/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0252 - mape: 8937.2539\n",
      "Epoch 00063: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0259 - mae: 0.0252 - mape: 8909.3174 - val_loss: 0.0441 - val_mae: 0.0434 - val_mape: 7.0393\n",
      "Epoch 64/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0251 - mape: 8836.8535\n",
      "Epoch 00064: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0257 - mae: 0.0251 - mape: 8826.5635 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.1966\n",
      "Epoch 65/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8864.3887\n",
      "Epoch 00065: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8845.3740 - val_loss: 0.0403 - val_mae: 0.0396 - val_mape: 6.6836\n",
      "Epoch 66/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0251 - mape: 8864.2441\n",
      "Epoch 00066: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0258 - mae: 0.0251 - mape: 8853.9238 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.2001\n",
      "Epoch 67/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8919.5137- ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8391\n",
      "Epoch 00067: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0256 - mae: 0.0249 - mape: 8900.3799 - val_loss: 0.0435 - val_mae: 0.0428 - val_mape: 6.9965\n",
      "Epoch 68/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0249 - mape: 8921.0537\n",
      "Epoch 00068: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0255 - mae: 0.0249 - mape: 8910.6650 - val_loss: 0.0368 - val_mae: 0.0361 - val_mape: 6.4016\n",
      "Epoch 69/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8904.1221\n",
      "Epoch 00069: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8885.0215 - val_loss: 0.0366 - val_mae: 0.0359 - val_mape: 6.4782\n",
      "Epoch 70/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0248 - mape: 8509.3799\n",
      "Epoch 00070: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0255 - mae: 0.0248 - mape: 8848.4863 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0247 - mape: 8894.0742\n",
      "Epoch 00071: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0254 - mae: 0.0247 - mape: 8874.9961 - val_loss: 0.0447 - val_mae: 0.0440 - val_mape: 7.2170\n",
      "Epoch 72/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0246 - mape: 8940.8789\n",
      "Epoch 00072: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0253 - mae: 0.0246 - mape: 8921.6992 - val_loss: 0.0399 - val_mae: 0.0393 - val_mape: 6.6394\n",
      "Epoch 73/80\n",
      "31840/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0246 - mape: 8697.8066"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d15c63a445f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BNTzwnub2Eh-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 577.97\n",
      "Median Absolute Error (MedAE): 474.07\n",
      "Mean Squared Error (MSE): 554533.54\n",
      "Root Mean Squared Error (RMSE): 744.67\n",
      "Mean Absolute Percentage Error (MAPE): 5.86 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.58 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tYU2gaBI4Yl"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fz9kU3b6IvyO"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5'\n",
    "start_epoch= 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2bMbPx8RJKL4"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5T4Ho3YJJKTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "14t41QwoJKZ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0241 - mae: 0.0235 - mape: 8787.5186\n",
      "Epoch 00001: val_loss improved from inf to 0.03371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 57s 2ms/sample - loss: 0.0241 - mae: 0.0235 - mape: 8785.9023 - val_loss: 0.0337 - val_mae: 0.0331 - val_mape: 5.6219\n",
      "Epoch 2/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8818.7939\n",
      "Epoch 00002: val_loss improved from 0.03371 to 0.03268, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 8808.5244 - val_loss: 0.0327 - val_mae: 0.0321 - val_mape: 5.5019\n",
      "Epoch 3/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 8832.4248\n",
      "Epoch 00003: val_loss improved from 0.03268 to 0.03238, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 8830.8018 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4341\n",
      "Epoch 4/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 8807.1309\n",
      "Epoch 00004: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 8805.5117 - val_loss: 0.0330 - val_mae: 0.0324 - val_mape: 5.4931\n",
      "Epoch 5/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0212 - mape: 8834.0068\n",
      "Epoch 00005: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0212 - mape: 8832.3828 - val_loss: 0.0333 - val_mae: 0.0328 - val_mape: 5.5382\n",
      "Epoch 6/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8771.0771\n",
      "Epoch 00006: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8769.4648 - val_loss: 0.0326 - val_mae: 0.0321 - val_mape: 5.4661\n",
      "Epoch 7/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8793.5381\n",
      "Epoch 00007: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8783.2979 - val_loss: 0.0325 - val_mae: 0.0320 - val_mape: 5.4550\n",
      "Epoch 8/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8746.4922\n",
      "Epoch 00008: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8744.8848 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.4740\n",
      "Epoch 9/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8799.6211\n",
      "Epoch 00009: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8789.3721 - val_loss: 0.0332 - val_mae: 0.0327 - val_mape: 5.5137\n",
      "Epoch 10/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8783.9893\n",
      "Epoch 00010: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8782.3740 - val_loss: 0.0327 - val_mae: 0.0322 - val_mape: 5.4856\n",
      "Epoch 11/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8814.5137\n",
      "Epoch 00011: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8804.2490 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.4683\n",
      "Epoch 12/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0208 - mape: 8829.3320\n",
      "Epoch 00012: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0213 - mae: 0.0208 - mape: 8819.0498 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4762\n",
      "Epoch 13/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8766.2891\n",
      "Epoch 00013: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8764.6777 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.6098\n",
      "Epoch 14/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0208 - mape: 8807.6914\n",
      "Epoch 00014: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0208 - mape: 8797.4355 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4454\n",
      "Epoch 15/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8763.3545\n",
      "Epoch 00015: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8761.7432 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4502\n",
      "Epoch 16/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0207 - mape: 8800.9727\n",
      "Epoch 00016: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0207 - mape: 8790.7227 - val_loss: 0.0336 - val_mae: 0.0331 - val_mape: 5.5508\n",
      "Epoch 17/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8805.7637\n",
      "Epoch 00017: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 8804.1465 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5136\n",
      "Epoch 18/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8785.8975\n",
      "Epoch 00018: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8775.6660 - val_loss: 0.0324 - val_mae: 0.0320 - val_mape: 5.4366\n",
      "Epoch 19/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8766.3740\n",
      "Epoch 00019: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8756.1650 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5053\n",
      "Epoch 20/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8869.6514- ETA: 1s - loss: 0.0209 - mae: 0.0205\n",
      "Epoch 00020: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8859.3301 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5394\n",
      "Epoch 21/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8719.9990\n",
      "Epoch 00021: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8718.3955 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.4912\n",
      "Epoch 22/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8816.5059\n",
      "Epoch 00022: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8814.8848 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5017\n",
      "Epoch 23/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8851.2988\n",
      "Epoch 00023: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8840.9902 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5257\n",
      "Epoch 24/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8849.9473\n",
      "Epoch 00024: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8839.6475 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8417.9062\n",
      "Epoch 00025: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8762.0635 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5165\n",
      "Epoch 26/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8781.5459\n",
      "Epoch 00026: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8771.3203 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5305\n",
      "Epoch 27/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8805.2725\n",
      "Epoch 00027: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8795.0176 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5373\n",
      "Epoch 28/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8814.1201\n",
      "Epoch 00028: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8803.8555 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5812\n",
      "Epoch 29/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8835.2373\n",
      "Epoch 00029: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8833.6133 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.5439\n",
      "Epoch 30/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8756.9062\n",
      "Epoch 00030: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8746.7100 - val_loss: 0.0335 - val_mae: 0.0331 - val_mape: 5.5782\n",
      "Epoch 31/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8848.8691\n",
      "Epoch 00031: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8847.2422 - val_loss: 0.0345 - val_mae: 0.0342 - val_mape: 5.6903\n",
      "Epoch 32/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8786.9775\n",
      "Epoch 00032: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8785.3623 - val_loss: 0.0348 - val_mae: 0.0344 - val_mape: 5.7179\n",
      "Epoch 33/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8790.1523\n",
      "Epoch 00033: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8779.9150 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5215\n",
      "Epoch 34/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8812.7725\n",
      "Epoch 00034: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8811.1523 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5479\n",
      "Epoch 35/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8778.3477-\n",
      "Epoch 00035: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8776.7334 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5697\n",
      "Epoch 36/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8766.7051\n",
      "Epoch 00036: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8756.4961 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.6073\n",
      "Epoch 37/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0200 - mape: 8769.1572\n",
      "Epoch 00037: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0200 - mape: 8767.5449 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5441\n",
      "Epoch 38/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0201 - mape: 8747.3877\n",
      "Epoch 00038: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0201 - mape: 8745.7793 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5797\n",
      "Epoch 39/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0200 - mape: 8836.6064\n",
      "Epoch 00039: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0200 - mape: 8834.9814 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5334\n",
      "Epoch 40/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8814.8398\n",
      "Epoch 00040: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8804.5742 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5985\n",
      "Epoch 41/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8839.9746\n",
      "Epoch 00041: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8829.6797 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5734\n",
      "Epoch 42/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0198 - mape: 8831.3848\n",
      "Epoch 00042: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0202 - mae: 0.0198 - mape: 8829.7607 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5997\n",
      "Epoch 43/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0201 - mae: 0.0198 - mape: 8783.8525\n",
      "Epoch 00043: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0201 - mae: 0.0198 - mape: 8782.2383 - val_loss: 0.0337 - val_mae: 0.0334 - val_mape: 5.6261\n",
      "Epoch 44/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0197 - mape: 8822.6426\n",
      "Epoch 00044: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0197 - mape: 8821.0205 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.6350\n",
      "Epoch 45/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8898.9219\n",
      "Epoch 00045: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8888.5576 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5640\n",
      "Epoch 46/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8848.5547\n",
      "Epoch 00046: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8846.9277 - val_loss: 0.0326 - val_mae: 0.0323 - val_mape: 5.5130\n",
      "Epoch 47/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8915.1211\n",
      "Epoch 00047: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8904.7383 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5152\n",
      "Epoch 48/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8884.6523\n",
      "Epoch 00048: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8874.3057 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5568\n",
      "Epoch 49/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0195 - mape: 8901.6914\n",
      "Epoch 00049: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0195 - mape: 8891.3252 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8912.0732\n",
      "Epoch 00050: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8901.6934 - val_loss: 0.0326 - val_mae: 0.0322 - val_mape: 5.5117\n",
      "Epoch 51/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0194 - mape: 8897.9141\n",
      "Epoch 00051: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0194 - mape: 8896.2773 - val_loss: 0.0339 - val_mae: 0.0336 - val_mape: 5.6620\n",
      "Epoch 52/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8820.4697\n",
      "Epoch 00052: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8810.1973 - val_loss: 0.0336 - val_mae: 0.0333 - val_mape: 5.6221\n",
      "Epoch 53/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8908.1738\n",
      "Epoch 00053: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8906.5361 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5511\n",
      "Epoch 54/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8871.0322\n",
      "Epoch 00054: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8869.4014 - val_loss: 0.0333 - val_mae: 0.0330 - val_mape: 5.5471\n",
      "Epoch 55/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8876.8662\n",
      "Epoch 00055: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8866.5283 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5071\n",
      "Epoch 56/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8917.7432- ETA: 2s - loss: 0.0196 - ma\n",
      "Epoch 00056: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8916.1035 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.5076\n",
      "Epoch 57/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8886.0176\n",
      "Epoch 00057: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8884.3838 - val_loss: 0.0330 - val_mae: 0.0327 - val_mape: 5.6687\n",
      "Epoch 58/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8843.5547\n",
      "Epoch 00058: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0195 - mae: 0.0192 - mape: 8833.2559 - val_loss: 0.0340 - val_mae: 0.0337 - val_mape: 5.6366\n",
      "Epoch 59/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8897.1895\n",
      "Epoch 00059: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8886.8271 - val_loss: 0.0344 - val_mae: 0.0340 - val_mape: 5.7448\n",
      "Epoch 60/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8878.5566\n",
      "Epoch 00060: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0192 - mape: 8868.2227 - val_loss: 0.0338 - val_mae: 0.0334 - val_mape: 5.6100\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.92\n",
      "Median Absolute Error (MedAE): 341.57\n",
      "Mean Squared Error (MSE): 329961.29\n",
      "Root Mean Squared Error (RMSE): 574.42\n",
      "Mean Absolute Percentage Error (MAPE): 4.31 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.33 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPZ0OMjDJhnO"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UZgZB020JqhL"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5'\n",
    "start_epoch= 53+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ssm3plQeJn6F"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "u-3UhfYuJlge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "9b_pJN2ZJKff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9133.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03257, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 9s 283us/sample - loss: 0.0214 - mae: 0.0208 - mape: 8890.3867 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4449\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8733.1416\n",
      "Epoch 00002: val_loss improved from 0.03257 to 0.03256, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8869.8643 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4437\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9107.9521\n",
      "Epoch 00003: val_loss improved from 0.03256 to 0.03242, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8865.1064 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8707.8369\n",
      "Epoch 00004: val_loss improved from 0.03242 to 0.03233, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8851.4385 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4285\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9089.2246\n",
      "Epoch 00005: val_loss improved from 0.03233 to 0.03228, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8846.9043 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8326.9922\n",
      "Epoch 00006: val_loss improved from 0.03228 to 0.03226, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8841.4912 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8705.7549\n",
      "Epoch 00007: val_loss improved from 0.03226 to 0.03223, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8839.4424 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4216\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9068.8096\n",
      "Epoch 00008: val_loss improved from 0.03223 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8827.0107 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4208\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8693.2275  ETA: 1s - loss: 0.0213 - mae: 0.020\n",
      "Epoch 00009: val_loss improved from 0.03222 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8828.1250 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8320.9189\n",
      "Epoch 00010: val_loss improved from 0.03222 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8823.5527 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8678.0664\n",
      "Epoch 00011: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8818.6221 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9058.5664\n",
      "Epoch 00012: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8817.0547 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.8018\n",
      "Epoch 00013: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8819.4121 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9055.2490\n",
      "Epoch 00014: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8813.7979 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9056.6045\n",
      "Epoch 00015: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8815.1211 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4180\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9046.7842\n",
      "Epoch 00016: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8805.6162 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.0303\n",
      "Epoch 00017: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8811.1191 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4184\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8288.7080\n",
      "Epoch 00018: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8806.9727 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9051.8076\n",
      "Epoch 00019: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.4658 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8302.6875\n",
      "Epoch 00020: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.8389 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4196\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.1914\n",
      "Epoch 00021: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8803.5537 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8289.9629\n",
      "Epoch 00022: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8808.2617 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.5645\n",
      "Epoch 00023: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8807.9199 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8657.6855\n",
      "Epoch 00024: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.0996 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8660.9229- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00025: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4287 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4194\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.9697\n",
      "Epoch 00026: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.0215 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4190\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.5674- ETA: 1s - loss: 0.0211 - mae: 0.0205\n",
      "Epoch 00027: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.4775 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.4648\n",
      "Epoch 00028: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4198\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8674.4756\n",
      "Epoch 00029: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.5635 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7906.3037\n",
      "Epoch 00030: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.4629 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.0000\n",
      "Epoch 00031: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.7725 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4202\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9045.3105\n",
      "Epoch 00032: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.1299 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.4668  ETA: 2s - loss: 0.0211 - m\n",
      "Epoch 00033: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.4404 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8285.9170\n",
      "Epoch 00034: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.4795 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4204\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.9082\n",
      "Epoch 00035: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6709 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8681.3047- ETA: 2s - loss: 0.0210 - mae: 0.\n",
      "Epoch 00036: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.3027 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1055- ETA: 2s - loss: 0.0211 - mae: \n",
      "Epoch 00037: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.6387 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.3789\n",
      "Epoch 00038: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4212\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7911.5845\n",
      "Epoch 00039: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.6006 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4207\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7988\n",
      "Epoch 00040: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.7549 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4209\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.4160\n",
      "Epoch 00041: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8797.7119 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4218\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.8086\n",
      "Epoch 00042: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.4561 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9040.8076- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8763.4\n",
      "Epoch 00043: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.7510 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8671.4062\n",
      "Epoch 00044: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9043.8984\n",
      "Epoch 00045: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.7754 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.5801\n",
      "Epoch 00046: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4297 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4199\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.4580\n",
      "Epoch 00047: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.2539 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4226\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8656.3926\n",
      "Epoch 00048: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8795.1895 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4227\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.8350\n",
      "Epoch 00049: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6064 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9053.7900\n",
      "Epoch 00050: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8812.3799 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4222\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.2178\n",
      "Epoch 00051: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.1719 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4229\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.2744\n",
      "Epoch 00052: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.0586 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4240\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7900\n",
      "Epoch 00053: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.5859 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8664.6738\n",
      "Epoch 00054: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.6367 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4230\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8658.2246\n",
      "Epoch 00055: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.2617 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4231\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9039.0703\n",
      "Epoch 00056: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.0605 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.2510\n",
      "Epoch 00057: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.1816 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4238\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.6846\n",
      "Epoch 00058: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.4893 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4235\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.6758\n",
      "Epoch 00059: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4250\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.4639\n",
      "Epoch 00060: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.2031 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.5479\n",
      "Epoch 00061: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8801.4482 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.7363\n",
      "Epoch 00062: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.5029 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1113- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7955. - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8069\n",
      "Epoch 00063: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8803.5488 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.6602\n",
      "Epoch 00064: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8809.3477 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8292.0371\n",
      "Epoch 00065: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8800.8613 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4243\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8661.9170\n",
      "Epoch 00066: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.5088 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4233\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.0117\n",
      "Epoch 00067: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.7666 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.6230- ETA: 2s - loss: 0.0212 - mae:\n",
      "Epoch 00068: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.3867 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4254\n",
      "Epoch 69/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8285.9189\n",
      "Epoch 00069: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.3457 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4237\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.5889\n",
      "Epoch 00070: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.3418 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.3438\n",
      "Epoch 00071: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8803.1846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4249\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.1455\n",
      "Epoch 00072: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.9678 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8665.8174\n",
      "Epoch 00073: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.1689 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.3008\n",
      "Epoch 00074: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.6846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.2236\n",
      "Epoch 00075: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.9883 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4261\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.5234  ETA: 2s - loss: 0.0209 - \n",
      "Epoch 00076: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4239\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9045.4512\n",
      "Epoch 00077: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.2715 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4265\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.2568\n",
      "Epoch 00078: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0527 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4256\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9055.4580\n",
      "Epoch 00079: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.0088 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4271\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9054.0859\n",
      "Epoch 00080: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8812.6738 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9052.4775\n",
      "Epoch 00081: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.1074 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4263\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.4385\n",
      "Epoch 00082: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.1035 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.2695\n",
      "Epoch 00083: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.9912 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8680.7773\n",
      "Epoch 00084: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8681.9316\n",
      "Epoch 00085: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.9023 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4258\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.9326\n",
      "Epoch 00086: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.5898 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.4629\n",
      "Epoch 00087: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.1172 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4259\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8666.3701\n",
      "Epoch 00088: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2188 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8664.1152\n",
      "Epoch 00089: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.5791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4258\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.7578\n",
      "Epoch 00090: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4267\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8674.5449\n",
      "Epoch 00091: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.6641 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4264\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.3330\n",
      "Epoch 00092: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.8057 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8680.4805\n",
      "Epoch 00093: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8807.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4268\n",
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8669.4492\n",
      "Epoch 00094: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0020 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8670.0195\n",
      "Epoch 00095: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.6592 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.8623\n",
      "Epoch 00096: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8808.5645 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4274\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8297.3535\n",
      "Epoch 00097: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8814.3789 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.8828\n",
      "Epoch 00098: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8803.7236 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8692.5449\n",
      "Epoch 00099: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8821.6729 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4253\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.0117  ETA: 2s - loss: 0.0212 - mae: 0.0207 - mape - ETA: 1s - loss: 0.0211 - mae: 0.0206 - mape: 13501.83 - ETA: 1s - loss: 0.0211 - mae: 0.0\n",
      "Epoch 00100: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.5918 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8291.2773\n",
      "Epoch 00101: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.7295 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4274\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.1973\n",
      "Epoch 00102: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.7637 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4269\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.2109\n",
      "Epoch 00103: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.1260 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.0146\n",
      "Epoch 00104: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.8604 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8309.9893\n",
      "Epoch 00105: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.9658 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8300.4561\n",
      "Epoch 00106: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.9160 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8687.0166\n",
      "Epoch 00107: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8820.6562 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4257\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.9922  ETA: 1s - loss: 0.021\n",
      "Epoch 00108: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8818.7070 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7918.0552\n",
      "Epoch 00109: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8806.1182 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4266\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9054.2461\n",
      "Epoch 00110: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8812.8340 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 111/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.2012\n",
      "Epoch 00111: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8809.0146 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 112/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.0439\n",
      "Epoch 00112: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.5566 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 113/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8305.8838\n",
      "Epoch 00113: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.7695 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4286\n",
      "Epoch 114/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9051.7646\n",
      "Epoch 00114: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.4189 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4268\n",
      "Epoch 115/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8671.9346\n",
      "Epoch 00115: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.5635 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4283\n",
      "Epoch 116/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.4092  ETA: 2s - loss: \n",
      "Epoch 00116: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.9219 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 117/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8685.8936- ETA: 1s - loss: 0.0209 - mae: 0.02\n",
      "Epoch 00117: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8817.3330 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 118/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8302.0400\n",
      "Epoch 00118: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8812.6797 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4707\n",
      "Epoch 00119: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.0967 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 120/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8304.8125\n",
      "Epoch 00120: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8818.3047 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4265\n",
      "Epoch 121/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9058.0273\n",
      "Epoch 00121: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5293 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4270\n",
      "Epoch 122/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8682.1084  ETA: 1s - loss: 0.0208 - mae:\n",
      "Epoch 00122: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.7100 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 123/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9050.8926\n",
      "Epoch 00123: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.5742 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4287\n",
      "Epoch 124/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.2852- ETA: 2s - loss: 0.0209 - mae:\n",
      "Epoch 00124: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.8037 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4277\n",
      "Epoch 125/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.9121\n",
      "Epoch 00125: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.4541 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 126/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7553.2466\n",
      "Epoch 00126: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.8301 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 127/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.8857\n",
      "Epoch 00127: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.4443 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 128/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.3145\n",
      "Epoch 00128: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.7705 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 129/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9062.5234\n",
      "Epoch 00129: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8820.9062 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 130/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8310.6270\n",
      "Epoch 00130: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.6572 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4276\n",
      "Epoch 131/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8676.7832\n",
      "Epoch 00131: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8813.8525 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 132/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.6250\n",
      "Epoch 00132: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1289 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 133/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.2627\n",
      "Epoch 00133: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.7959 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 134/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9385\n",
      "Epoch 00134: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.0283 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 135/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9990\n",
      "Epoch 00135: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.8438 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 136/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9050.7119- ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8224.\n",
      "Epoch 00136: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.3926 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 137/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9065.7979\n",
      "Epoch 00137: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.0713 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 138/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.0449\n",
      "Epoch 00138: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.1436 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4327\n",
      "Epoch 139/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.4678\n",
      "Epoch 00139: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.9834 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 140/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.8877\n",
      "Epoch 00140: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.4102 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 141/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.1895\n",
      "Epoch 00141: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.6465 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4293\n",
      "Epoch 142/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9068.6006\n",
      "Epoch 00142: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8826.8496 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4311\n",
      "Epoch 143/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.5254\n",
      "Epoch 00143: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.0791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8303.7139\n",
      "Epoch 00144: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.9434 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 145/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9057.6035\n",
      "Epoch 00145: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1113 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 146/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8295.9824\n",
      "Epoch 00146: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 147/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8299.4932- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00147: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.4980 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 148/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8677.7822\n",
      "Epoch 00148: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.8496 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 149/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4521\n",
      "Epoch 00149: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8807.3672 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4317\n",
      "Epoch 150/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8687.6270\n",
      "Epoch 00150: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.2520 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 151/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7550.1221\n",
      "Epoch 00151: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5537 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4318\n",
      "Epoch 152/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8665.8770\n",
      "Epoch 00152: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.2012 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4307\n",
      "Epoch 153/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8305.3477\n",
      "Epoch 00153: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.7168 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 154/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8664.9102\n",
      "Epoch 00154: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8803.5449 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 155/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.1807\n",
      "Epoch 00155: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.1172 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4323\n",
      "Epoch 156/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9061.2051\n",
      "Epoch 00156: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8819.6104 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 157/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9058.3799\n",
      "Epoch 00157: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.8613 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4316\n",
      "Epoch 158/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9053.1445\n",
      "Epoch 00158: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8811.7646 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 159/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.0205\n",
      "Epoch 00159: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8813.5781 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 160/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8673.6445\n",
      "Epoch 00160: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8812.8994 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4306\n",
      "Epoch 161/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.0654\n",
      "Epoch 00161: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8804.1348 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 162/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8678.7412\n",
      "Epoch 00162: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8817.2510 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 163/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9058.5049  ETA: 1s - loss: 0.0209 - mae: 0.0204 - map\n",
      "Epoch 00163: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.9990 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4288\n",
      "Epoch 164/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8671.3730\n",
      "Epoch 00164: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8807.5947 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 165/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8304.8350\n",
      "Epoch 00165: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8825.1436 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4294\n",
      "Epoch 166/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7930.7822\n",
      "Epoch 00166: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.1318 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 167/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8676.1992  ETA: 1s - loss: 0.0209 - mae: 0.0203 - mape:  - ETA: 1s - loss: 0.0208 - mae: 0.0202 - mape: - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9\n",
      "Epoch 00167: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.6123 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8682.7363\n",
      "Epoch 00168: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.8896 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 169/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9065.5938\n",
      "Epoch 00169: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8823.9004 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 170/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9051.8340\n",
      "Epoch 00170: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.4756 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 171/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8678.6602- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8967.800\n",
      "Epoch 00171: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8815.0049 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 172/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.9189\n",
      "Epoch 00172: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.3682 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 173/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9055.6904\n",
      "Epoch 00173: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2402 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 174/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.9043- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8588.017\n",
      "Epoch 00174: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.3857 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 175/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.7666\n",
      "Epoch 00175: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3809 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 176/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8673.9277\n",
      "Epoch 00176: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.3027 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4343\n",
      "Epoch 177/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.8848\n",
      "Epoch 00177: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2305 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4321\n",
      "Epoch 178/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9054.9736  ETA: 1s - loss: 0.0208 - mae: 0.0203 \n",
      "Epoch 00178: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.5439 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4330\n",
      "Epoch 179/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8675.4736\n",
      "Epoch 00179: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8809.1797 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4327\n",
      "Epoch 180/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9061.8086\n",
      "Epoch 00180: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2021 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4329\n",
      "Epoch 181/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9060.0869\n",
      "Epoch 00181: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5107 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4319\n",
      "Epoch 182/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.6191\n",
      "Epoch 00182: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5000 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 183/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.7178\n",
      "Epoch 00183: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2656 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4324\n",
      "Epoch 184/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8291.2705\n",
      "Epoch 00184: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.1533 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4331\n",
      "Epoch 185/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.7324\n",
      "Epoch 00185: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.8145 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 186/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.2549\n",
      "Epoch 00186: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8817.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4305\n",
      "Epoch 187/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.2168\n",
      "Epoch 00187: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8819.0938 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4315\n",
      "Epoch 188/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.7939\n",
      "Epoch 00188: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.2949 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 189/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.9834\n",
      "Epoch 00189: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.4678 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 190/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.2012\n",
      "Epoch 00190: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7588 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4332\n",
      "Epoch 191/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8679.1182\n",
      "Epoch 00191: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.3721 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4325\n",
      "Epoch 192/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8303.0508\n",
      "Epoch 00192: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7334 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 193/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8677.2891\n",
      "Epoch 00193: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.0742 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4315\n",
      "Epoch 194/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9051.3916\n",
      "Epoch 00194: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.0596 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4353\n",
      "Epoch 195/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 7911.0254\n",
      "Epoch 00195: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.6299 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 196/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9053.1436\n",
      "Epoch 00196: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.7627 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4345\n",
      "Epoch 197/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9056.7646- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 73\n",
      "Epoch 00197: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.2822 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 198/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8293.9717\n",
      "Epoch 00198: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3867 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4346\n",
      "Epoch 199/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.1162\n",
      "Epoch 00199: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.7646 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4328\n",
      "Epoch 200/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.9668- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 796\n",
      "Epoch 00200: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8808.9336 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4344\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.57\n",
      "Median Absolute Error (MedAE): 338.68\n",
      "Mean Squared Error (MSE): 332645.96\n",
      "Root Mean Squared Error (RMSE): 576.75\n",
      "Mean Absolute Percentage Error (MAPE): 4.3 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.3 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-0013-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aW5pp6LD-1"
   },
   "source": [
    "# E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "j9RoOmzuLgHR"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B2H5iBwQLfhj"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "k06fUuK7LerZ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mB2kXVVCLRBA"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sizKJbygLNBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Q6uDSkmdLJlR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.1973 - mae: 0.0794 - mape: 9692.2637\n",
      "Epoch 00001: val_loss improved from inf to 0.10644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0001-loss0.11.h5\n",
      "32614/32614 [==============================] - 19s 584us/sample - loss: 0.1972 - mae: 0.0794 - mape: 9680.9932 - val_loss: 0.1064 - val_mae: 0.0766 - val_mape: 12.9892\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0557 - mae: 0.0421 - mape: 9249.9512\n",
      "Epoch 00002: val_loss improved from 0.10644 to 0.05840, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0002-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0556 - mae: 0.0420 - mape: 9230.1133 - val_loss: 0.0584 - val_mae: 0.0523 - val_mape: 8.7301\n",
      "Epoch 3/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0416 - mae: 0.0375 - mape: 9262.3770\n",
      "Epoch 00003: val_loss improved from 0.05840 to 0.05401, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0003-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0416 - mae: 0.0375 - mape: 9224.3594 - val_loss: 0.0540 - val_mae: 0.0503 - val_mape: 8.7703\n",
      "Epoch 4/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0398 - mae: 0.0367 - mape: 9258.6729\n",
      "Epoch 00004: val_loss improved from 0.05401 to 0.04371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 431us/sample - loss: 0.0398 - mae: 0.0367 - mape: 9247.8994 - val_loss: 0.0437 - val_mae: 0.0407 - val_mape: 6.7483\n",
      "Epoch 5/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0336 - mape: 9132.0303  ETA: 4s - \n",
      "Epoch 00005: val_loss did not improve from 0.04371\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0362 - mae: 0.0336 - mape: 9121.3975 - val_loss: 0.0456 - val_mae: 0.0433 - val_mape: 7.2883\n",
      "Epoch 6/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0332 - mape: 9247.4834\n",
      "Epoch 00006: val_loss improved from 0.04371 to 0.03429, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0358 - mae: 0.0332 - mape: 9227.6475 - val_loss: 0.0343 - val_mae: 0.0317 - val_mape: 5.5244\n",
      "Epoch 7/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0334 - mae: 0.0314 - mape: 8832.6689\n",
      "Epoch 00007: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0334 - mae: 0.0314 - mape: 9193.4805 - val_loss: 0.0770 - val_mae: 0.0752 - val_mape: 10.8461\n",
      "Epoch 8/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0309 - mape: 9279.7900\n",
      "Epoch 00008: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0326 - mae: 0.0309 - mape: 9241.6865 - val_loss: 0.0911 - val_mae: 0.0895 - val_mape: 12.6461\n",
      "Epoch 9/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0301 - mape: 9240.0088\n",
      "Epoch 00009: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0315 - mae: 0.0301 - mape: 9211.1299 - val_loss: 0.0497 - val_mae: 0.0483 - val_mape: 7.3868\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0289 - mape: 8889.9570\n",
      "Epoch 00010: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0303 - mae: 0.0289 - mape: 9243.3398 - val_loss: 0.0727 - val_mae: 0.0714 - val_mape: 10.6575\n",
      "Epoch 11/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0290 - mape: 9401.6436\n",
      "Epoch 00011: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0303 - mae: 0.0290 - mape: 9372.2676 - val_loss: 0.0601 - val_mae: 0.0589 - val_mape: 8.9566\n",
      "Epoch 12/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0292 - mae: 0.0280 - mape: 9176.0518\n",
      "Epoch 00012: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0292 - mae: 0.0280 - mape: 9147.3730 - val_loss: 0.0464 - val_mae: 0.0452 - val_mape: 7.1787\n",
      "Epoch 13/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0291 - mae: 0.0279 - mape: 9234.7246\n",
      "Epoch 00013: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0291 - mae: 0.0279 - mape: 9214.9170 - val_loss: 0.0398 - val_mae: 0.0387 - val_mape: 6.3188\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.0278 - mape: 9259.1611\n",
      "Epoch 00014: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0290 - mae: 0.0278 - mape: 9221.1396 - val_loss: 0.0613 - val_mae: 0.0602 - val_mape: 8.9035\n",
      "Epoch 15/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0285 - mae: 0.0274 - mape: 9332.2979\n",
      "Epoch 00015: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0285 - mae: 0.0274 - mape: 9293.9756 - val_loss: 0.0448 - val_mae: 0.0438 - val_mape: 6.8179\n",
      "Epoch 16/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9270.8516- ETA: 0s - loss: 0.0278 - mae: 0.0268 - ma\n",
      "Epoch 00016: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9269.1475 - val_loss: 0.0455 - val_mae: 0.0446 - val_mape: 7.0034\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9212.3809\n",
      "Epoch 00017: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9210.6865 - val_loss: 0.0670 - val_mae: 0.0661 - val_mape: 9.5463\n",
      "Epoch 18/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9337.9092\n",
      "Epoch 00018: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9308.7227 - val_loss: 0.0456 - val_mae: 0.0447 - val_mape: 7.0638\n",
      "Epoch 19/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9224.4463\n",
      "Epoch 00019: val_loss improved from 0.03429 to 0.03249, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0019-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9213.7041 - val_loss: 0.0325 - val_mae: 0.0316 - val_mape: 5.3572\n",
      "Epoch 20/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9302.4258\n",
      "Epoch 00020: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9264.2266 - val_loss: 0.0382 - val_mae: 0.0373 - val_mape: 6.0539\n",
      "Epoch 21/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9244.2197\n",
      "Epoch 00021: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 436us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9206.2666 - val_loss: 0.0409 - val_mae: 0.0400 - val_mape: 6.2748\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0260 - mape: 9348.6699\n",
      "Epoch 00022: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0269 - mae: 0.0260 - mape: 9337.7842 - val_loss: 0.0328 - val_mae: 0.0319 - val_mape: 5.4575\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9254.0684\n",
      "Epoch 00023: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0266 - mae: 0.0257 - mape: 9252.3672 - val_loss: 0.0552 - val_mae: 0.0543 - val_mape: 8.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9298.8604\n",
      "Epoch 00024: val_loss improved from 0.03249 to 0.03114, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0024-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0265 - mae: 0.0257 - mape: 9260.6738 - val_loss: 0.0311 - val_mae: 0.0303 - val_mape: 5.1846\n",
      "Epoch 25/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0254 - mape: 9225.4922\n",
      "Epoch 00025: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0262 - mae: 0.0254 - mape: 9223.7969 - val_loss: 0.0519 - val_mae: 0.0511 - val_mape: 7.9640\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0254 - mape: 9168.1641\n",
      "Epoch 00026: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0263 - mae: 0.0254 - mape: 9139.5078 - val_loss: 0.0324 - val_mae: 0.0315 - val_mape: 5.4029\n",
      "Epoch 27/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0252 - mape: 9292.8506\n",
      "Epoch 00027: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 435us/sample - loss: 0.0261 - mae: 0.0252 - mape: 9282.0293 - val_loss: 0.0368 - val_mae: 0.0359 - val_mape: 5.8316\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0250 - mape: 9257.2148\n",
      "Epoch 00028: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0259 - mae: 0.0250 - mape: 9246.4346 - val_loss: 0.0322 - val_mae: 0.0313 - val_mape: 5.3347\n",
      "Epoch 29/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0249 - mape: 9201.8809\n",
      "Epoch 00029: val_loss improved from 0.03114 to 0.03017, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0257 - mae: 0.0249 - mape: 9173.1152 - val_loss: 0.0302 - val_mae: 0.0293 - val_mape: 5.0982\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0246 - mape: 9224.1299\n",
      "Epoch 00030: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0255 - mae: 0.0246 - mape: 9195.2969 - val_loss: 0.0316 - val_mae: 0.0308 - val_mape: 5.3475\n",
      "Epoch 31/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0247 - mape: 9221.6504\n",
      "Epoch 00031: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0256 - mae: 0.0247 - mape: 9219.9551 - val_loss: 0.0383 - val_mae: 0.0375 - val_mape: 6.0447\n",
      "Epoch 32/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9185.3818\n",
      "Epoch 00032: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0253 - mae: 0.0244 - mape: 9156.6689 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.6061\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9151.5020\n",
      "Epoch 00033: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0252 - mae: 0.0243 - mape: 9104.9512 - val_loss: 0.0435 - val_mae: 0.0426 - val_mape: 6.6821\n",
      "Epoch 34/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0240 - mape: 9209.8779\n",
      "Epoch 00034: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0248 - mae: 0.0240 - mape: 9163.0439 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 5.9090\n",
      "Epoch 35/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0243 - mape: 9190.4863\n",
      "Epoch 00035: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0251 - mae: 0.0243 - mape: 9170.7715 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.4763\n",
      "Epoch 36/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0241 - mape: 9201.8359\n",
      "Epoch 00036: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0250 - mae: 0.0241 - mape: 9182.0967 - val_loss: 0.0393 - val_mae: 0.0384 - val_mape: 6.1283\n",
      "Epoch 37/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0247 - mae: 0.0238 - mape: 9064.8945\n",
      "Epoch 00037: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0247 - mae: 0.0238 - mape: 9063.2285 - val_loss: 0.0322 - val_mae: 0.0314 - val_mape: 5.4603\n",
      "Epoch 38/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0236 - mape: 9153.4014\n",
      "Epoch 00038: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0245 - mae: 0.0236 - mape: 9142.7412 - val_loss: 0.0396 - val_mae: 0.0387 - val_mape: 6.1510\n",
      "Epoch 39/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9212.5186\n",
      "Epoch 00039: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9183.7207 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5752\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0237 - mape: 9298.7363\n",
      "Epoch 00040: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0246 - mae: 0.0237 - mape: 9278.7891 - val_loss: 0.0358 - val_mae: 0.0349 - val_mape: 5.7883\n",
      "Epoch 41/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9290.7383\n",
      "Epoch 00041: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9252.5859 - val_loss: 0.0359 - val_mae: 0.0350 - val_mape: 6.1253\n",
      "Epoch 42/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0244 - mae: 0.0235 - mape: 9100.7422\n",
      "Epoch 00042: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0244 - mae: 0.0235 - mape: 9099.0762 - val_loss: 0.0365 - val_mae: 0.0356 - val_mape: 5.8232\n",
      "Epoch 43/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.0231 - mape: 9206.7373\n",
      "Epoch 00043: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0240 - mae: 0.0231 - mape: 9186.9854 - val_loss: 0.0428 - val_mae: 0.0419 - val_mape: 6.5949\n",
      "Epoch 44/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.0229 - mape: 9111.2773\n",
      "Epoch 00044: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0238 - mae: 0.0229 - mape: 9109.6025 - val_loss: 0.0335 - val_mae: 0.0326 - val_mape: 5.5624\n",
      "Epoch 45/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0229 - mape: 9251.8613\n",
      "Epoch 00045: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0229 - mape: 9250.1602 - val_loss: 0.0383 - val_mae: 0.0374 - val_mape: 6.0213\n",
      "Epoch 46/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0228 - mape: 9249.3652\n",
      "Epoch 00046: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9229.5244 - val_loss: 0.0470 - val_mae: 0.0461 - val_mape: 7.1372\n",
      "Epoch 47/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0228 - mape: 9240.2764\n",
      "Epoch 00047: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9220.4561 - val_loss: 0.0355 - val_mae: 0.0346 - val_mape: 5.8071\n",
      "Epoch 48/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0227 - mape: 9228.7842\n",
      "Epoch 00048: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0236 - mae: 0.0227 - mape: 9227.0869 - val_loss: 0.0335 - val_mae: 0.0327 - val_mape: 5.7051\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0225 - mape: 9308.8867\n",
      "Epoch 00049: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0234 - mae: 0.0225 - mape: 9279.7959 - val_loss: 0.0391 - val_mae: 0.0382 - val_mape: 6.2820\n",
      "Epoch 50/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0223 - mape: 8910.0205\n",
      "Epoch 00050: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0232 - mae: 0.0223 - mape: 9253.5254 - val_loss: 0.0347 - val_mae: 0.0338 - val_mape: 5.7545\n",
      "Epoch 51/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0224 - mape: 9279.0811\n",
      "Epoch 00051: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0233 - mae: 0.0224 - mape: 9277.3750 - val_loss: 0.0390 - val_mae: 0.0381 - val_mape: 6.1803\n",
      "Epoch 52/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0222 - mape: 9221.8008\n",
      "Epoch 00052: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0231 - mae: 0.0222 - mape: 9202.0244 - val_loss: 0.0355 - val_mae: 0.0347 - val_mape: 5.8992\n",
      "Epoch 53/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0218 - mape: 9313.6279\n",
      "Epoch 00053: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0227 - mae: 0.0219 - mape: 9284.5176 - val_loss: 0.0346 - val_mae: 0.0337 - val_mape: 5.8458\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0221 - mape: 9235.9512\n",
      "Epoch 00054: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0230 - mae: 0.0221 - mape: 9207.0791 - val_loss: 0.0399 - val_mae: 0.0391 - val_mape: 6.3575\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.0219 - mape: 9211.0430\n",
      "Epoch 00055: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0228 - mae: 0.0219 - mape: 9191.2910 - val_loss: 0.0397 - val_mae: 0.0388 - val_mape: 6.2600\n",
      "Epoch 56/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0217 - mape: 9251.5020\n",
      "Epoch 00056: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0226 - mae: 0.0217 - mape: 9240.7275 - val_loss: 0.0405 - val_mae: 0.0397 - val_mape: 6.3565\n",
      "Epoch 57/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 9213.3115\n",
      "Epoch 00057: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9166.4385 - val_loss: 0.0343 - val_mae: 0.0334 - val_mape: 5.7472\n",
      "Epoch 58/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0216 - mape: 9254.9170\n",
      "Epoch 00058: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9253.2207 - val_loss: 0.0376 - val_mae: 0.0367 - val_mape: 6.0893\n",
      "Epoch 59/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0220 - mape: 9354.7930\n",
      "Epoch 00059: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0230 - mae: 0.0220 - mape: 9325.5488 - val_loss: 0.0341 - val_mae: 0.0332 - val_mape: 5.7046\n",
      "Epoch 60/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0214 - mape: 9235.1680\n",
      "Epoch 00060: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0222 - mae: 0.0214 - mape: 9188.1836 - val_loss: 0.0366 - val_mae: 0.0358 - val_mape: 5.9825\n",
      "Epoch 61/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0215 - mape: 9196.2959\n",
      "Epoch 00061: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0223 - mae: 0.0215 - mape: 9167.5479 - val_loss: 0.0399 - val_mae: 0.0390 - val_mape: 6.2796\n",
      "Epoch 62/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0213 - mape: 9107.4619\n",
      "Epoch 00062: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0222 - mae: 0.0213 - mape: 9087.9248 - val_loss: 0.0385 - val_mae: 0.0376 - val_mape: 6.1628\n",
      "Epoch 63/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0213 - mape: 9210.2637\n",
      "Epoch 00063: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0221 - mae: 0.0213 - mape: 9172.4463 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5972\n",
      "Epoch 64/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9124.6074\n",
      "Epoch 00064: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9122.9297 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 6.0281\n",
      "Epoch 65/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0212 - mape: 9226.6611\n",
      "Epoch 00065: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0220 - mae: 0.0212 - mape: 9188.7695 - val_loss: 0.0377 - val_mae: 0.0369 - val_mape: 6.2115\n",
      "Epoch 66/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9052.7314\n",
      "Epoch 00066: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9051.0664 - val_loss: 0.0405 - val_mae: 0.0396 - val_mape: 6.4286\n",
      "Epoch 67/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0209 - mape: 9083.4209\n",
      "Epoch 00067: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0218 - mae: 0.0209 - mape: 9072.8418 - val_loss: 0.0365 - val_mae: 0.0357 - val_mape: 6.0204\n",
      "Epoch 68/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9076.5205\n",
      "Epoch 00068: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9039.2510 - val_loss: 0.0342 - val_mae: 0.0334 - val_mape: 5.6559\n",
      "Epoch 69/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0208 - mape: 9033.5391\n",
      "Epoch 00069: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0217 - mae: 0.0208 - mape: 9005.3018 - val_loss: 0.0396 - val_mae: 0.0388 - val_mape: 6.4502\n",
      "Epoch 70/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9120.9199\n",
      "Epoch 00070: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9083.4629 - val_loss: 0.0361 - val_mae: 0.0353 - val_mape: 5.8008\n",
      "Epoch 71/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9097.3145\n",
      "Epoch 00071: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9095.6416 - val_loss: 0.0361 - val_mae: 0.0352 - val_mape: 5.9642\n",
      "Epoch 72/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0206 - mape: 9088.2021\n",
      "Epoch 00072: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0206 - mape: 9086.5312 - val_loss: 0.0380 - val_mae: 0.0372 - val_mape: 6.1542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9105.6396\n",
      "Epoch 00073: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9059.3135 - val_loss: 0.0434 - val_mae: 0.0426 - val_mape: 6.7060\n",
      "Epoch 74/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0205 - mape: 9066.7041\n",
      "Epoch 00074: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0205 - mape: 9065.0361 - val_loss: 0.0346 - val_mae: 0.0338 - val_mape: 5.7416\n",
      "Epoch 75/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0205 - mape: 8978.4404\n",
      "Epoch 00075: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0213 - mae: 0.0205 - mape: 8967.9941 - val_loss: 0.0348 - val_mae: 0.0340 - val_mape: 5.9216\n",
      "Epoch 76/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9057.9648\n",
      "Epoch 00076: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0210 - mae: 0.0202 - mape: 9056.2988 - val_loss: 0.0398 - val_mae: 0.0390 - val_mape: 6.3300\n",
      "Epoch 77/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0203 - mape: 8981.2236\n",
      "Epoch 00077: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0212 - mae: 0.0203 - mape: 8979.5723 - val_loss: 0.0451 - val_mae: 0.0444 - val_mape: 7.0400\n",
      "Epoch 78/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9038.3955\n",
      "Epoch 00078: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0210 - mae: 0.0202 - mape: 8992.4121 - val_loss: 0.0360 - val_mae: 0.0353 - val_mape: 5.9399\n",
      "Epoch 79/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0201 - mape: 8948.2764\n",
      "Epoch 00079: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0209 - mae: 0.0201 - mape: 8920.3037 - val_loss: 0.0350 - val_mae: 0.0342 - val_mape: 5.7982\n",
      "Epoch 80/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0200 - mape: 8990.1953\n",
      "Epoch 00080: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0208 - mae: 0.0200 - mape: 8962.0898 - val_loss: 0.0358 - val_mae: 0.0350 - val_mape: 5.8451\n"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "cW3NWXtXLDA-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 420.56\n",
      "Median Absolute Error (MedAE): 333.06\n",
      "Mean Squared Error (MSE): 313586.16\n",
      "Root Mean Squared Error (RMSE): 559.99\n",
      "Mean Absolute Percentage Error (MAPE): 4.19 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.25 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5'\n",
    "start_epoch= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0228 - mape: 9138.6191- ETA: 3s - loss: 0.023\n",
      "Epoch 00001: val_loss improved from inf to 0.03148, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 56s 2ms/sample - loss: 0.0235 - mae: 0.0228 - mape: 9136.9395 - val_loss: 0.0315 - val_mae: 0.0307 - val_mape: 5.2145\n",
      "Epoch 2/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0225 - mape: 9164.0752\n",
      "Epoch 00002: val_loss improved from 0.03148 to 0.03063, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0232 - mae: 0.0225 - mape: 9162.3906 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1229\n",
      "Epoch 3/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0224 - mape: 9196.9521- ETA: 3s - loss: 0.0\n",
      "Epoch 00003: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0231 - mae: 0.0224 - mape: 9195.2607 - val_loss: 0.0314 - val_mae: 0.0307 - val_mape: 5.2105\n",
      "Epoch 4/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0223 - mape: 9131.6660\n",
      "Epoch 00004: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0230 - mae: 0.0223 - mape: 9121.0312 - val_loss: 0.0310 - val_mae: 0.0304 - val_mape: 5.1898\n",
      "Epoch 5/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.0223 - mape: 9159.3213\n",
      "Epoch 00005: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0229 - mae: 0.0223 - mape: 9148.6533 - val_loss: 0.0321 - val_mae: 0.0314 - val_mape: 5.2817\n",
      "Epoch 6/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9180.6631\n",
      "Epoch 00006: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0228 - mae: 0.0221 - mape: 9178.9824 - val_loss: 0.0309 - val_mae: 0.0303 - val_mape: 5.1679\n",
      "Epoch 7/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9175.2949\n",
      "Epoch 00007: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0227 - mae: 0.0221 - mape: 9173.6084 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.2847\n",
      "Epoch 8/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0220 - mape: 9205.9355\n",
      "Epoch 00008: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0226 - mae: 0.0220 - mape: 9195.2217 - val_loss: 0.0307 - val_mae: 0.0301 - val_mape: 5.2188\n",
      "Epoch 9/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0219 - mape: 9181.9873\n",
      "Epoch 00009: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0225 - mae: 0.0219 - mape: 9180.2988 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.2595\n",
      "Epoch 10/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0218 - mape: 9186.6875\n",
      "Epoch 00010: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0224 - mae: 0.0218 - mape: 9184.9990 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2146\n",
      "Epoch 11/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0217 - mape: 9203.6816\n",
      "Epoch 00011: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0223 - mae: 0.0217 - mape: 9192.9629 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.2609\n",
      "Epoch 12/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9241.5742\n",
      "Epoch 00012: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 9239.8750 - val_loss: 0.0311 - val_mae: 0.0305 - val_mape: 5.2300\n",
      "Epoch 13/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0216 - mape: 9247.9932\n",
      "Epoch 00013: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0216 - mape: 9237.2227 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4177\n",
      "Epoch 14/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0215 - mape: 9297.9727\n",
      "Epoch 00014: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0215 - mape: 9296.2627 - val_loss: 0.0307 - val_mae: 0.0302 - val_mape: 5.2173\n",
      "Epoch 15/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0214 - mape: 9263.5205\n",
      "Epoch 00015: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0220 - mae: 0.0214 - mape: 9261.8164 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2651\n",
      "Epoch 16/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 9198.6846\n",
      "Epoch 00016: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 9196.9941 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.3174\n",
      "Epoch 17/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 9260.9404\n",
      "Epoch 00017: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 9250.1543 - val_loss: 0.0309 - val_mae: 0.0304 - val_mape: 5.2860\n",
      "Epoch 18/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0212 - mape: 9216.3271\n",
      "Epoch 00018: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0212 - mape: 9214.6318 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3829\n",
      "Epoch 19/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0211 - mape: 9211.0586\n",
      "Epoch 00019: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0211 - mape: 9200.3320 - val_loss: 0.0341 - val_mae: 0.0336 - val_mape: 5.5767\n",
      "Epoch 20/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0209 - mape: 9201.0938\n",
      "Epoch 00020: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0215 - mae: 0.0209 - mape: 9190.3779 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3684\n",
      "Epoch 21/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9219.9883\n",
      "Epoch 00021: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0214 - mae: 0.0208 - mape: 9218.2930 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3618\n",
      "Epoch 22/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 9187.7080\n",
      "Epoch 00022: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 9186.0186 - val_loss: 0.0316 - val_mae: 0.0310 - val_mape: 5.3197\n",
      "Epoch 23/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9196.0430\n",
      "Epoch 00023: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0206 - mape: 9194.3525 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2367\n",
      "Epoch 24/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9184.2910\n",
      "Epoch 00024: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 9173.5947 - val_loss: 0.0315 - val_mae: 0.0309 - val_mape: 5.2920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9166.1699\n",
      "Epoch 00025: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0210 - mae: 0.0205 - mape: 9155.4951 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.3492\n",
      "Epoch 26/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9205.6270\n",
      "Epoch 00026: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9203.9385 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2379\n",
      "Epoch 27/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9117.5986\n",
      "Epoch 00027: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9115.9219 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4364\n",
      "Epoch 28/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9204.3340\n",
      "Epoch 00028: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9193.6143 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2468\n",
      "Epoch 29/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9128.1553\n",
      "Epoch 00029: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9126.4766 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3819\n",
      "Epoch 30/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0202 - mape: 9126.3975\n",
      "Epoch 00030: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0207 - mae: 0.0202 - mape: 9124.7197 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.3127\n",
      "Epoch 31/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9127.3574\n",
      "Epoch 00031: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9125.6787 - val_loss: 0.0314 - val_mae: 0.0308 - val_mape: 5.3096\n",
      "Epoch 32/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9139.1289\n",
      "Epoch 00032: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9137.4482 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3143\n",
      "Epoch 33/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0200 - mape: 9129.0029\n",
      "Epoch 00033: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0205 - mae: 0.0200 - mape: 9127.3242 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3023\n",
      "Epoch 34/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9085.1152\n",
      "Epoch 00034: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9083.4443 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3421\n",
      "Epoch 35/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9161.0391\n",
      "Epoch 00035: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9159.3545 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2771\n",
      "Epoch 36/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0198 - mape: 9147.9355\n",
      "Epoch 00036: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0203 - mae: 0.0198 - mape: 9137.2803 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3513\n",
      "Epoch 37/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9170.7051- ETA: 4s - loss: 0. - ETA: 2s - loss: 0.0203\n",
      "Epoch 00037: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9169.0186 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2897\n",
      "Epoch 38/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9186.7959\n",
      "Epoch 00038: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9176.0967 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2613\n",
      "Epoch 39/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0195 - mape: 9121.9736\n",
      "Epoch 00039: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0200 - mae: 0.0195 - mape: 9120.2959 - val_loss: 0.0348 - val_mae: 0.0343 - val_mape: 5.6403\n",
      "Epoch 40/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0194 - mape: 9143.8574\n",
      "Epoch 00040: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0199 - mae: 0.0194 - mape: 9142.1768 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3696\n",
      "Epoch 41/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0192 - mape: 9166.4883\n",
      "Epoch 00041: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0197 - mae: 0.0192 - mape: 9155.8125 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3686\n",
      "Epoch 42/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9157.3496\n",
      "Epoch 00042: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9146.6846 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3299\n",
      "Epoch 43/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9169.7188\n",
      "Epoch 00043: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9159.0391 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2889\n",
      "Epoch 44/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0190 - mape: 9109.2627\n",
      "Epoch 00044: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0195 - mae: 0.0190 - mape: 9107.5879 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3664\n",
      "Epoch 45/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0189 - mape: 9101.7988\n",
      "Epoch 00045: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0195 - mae: 0.0189 - mape: 9100.1250 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4306\n",
      "Epoch 46/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0194 - mae: 0.0189 - mape: 9152.3506\n",
      "Epoch 00046: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0194 - mae: 0.0189 - mape: 9150.6680 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.3450\n",
      "Epoch 47/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0193 - mae: 0.0187 - mape: 9092.9639\n",
      "Epoch 00047: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0193 - mae: 0.0187 - mape: 9091.2920 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3590\n",
      "Epoch 48/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.0186 - mape: 9166.8369\n",
      "Epoch 00048: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0191 - mae: 0.0186 - mape: 9165.1514 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4518\n",
      "Epoch 49/200\n",
      "11776/32614 [=========>....................] - ETA: 32s - loss: 0.0191 - mae: 0.0186 - mape: 7337.6323"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-048f600f12b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 412.44\n",
      "Median Absolute Error (MedAE): 328.08\n",
      "Mean Squared Error (MSE): 301264.17\n",
      "Root Mean Squared Error (RMSE): 548.88\n",
      "Mean Absolute Percentage Error (MAPE): 4.12 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.19 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5'\n",
    "start_epoch= 30+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0218 - mape: 9021.4141\n",
      "Epoch 00001: val_loss improved from inf to 0.03075, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 7s 222us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9155.7246 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1276\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9392.5967\n",
      "Epoch 00002: val_loss improved from 0.03075 to 0.03067, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9142.1660 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1187\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 8595.9766\n",
      "Epoch 00003: val_loss improved from 0.03067 to 0.03054, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9143.0762 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1073\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9385.6611\n",
      "Epoch 00004: val_loss improved from 0.03054 to 0.03050, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9135.4072 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1036\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9377.5986\n",
      "Epoch 00005: val_loss improved from 0.03050 to 0.03046, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9127.5752 - val_loss: 0.0305 - val_mae: 0.0297 - val_mape: 5.0995\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8606.6309\n",
      "Epoch 00006: val_loss improved from 0.03046 to 0.03042, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9137.0928 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0965\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9375.1191\n",
      "Epoch 00007: val_loss improved from 0.03042 to 0.03041, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9125.1416 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9374.5898\n",
      "Epoch 00008: val_loss improved from 0.03041 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9124.6318 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0924\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8210.0459\n",
      "Epoch 00009: val_loss improved from 0.03038 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9126.5254 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0921\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8572.6328  ETA: 1s - loss: 0.0225 - mae: 0.0218 \n",
      "Epoch 00010: val_loss improved from 0.03038 to 0.03035, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9118.1846 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0894\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8207.2930\n",
      "Epoch 00011: val_loss did not improve from 0.03035\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9111.9365 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0895\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9357.2139\n",
      "Epoch 00012: val_loss improved from 0.03035 to 0.03034, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0012-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.7373 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0893\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8967.2090\n",
      "Epoch 00013: val_loss improved from 0.03034 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0859 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0887\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8584.7832\n",
      "Epoch 00014: val_loss improved from 0.03033 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0014-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9115.9170 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9359.8037\n",
      "Epoch 00015: val_loss improved from 0.03033 to 0.03030, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0015-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.2441 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0861\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8970.0029\n",
      "Epoch 00016: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9116.9443 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0868\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8182.7358\n",
      "Epoch 00017: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.9473 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0872\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9360.5244\n",
      "Epoch 00018: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.9551 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8964.7793\n",
      "Epoch 00019: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0879 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8972.5371\n",
      "Epoch 00020: val_loss improved from 0.03030 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0020-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9106.9902 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0856\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8572.1016\n",
      "Epoch 00021: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9105.2520 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0876\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9354.3223\n",
      "Epoch 00022: val_loss improved from 0.03028 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.9277 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0858\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9355.4004- ETA: 1s - loss: 0.0223 - mae: 0.0216 - mape: 9009 - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 869\n",
      "Epoch 00023: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9105.9697 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9358.2070\n",
      "Epoch 00024: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9108.6885 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0860\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8967.9619\n",
      "Epoch 00025: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.4453 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8956.2822\n",
      "Epoch 00026: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.1289 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0879\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9353.0088\n",
      "Epoch 00027: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9103.6240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.5430\n",
      "Epoch 00028: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1504 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0871\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9354.4570\n",
      "Epoch 00029: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.0547 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0886\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.4277\n",
      "Epoch 00030: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.0498 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2637\n",
      "Epoch 00031: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0391 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0890\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.9912\n",
      "Epoch 00032: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.1416 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0896\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8960.5801\n",
      "Epoch 00033: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7783 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0913\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9346.8809\n",
      "Epoch 00034: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9097.6689 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8965.5186\n",
      "Epoch 00035: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8569.2568\n",
      "Epoch 00036: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1602 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0915\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.2012  ETA: 2s - loss: 0.02\n",
      "Epoch 00037: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1436 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0920\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8574.7637\n",
      "Epoch 00038: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.4717 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0902\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.5205\n",
      "Epoch 00039: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.2197 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.8828\n",
      "Epoch 00040: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.5615 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.7637\n",
      "Epoch 00041: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.3633 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0908\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9359.6875\n",
      "Epoch 00042: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0914\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.9414\n",
      "Epoch 00043: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9099.0850 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3818\n",
      "Epoch 00044: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9072 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0957\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2998\n",
      "Epoch 00045: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0811 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8428- ETA: 2s - loss: 0.0223 - mae: 0\n",
      "Epoch 00046: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9570 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0959\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.5625\n",
      "Epoch 00047: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.1621 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0927\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8184\n",
      "Epoch 00048: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.2930 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0955\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8958.4678- ETA: 2s - loss: 0.0224 - mae: 0.0217 - mape: 4847. - ETA: 1s - loss: 0.0222 - mae: 0.0215 \n",
      "Epoch 00049: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.8057 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0937\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8977.0430\n",
      "Epoch 00050: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.9639 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0950\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.6865\n",
      "Epoch 00051: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.2705 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0954\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8964.1064\n",
      "Epoch 00052: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.1123 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0919\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.2607\n",
      "Epoch 00053: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.9170 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0989\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8959.4570\n",
      "Epoch 00054: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.2852 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0969\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.4199- ETA: 2s - loss: 0.0225 - mae: 0\n",
      "Epoch 00055: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.2910 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0990\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6221\n",
      "Epoch 00056: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1025 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0925\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.5264\n",
      "Epoch 00057: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8096 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1009\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8963.5762\n",
      "Epoch 00058: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.3652 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1010\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8584.6045\n",
      "Epoch 00059: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9108.2227 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0963\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.8242\n",
      "Epoch 00060: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.3945 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0996\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3730  ETA: 2s - loss: 0.0\n",
      "Epoch 00061: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8975 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1003\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.2998\n",
      "Epoch 00062: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9113.6562 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0987\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8572.1582\n",
      "Epoch 00063: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.7090 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1015\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6914\n",
      "Epoch 00064: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1013\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.9131\n",
      "Epoch 00065: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.5430 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1008\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9361.4287\n",
      "Epoch 00066: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9111.8457 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1032\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8968.2002\n",
      "Epoch 00067: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.6504 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1028\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.6250\n",
      "Epoch 00068: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7305 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8962.6729- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8456.346\n",
      "Epoch 00069: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9107.0010 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1072\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.2842\n",
      "Epoch 00070: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.8740 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1061\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.1514\n",
      "Epoch 00071: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9108.2100 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1067\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9350.1494\n",
      "Epoch 00072: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9100.8525 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.5088\n",
      "Epoch 00073: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9105.0918 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1070\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8561.8984\n",
      "Epoch 00074: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1073\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.5430\n",
      "Epoch 00075: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.3232 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1101\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.6846\n",
      "Epoch 00076: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.0889 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1018\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.6553\n",
      "Epoch 00077: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.3877 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1131\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.4307\n",
      "Epoch 00078: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.1025 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1066\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8972.5918\n",
      "Epoch 00079: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7490 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.0195\n",
      "Epoch 00080: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.7041 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1059\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.1738\n",
      "Epoch 00081: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.8281 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1086\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9359.2744\n",
      "Epoch 00082: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7461 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1125\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9355.9951\n",
      "Epoch 00083: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.5498 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1091\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.0264\n",
      "Epoch 00084: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9111.4297 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8973.9590\n",
      "Epoch 00085: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.7012 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1117\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8980.7080\n",
      "Epoch 00086: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.9180 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1181\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9362.3105\n",
      "Epoch 00087: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.6982 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1146\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.5547\n",
      "Epoch 00088: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1924 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.5771\n",
      "Epoch 00089: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.1777 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1158\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.7559\n",
      "Epoch 00090: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.4727 - val_loss: 0.0304 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9344.5049\n",
      "Epoch 00091: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9095.3486 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1201\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.3213\n",
      "Epoch 00092: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.6084 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1109\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.4619\n",
      "Epoch 00093: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1074 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9357.2207\n",
      "Epoch 00094: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.7432 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1149\n",
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.8730\n",
      "Epoch 00095: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.5840 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9366.0117\n",
      "Epoch 00096: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9116.2979 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1090\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 7807.1338- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape:\n",
      "Epoch 00097: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9097.1768 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8969.1562\n",
      "Epoch 00098: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9108.1855 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1151\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9350.9482  ETA\n",
      "Epoch 00099: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.6289 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1170\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.7568\n",
      "Epoch 00100: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.3867 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1185\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.9570\n",
      "Epoch 00101: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.5547 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.7695- ETA: 1s - loss: 0.0222 - mae: 0.0215 - ma\n",
      "Epoch 00102: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.2959 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1222\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9360.5537\n",
      "Epoch 00103: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.9805 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1187\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8967.6729\n",
      "Epoch 00104: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1299 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1163\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.5879\n",
      "Epoch 00105: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.3262 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1221\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.9297\n",
      "Epoch 00106: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9114.2559 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1136\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8575.9648\n",
      "Epoch 00107: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.5449 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1218\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.1768\n",
      "Epoch 00108: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9098.9189 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1236\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.6055\n",
      "Epoch 00109: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.2510 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1214\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0214 - mape: 9352.8691\n",
      "Epoch 00110: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0221 - mae: 0.0215 - mape: 9103.5098 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1184\n",
      "Epoch 111/200\n",
      "23552/32614 [====================>.........] - ETA: 0s - loss: 0.0220 - mae: 0.0213 - mape: 9332.0186"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-aa18ff9589c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 411.01\n",
      "Median Absolute Error (MedAE): 326.15\n",
      "Mean Squared Error (MSE): 300114.5\n",
      "Root Mean Squared Error (RMSE): 547.83\n",
      "Mean Absolute Percentage Error (MAPE): 4.11 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.17 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-0022-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
