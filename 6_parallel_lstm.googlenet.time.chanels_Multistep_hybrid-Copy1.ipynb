{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tWP4R-BZmaZi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\pyimagesearch')\n",
    "from timeseires.CNN1D.GoogLeNet1D import PC\n",
    "from timeseires.utils.to_split import to_split\n",
    "#from timeseires.callbacks.EpochCheckpoint import EpochCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeseires.callbacks.TrainingMonitor import TrainingMonitor\n",
    "from timeseires.utils.load_only_multi_step import load_only_multi_step\n",
    "from timeseires.utils.multivariate_multi_step import multivariate_multi_step\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD  \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,MaxPooling1D,Concatenate,AveragePooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "import pandas as pd\n",
    "import time, pickle\n",
    "import pydot\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, Reshape, Dense, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten, LeakyReLU, concatenate\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12130, 21), (24259, 21))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tr = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_train.csv'\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values \n",
    "\n",
    "path_v = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_validation.csv'\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "\n",
    "path_te = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_test.csv'\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "#.......................................................................................................\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_scaler.pkl\", 'rb'))\n",
    "df_te.shape,df_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nmYgpwTPnsf4"
   },
   "outputs": [],
   "source": [
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIYjXnV81ZHC",
    "outputId": "4520a0c1-81f1-4cf6-9b17-d15f5ae9c7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.37147045135498047 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X, train_y = multivariate_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24,ahead=0)\n",
    "validation_X, validation_y = multivariate_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24,ahead=0)\n",
    "test_X, test_y = multivariate_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24,ahead=0)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.2492685317993164 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X_load, train_y_load = load_only_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24)\n",
    "validation_X_load, validation_y_load = load_only_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24)\n",
    "test_X_load, test_y_load = load_only_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PbWIXWwSJN3k"
   },
   "outputs": [],
   "source": [
    "#EXP 1 STEPS 24 LOOKBACK 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_goog():   \n",
    "    load_one_trig = Input(shape=(time_steps, num_features))\n",
    "    #--------------------------------------------------------\n",
    "    inputs_load = Input(shape=(time_steps, 1))\n",
    "    l = Bidirectional(LSTM(32, return_sequences=True))(inputs_load)\n",
    "    l = Bidirectional(LSTM(32))(l)\n",
    "    \n",
    "    c11 = PC.inception_module(inputs_load, 32, 32, 64, 8, 32, \"1a\", 0.01)\n",
    "    c12 = Concatenate(axis=-1)([inputs_load, c11])\n",
    "    \n",
    "    c21 = PC.inception_module(c12, 64, 64, 128, 16, 64, \"2a\", 0.01)\n",
    "    c22 = Concatenate(axis=-1)([ c11, c21])\n",
    "    \n",
    "    c31 = PC.inception_module(c22, 64, 64, 128, 16, 64, \"3a\", 0.01)\n",
    "    #---------------------------------------------------------\n",
    " #   x = Concatenate(axis=-1)([inputs_load, l, c21, c31])\n",
    " #   x = Flatten()(x)\n",
    "    #--------------------------------------------------------\n",
    "    calender_load_time_spatial_features = Concatenate(axis=-1)([load_one_trig, c22, c31])\n",
    "    lstm1   = LSTM(20, return_sequences = True)(calender_load_time_spatial_features)\n",
    "    lstm2   = LSTM(20)(lstm1)\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "    #y= PC.conv_module(y, 128, 3, 1)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([l, lstm2])\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(24, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inputs_load, load_one_trig], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = review_goog()\n",
    "tensorflow.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 24, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_conv (Conv1D)        (None, 24, 32)       64          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_conv (Conv1D)         (None, 24, 8)        16          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_bn (BatchNormalizati (None, 24, 32)       128         1a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_bn (BatchNormalizatio (None, 24, 8)        32          1a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_act (Activation)     (None, 24, 32)       0           1a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_act (Activation)      (None, 24, 8)        0           1a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_conv (Conv1D)          (None, 24, 32)       64          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_conv (Conv1D)        (None, 24, 64)       6208        1a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_conv (Conv1D)         (None, 24, 32)       1312        1a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_bn (BatchNormalization (None, 24, 32)       128         1a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_bn (BatchNormalizati (None, 24, 64)       256         1a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_bn (BatchNormalizatio (None, 24, 32)       128         1a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_act (Activation)       (None, 24, 32)       0           1a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_act (Activation)     (None, 24, 64)       0           1a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_act (Activation)      (None, 24, 32)       0           1a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_mixed (Concatenate)          (None, 24, 128)      0           1a_first_act[0][0]               \n",
      "                                                                 1a_second2_act[0][0]             \n",
      "                                                                 1a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 24, 129)      0           input_2[0][0]                    \n",
      "                                                                 1a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_conv (Conv1D)        (None, 24, 64)       8320        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_conv (Conv1D)         (None, 24, 16)       2080        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_bn (BatchNormalizati (None, 24, 64)       256         2a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_bn (BatchNormalizatio (None, 24, 16)       64          2a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_act (Activation)     (None, 24, 64)       0           2a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_act (Activation)      (None, 24, 16)       0           2a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_conv (Conv1D)          (None, 24, 64)       8320        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_conv (Conv1D)        (None, 24, 128)      24704       2a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_conv (Conv1D)         (None, 24, 64)       5184        2a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_bn (BatchNormalization (None, 24, 64)       256         2a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_bn (BatchNormalizati (None, 24, 128)      512         2a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_bn (BatchNormalizatio (None, 24, 64)       256         2a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_act (Activation)       (None, 24, 64)       0           2a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_act (Activation)     (None, 24, 128)      0           2a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_act (Activation)      (None, 24, 64)       0           2a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_mixed (Concatenate)          (None, 24, 256)      0           2a_first_act[0][0]               \n",
      "                                                                 2a_second2_act[0][0]             \n",
      "                                                                 2a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24, 384)      0           1a_mixed[0][0]                   \n",
      "                                                                 2a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_conv (Conv1D)        (None, 24, 64)       24640       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_conv (Conv1D)         (None, 24, 16)       6160        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_bn (BatchNormalizati (None, 24, 64)       256         3a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_bn (BatchNormalizatio (None, 24, 16)       64          3a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_act (Activation)     (None, 24, 64)       0           3a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_act (Activation)      (None, 24, 16)       0           3a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_conv (Conv1D)          (None, 24, 64)       24640       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_conv (Conv1D)        (None, 24, 128)      24704       3a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_conv (Conv1D)         (None, 24, 64)       5184        3a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_bn (BatchNormalization (None, 24, 64)       256         3a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_bn (BatchNormalizati (None, 24, 128)      512         3a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_bn (BatchNormalizatio (None, 24, 64)       256         3a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_act (Activation)       (None, 24, 64)       0           3a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_act (Activation)     (None, 24, 128)      0           3a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_act (Activation)      (None, 24, 64)       0           3a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 24, 21)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "3a_mixed (Concatenate)          (None, 24, 256)      0           3a_first_act[0][0]               \n",
      "                                                                 3a_second2_act[0][0]             \n",
      "                                                                 3a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 24, 661)      0           input_1[0][0]                    \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 3a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 24, 64)       177664      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 24, 20)       54560       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 64)           24832       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 20)           3280        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 84)           0           bidirectional_1[0][0]            \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 24)           2040        concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 407,336\n",
      "Trainable params: 405,656\n",
      "Non-trainable params: 1,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9rQrz9B0JWvR"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\6\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\6'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h6NbczAlOpSQ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NkF7jg_XO346"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-rIEZj0PCPF",
    "outputId": "95edda02-84de-4f8a-83a2-b4d53387049d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = review_goog()\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-o0qgceiPFdI",
    "outputId": "ebb2c588-e73f-4182-baad-2f4c28f3db95",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.1596 - mae: 0.0471 - mape: 2985.6626\n",
      "Epoch 00001: val_loss improved from inf to 0.03809, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\6\\E1-cp-0001-loss0.04.h5\n",
      "2652/2652 [==============================] - 79s 30ms/step - loss: 0.1596 - mae: 0.0471 - mape: 2985.6626 - val_loss: 0.0381 - val_mae: 0.0376 - val_mape: 16.5687\n",
      "Epoch 2/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0358 - mae: 0.0353 - mape: 2541.6213\n",
      "Epoch 00002: val_loss did not improve from 0.03809\n",
      "2652/2652 [==============================] - 71s 27ms/step - loss: 0.0358 - mae: 0.0353 - mape: 2541.6213 - val_loss: 0.0522 - val_mae: 0.0518 - val_mape: 25.7652\n",
      "Epoch 3/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0337 - mae: 0.0333 - mape: 2512.8352\n",
      "Epoch 00003: val_loss improved from 0.03809 to 0.03547, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\6\\E1-cp-0003-loss0.04.h5\n",
      "2652/2652 [==============================] - 71s 27ms/step - loss: 0.0337 - mae: 0.0333 - mape: 2512.0386 - val_loss: 0.0355 - val_mae: 0.0351 - val_mape: 15.7182\n",
      "Epoch 4/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0322 - mape: 2588.3792\n",
      "Epoch 00004: val_loss improved from 0.03547 to 0.03291, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\6\\E1-cp-0004-loss0.03.h5\n",
      "2652/2652 [==============================] - 72s 27ms/step - loss: 0.0326 - mae: 0.0322 - mape: 2587.5588 - val_loss: 0.0329 - val_mae: 0.0326 - val_mape: 14.6760\n",
      "Epoch 5/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0318 - mae: 0.0315 - mape: 2570.1426\n",
      "Epoch 00005: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0318 - mae: 0.0315 - mape: 2570.1426 - val_loss: 0.0339 - val_mae: 0.0336 - val_mape: 15.2178\n",
      "Epoch 6/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0312 - mae: 0.0309 - mape: 2525.5015\n",
      "Epoch 00006: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 70s 27ms/step - loss: 0.0312 - mae: 0.0309 - mape: 2525.5015 - val_loss: 0.0394 - val_mae: 0.0391 - val_mape: 16.7239\n",
      "Epoch 7/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0307 - mae: 0.0304 - mape: 2541.0439- ETA: 0s - loss: 0.0307 - mae: 0.0304 - mape:\n",
      "Epoch 00007: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0307 - mae: 0.0304 - mape: 2541.0439 - val_loss: 0.0357 - val_mae: 0.0355 - val_mape: 14.9525\n",
      "Epoch 8/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0303 - mae: 0.0300 - mape: 2562.5623\n",
      "Epoch 00008: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0303 - mae: 0.0300 - mape: 2562.5623 - val_loss: 0.0334 - val_mae: 0.0331 - val_mape: 15.0802\n",
      "Epoch 9/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0299 - mae: 0.0296 - mape: 2480.6306\n",
      "Epoch 00009: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0299 - mae: 0.0296 - mape: 2479.8438 - val_loss: 0.0340 - val_mae: 0.0338 - val_mape: 14.3567\n",
      "Epoch 10/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0296 - mae: 0.0293 - mape: 2468.4482\n",
      "Epoch 00010: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0296 - mae: 0.0293 - mape: 2466.7385 - val_loss: 0.0391 - val_mae: 0.0388 - val_mape: 16.1272\n",
      "Epoch 11/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0293 - mae: 0.0290 - mape: 2456.8352\n",
      "Epoch 00011: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0293 - mae: 0.0290 - mape: 2455.1360 - val_loss: 0.0385 - val_mae: 0.0383 - val_mape: 16.3738\n",
      "Epoch 12/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0290 - mae: 0.0288 - mape: 2393.7715\n",
      "Epoch 00012: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0290 - mae: 0.0288 - mape: 2393.7715 - val_loss: 0.0404 - val_mae: 0.0402 - val_mape: 18.4891\n",
      "Epoch 13/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0288 - mae: 0.0285 - mape: 2416.5247\n",
      "Epoch 00013: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0288 - mae: 0.0285 - mape: 2416.5247 - val_loss: 0.0386 - val_mae: 0.0384 - val_mape: 17.0216\n",
      "Epoch 14/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0286 - mae: 0.0284 - mape: 2374.1089- ETA: 1s - loss: 0.0286 - m\n",
      "Epoch 00014: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 70s 27ms/step - loss: 0.0286 - mae: 0.0284 - mape: 2374.1089 - val_loss: 0.0378 - val_mae: 0.0376 - val_mape: 15.7497\n",
      "Epoch 15/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0284 - mae: 0.0282 - mape: 2367.2637\n",
      "Epoch 00015: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0284 - mae: 0.0282 - mape: 2366.5127 - val_loss: 0.0415 - val_mae: 0.0413 - val_mape: 19.5371\n",
      "Epoch 16/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0283 - mae: 0.0280 - mape: 2348.2178\n",
      "Epoch 00016: val_loss did not improve from 0.03291\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0283 - mae: 0.0280 - mape: 2346.5903 - val_loss: 0.0350 - val_mae: 0.0348 - val_mape: 14.8429\n",
      "Epoch 17/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0281 - mae: 0.0279 - mape: 2356.8884\n",
      "Epoch 00017: val_loss improved from 0.03291 to 0.03200, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\6\\E1-cp-0017-loss0.03.h5\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0281 - mae: 0.0279 - mape: 2356.1406 - val_loss: 0.0320 - val_mae: 0.0318 - val_mape: 14.5646\n",
      "Epoch 18/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0280 - mae: 0.0277 - mape: 2333.2886\n",
      "Epoch 00018: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0280 - mae: 0.0277 - mape: 2333.2886 - val_loss: 0.0348 - val_mae: 0.0345 - val_mape: 15.1492\n",
      "Epoch 19/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0279 - mae: 0.0277 - mape: 2302.8333\n",
      "Epoch 00019: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 70s 27ms/step - loss: 0.0279 - mae: 0.0277 - mape: 2302.8333 - val_loss: 0.0323 - val_mae: 0.0321 - val_mape: 13.8618\n",
      "Epoch 20/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0278 - mae: 0.0275 - mape: 2287.1699- ETA: 0s - loss: 0.0278 - mae: 0.0275 - mape: 229\n",
      "Epoch 00020: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0278 - mae: 0.0275 - mape: 2285.5859 - val_loss: 0.0362 - val_mae: 0.0360 - val_mape: 15.6578\n",
      "Epoch 21/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0274 - mape: 2307.5969\n",
      "Epoch 00021: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0276 - mae: 0.0274 - mape: 2305.9983 - val_loss: 0.0326 - val_mae: 0.0323 - val_mape: 14.3112\n",
      "Epoch 22/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0275 - mae: 0.0273 - mape: 2231.4822\n",
      "Epoch 00022: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0275 - mae: 0.0273 - mape: 2231.4822 - val_loss: 0.0328 - val_mae: 0.0326 - val_mape: 14.3109\n",
      "Epoch 23/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0274 - mae: 0.0272 - mape: 2225.2649\n",
      "Epoch 00023: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0274 - mae: 0.0272 - mape: 2224.5593 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 15.8688\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0273 - mae: 0.0271 - mape: 2208.0645\n",
      "Epoch 00024: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0273 - mae: 0.0271 - mape: 2208.0645 - val_loss: 0.0327 - val_mae: 0.0325 - val_mape: 13.9462\n",
      "Epoch 25/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0270 - mape: 2161.2505\n",
      "Epoch 00025: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0272 - mae: 0.0270 - mape: 2160.5657 - val_loss: 0.0360 - val_mae: 0.0358 - val_mape: 15.7768\n",
      "Epoch 26/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0269 - mape: 2220.8362\n",
      "Epoch 00026: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0271 - mae: 0.0269 - mape: 2219.2981 - val_loss: 0.0395 - val_mae: 0.0392 - val_mape: 16.2596\n",
      "Epoch 27/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0268 - mape: 2157.1340\n",
      "Epoch 00027: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0270 - mae: 0.0268 - mape: 2156.4502 - val_loss: 0.0371 - val_mae: 0.0369 - val_mape: 17.0335\n",
      "Epoch 28/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0267 - mape: 2167.3540\n",
      "Epoch 00028: val_loss did not improve from 0.03200\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0269 - mae: 0.0267 - mape: 2165.8530 - val_loss: 0.0362 - val_mae: 0.0360 - val_mape: 17.2273\n",
      "Epoch 29/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0268 - mae: 0.0266 - mape: 2177.0142- ETA: 1s - loss: 0.0268 -\n",
      "Epoch 00029: val_loss improved from 0.03200 to 0.03168, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\6\\E1-cp-0029-loss0.03.h5\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0268 - mae: 0.0266 - mape: 2177.0142 - val_loss: 0.0317 - val_mae: 0.0315 - val_mape: 14.5154\n",
      "Epoch 30/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0267 - mae: 0.0265 - mape: 2174.4998\n",
      "Epoch 00030: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0267 - mae: 0.0265 - mape: 2174.4998 - val_loss: 0.0322 - val_mae: 0.0320 - val_mape: 13.5484\n",
      "Epoch 31/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0267 - mae: 0.0265 - mape: 2154.8357\n",
      "Epoch 00031: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0267 - mae: 0.0265 - mape: 2153.3425 - val_loss: 0.0339 - val_mae: 0.0337 - val_mape: 16.4492\n",
      "Epoch 32/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0264 - mape: 2120.0444\n",
      "Epoch 00032: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 27ms/step - loss: 0.0266 - mae: 0.0264 - mape: 2119.3730 - val_loss: 0.0318 - val_mae: 0.0315 - val_mape: 13.2376\n",
      "Epoch 33/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.0263 - mape: 2112.5791\n",
      "Epoch 00033: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0265 - mae: 0.0263 - mape: 2111.9094 - val_loss: 0.0337 - val_mae: 0.0335 - val_mape: 14.9016\n",
      "Epoch 34/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0264 - mae: 0.0262 - mape: 2128.7349\n",
      "Epoch 00034: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0264 - mae: 0.0262 - mape: 2128.7349 - val_loss: 0.0388 - val_mae: 0.0386 - val_mape: 15.7660\n",
      "Epoch 35/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0264 - mae: 0.0262 - mape: 2106.7979\n",
      "Epoch 00035: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 27ms/step - loss: 0.0264 - mae: 0.0262 - mape: 2106.7979 - val_loss: 0.0379 - val_mae: 0.0377 - val_mape: 15.6365\n",
      "Epoch 36/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0263 - mae: 0.0261 - mape: 2098.5669\n",
      "Epoch 00036: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0263 - mae: 0.0261 - mape: 2098.5669 - val_loss: 0.0327 - val_mae: 0.0325 - val_mape: 14.2565\n",
      "Epoch 37/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0263 - mae: 0.0261 - mape: 2135.5190\n",
      "Epoch 00037: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 71s 27ms/step - loss: 0.0263 - mae: 0.0261 - mape: 2135.5190 - val_loss: 0.0336 - val_mae: 0.0334 - val_mape: 14.6601\n",
      "Epoch 38/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0260 - mape: 2115.1609\n",
      "Epoch 00038: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0262 - mae: 0.0260 - mape: 2152.8484 - val_loss: 0.0373 - val_mae: 0.0371 - val_mape: 15.5629\n",
      "Epoch 39/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0259 - mape: 2109.4365- ETA: 5s - loss: \n",
      "Epoch 00039: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0261 - mae: 0.0259 - mape: 2107.9753 - val_loss: 0.0353 - val_mae: 0.0351 - val_mape: 14.8676\n",
      "Epoch 40/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0259 - mape: 2133.0940- ETA: 0s - loss: 0.0261 - mae: 0.0259 - mape: 2138.7\n",
      "Epoch 00040: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 27ms/step - loss: 0.0261 - mae: 0.0259 - mape: 2132.4182 - val_loss: 0.0319 - val_mae: 0.0317 - val_mape: 13.8113\n",
      "Epoch 41/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0260 - mae: 0.0258 - mape: 2105.2869\n",
      "Epoch 00041: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0260 - mae: 0.0258 - mape: 2105.2869 - val_loss: 0.0322 - val_mae: 0.0320 - val_mape: 14.2718\n",
      "Epoch 42/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0257 - mape: 2108.7251\n",
      "Epoch 00042: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0259 - mae: 0.0257 - mape: 2108.0564 - val_loss: 0.0320 - val_mae: 0.0318 - val_mape: 14.8170\n",
      "Epoch 43/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0257 - mape: 2069.4788- ETA: 0s - loss: 0.0259 - mae: 0.0257 - mape\n",
      "Epoch 00043: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0259 - mae: 0.0257 - mape: 2068.8228 - val_loss: 0.0355 - val_mae: 0.0353 - val_mape: 15.2009\n",
      "Epoch 44/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0256 - mape: 2087.9407\n",
      "Epoch 00044: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0258 - mae: 0.0256 - mape: 2087.2783 - val_loss: 0.0330 - val_mae: 0.0328 - val_mape: 13.9014\n",
      "Epoch 45/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0256 - mape: 2078.6443\n",
      "Epoch 00045: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 71s 27ms/step - loss: 0.0258 - mae: 0.0256 - mape: 2077.9854 - val_loss: 0.0356 - val_mae: 0.0353 - val_mape: 14.7491\n",
      "Epoch 46/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0257 - mae: 0.0255 - mape: 2087.4983\n",
      "Epoch 00046: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0257 - mae: 0.0255 - mape: 2087.4983 - val_loss: 0.0410 - val_mae: 0.0408 - val_mape: 17.2253\n",
      "Epoch 47/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0255 - mape: 2104.3591- ETA: 5s  - ETA: 2s - los\n",
      "Epoch 00047: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0257 - mae: 0.0255 - mape: 2102.9011 - val_loss: 0.0356 - val_mae: 0.0354 - val_mape: 15.4564\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0256 - mae: 0.0254 - mape: 2075.9324\n",
      "Epoch 00048: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0256 - mae: 0.0254 - mape: 2075.9324 - val_loss: 0.0368 - val_mae: 0.0366 - val_mape: 15.6200\n",
      "Epoch 49/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0256 - mae: 0.0254 - mape: 2014.2952\n",
      "Epoch 00049: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0256 - mae: 0.0254 - mape: 2014.2952 - val_loss: 0.0350 - val_mae: 0.0349 - val_mape: 15.0380\n",
      "Epoch 50/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0254 - mape: 2067.5256\n",
      "Epoch 00050: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0256 - mae: 0.0254 - mape: 2066.0928 - val_loss: 0.0335 - val_mae: 0.0333 - val_mape: 14.6952\n",
      "Epoch 51/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0254 - mae: 0.0252 - mape: 2006.8047\n",
      "Epoch 00051: val_loss did not improve from 0.03168\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0254 - mae: 0.0252 - mape: 2006.8047 - val_loss: 0.0364 - val_mae: 0.0362 - val_mape: 15.8175\n",
      "Epoch 52/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0255 - mae: 0.0253 - mape: 2035.1552\n",
      "Epoch 00052: val_loss improved from 0.03168 to 0.03083, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\6\\E1-cp-0052-loss0.03.h5\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0255 - mae: 0.0253 - mape: 2035.1552 - val_loss: 0.0308 - val_mae: 0.0306 - val_mape: 13.5226\n",
      "Epoch 53/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0251 - mape: 2015.4796- ETA: 0s - loss: 0.0253 - mae: 0.0251 - mape: 2\n",
      "Epoch 00053: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0253 - mae: 0.0251 - mape: 2014.0840 - val_loss: 0.0360 - val_mae: 0.0358 - val_mape: 15.1510\n",
      "Epoch 54/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0251 - mape: 1984.4474\n",
      "Epoch 00054: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0253 - mae: 0.0251 - mape: 1983.8179 - val_loss: 0.0319 - val_mae: 0.0317 - val_mape: 13.7319\n",
      "Epoch 55/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0251 - mape: 2022.3003\n",
      "Epoch 00055: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0253 - mae: 0.0251 - mape: 2021.6589 - val_loss: 0.0349 - val_mae: 0.0347 - val_mape: 15.1394\n",
      "Epoch 56/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0250 - mape: 2037.3558\n",
      "Epoch 00056: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 71s 27ms/step - loss: 0.0252 - mae: 0.0250 - mape: 2035.9449 - val_loss: 0.0350 - val_mae: 0.0348 - val_mape: 15.5939\n",
      "Epoch 57/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0250 - mape: 1991.9296\n",
      "Epoch 00057: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0252 - mae: 0.0250 - mape: 1990.5491 - val_loss: 0.0323 - val_mae: 0.0321 - val_mape: 14.5153\n",
      "Epoch 58/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0249 - mape: 2026.8035\n",
      "Epoch 00058: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0251 - mae: 0.0249 - mape: 2026.1605 - val_loss: 0.0312 - val_mae: 0.0310 - val_mape: 13.5450\n",
      "Epoch 59/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0251 - mae: 0.0249 - mape: 1991.6637\n",
      "Epoch 00059: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 27ms/step - loss: 0.0251 - mae: 0.0249 - mape: 1991.6637 - val_loss: 0.0354 - val_mae: 0.0352 - val_mape: 16.1796\n",
      "Epoch 60/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0248 - mape: 2015.1785\n",
      "Epoch 00060: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0250 - mae: 0.0248 - mape: 2014.5396 - val_loss: 0.0346 - val_mae: 0.0344 - val_mape: 14.9035\n",
      "Epoch 61/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0248 - mape: 1968.6271\n",
      "Epoch 00061: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0250 - mae: 0.0248 - mape: 1968.0042 - val_loss: 0.0314 - val_mae: 0.0312 - val_mape: 13.5452\n",
      "Epoch 62/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0249 - mae: 0.0247 - mape: 1943.3046\n",
      "Epoch 00062: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0249 - mae: 0.0247 - mape: 1942.6887 - val_loss: 0.0316 - val_mae: 0.0314 - val_mape: 13.5993\n",
      "Epoch 63/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0249 - mae: 0.0247 - mape: 1962.4587\n",
      "Epoch 00063: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0249 - mae: 0.0247 - mape: 1962.4587 - val_loss: 0.0342 - val_mae: 0.0340 - val_mape: 14.7430\n",
      "Epoch 64/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0246 - mape: 1991.8239\n",
      "Epoch 00064: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 27ms/step - loss: 0.0248 - mae: 0.0246 - mape: 1991.1920 - val_loss: 0.0333 - val_mae: 0.0331 - val_mape: 14.4705\n",
      "Epoch 65/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0246 - mape: 1921.5918\n",
      "Epoch 00065: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0248 - mae: 0.0246 - mape: 1920.9825 - val_loss: 0.0336 - val_mae: 0.0334 - val_mape: 14.8547\n",
      "Epoch 66/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0248 - mae: 0.0246 - mape: 1950.6085\n",
      "Epoch 00066: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0248 - mae: 0.0246 - mape: 1950.6085 - val_loss: 0.0366 - val_mae: 0.0364 - val_mape: 15.3825\n",
      "Epoch 67/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0247 - mae: 0.0245 - mape: 1911.5311\n",
      "Epoch 00067: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0247 - mae: 0.0245 - mape: 1910.9252 - val_loss: 0.0323 - val_mae: 0.0321 - val_mape: 14.2975\n",
      "Epoch 68/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0247 - mae: 0.0245 - mape: 1932.0565\n",
      "Epoch 00068: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0247 - mae: 0.0245 - mape: 1932.0565 - val_loss: 0.0345 - val_mae: 0.0343 - val_mape: 14.8808\n",
      "Epoch 69/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0244 - mape: 1883.2880\n",
      "Epoch 00069: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 27ms/step - loss: 0.0246 - mae: 0.0244 - mape: 1882.6920 - val_loss: 0.0370 - val_mae: 0.0368 - val_mape: 15.9167\n",
      "Epoch 70/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0243 - mape: 1894.7795\n",
      "Epoch 00070: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0245 - mae: 0.0243 - mape: 1894.1791 - val_loss: 0.0326 - val_mae: 0.0324 - val_mape: 14.0202\n",
      "Epoch 71/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0243 - mape: 1923.1027\n",
      "Epoch 00071: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0245 - mae: 0.0243 - mape: 1922.4943 - val_loss: 0.0315 - val_mae: 0.0313 - val_mape: 13.7110\n",
      "Epoch 72/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0243 - mape: 1911.8583\n",
      "Epoch 00072: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0245 - mae: 0.0243 - mape: 1910.5347 - val_loss: 0.0322 - val_mae: 0.0320 - val_mape: 14.1053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mae: 0.0242 - mape: 1890.4041\n",
      "Epoch 00073: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0244 - mae: 0.0242 - mape: 1889.8046 - val_loss: 0.0347 - val_mae: 0.0345 - val_mape: 14.6532\n",
      "Epoch 74/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mae: 0.0242 - mape: 1897.6039\n",
      "Epoch 00074: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0244 - mae: 0.0242 - mape: 1897.0024 - val_loss: 0.0350 - val_mae: 0.0348 - val_mape: 14.7629\n",
      "Epoch 75/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0243 - mae: 0.0241 - mape: 1856.9005\n",
      "Epoch 00075: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0243 - mae: 0.0241 - mape: 1856.9005 - val_loss: 0.0354 - val_mae: 0.0352 - val_mape: 14.7330\n",
      "Epoch 76/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0241 - mape: 1932.1443\n",
      "Epoch 00076: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0243 - mae: 0.0241 - mape: 1931.5316 - val_loss: 0.0346 - val_mae: 0.0344 - val_mape: 15.1340\n",
      "Epoch 77/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0243 - mae: 0.0241 - mape: 1869.1395\n",
      "Epoch 00077: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0243 - mae: 0.0241 - mape: 1869.1395 - val_loss: 0.0370 - val_mae: 0.0368 - val_mape: 15.9553\n",
      "Epoch 78/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0242 - mae: 0.0240 - mape: 1900.3022\n",
      "Epoch 00078: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0242 - mae: 0.0240 - mape: 1900.3022 - val_loss: 0.0318 - val_mae: 0.0316 - val_mape: 13.7583\n",
      "Epoch 79/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0242 - mae: 0.0240 - mape: 1859.4274\n",
      "Epoch 00079: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0242 - mae: 0.0240 - mape: 1858.8384 - val_loss: 0.0324 - val_mae: 0.0322 - val_mape: 14.2369\n",
      "Epoch 80/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0241 - mae: 0.0240 - mape: 1861.7527\n",
      "Epoch 00080: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0241 - mae: 0.0240 - mape: 1860.4628 - val_loss: 0.0345 - val_mae: 0.0343 - val_mape: 14.7167\n",
      "Epoch 81/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0241 - mae: 0.0239 - mape: 1832.2983\n",
      "Epoch 00081: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 68s 26ms/step - loss: 0.0241 - mae: 0.0239 - mape: 1832.2983 - val_loss: 0.0338 - val_mae: 0.0336 - val_mape: 14.7284\n",
      "Epoch 82/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.0238 - mape: 1861.9153- ETA: 0s - loss: 0.0240 - mae: 0.0238 - mape: 1823.48\n",
      "Epoch 00082: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0240 - mae: 0.0238 - mape: 1861.3250 - val_loss: 0.0313 - val_mae: 0.0311 - val_mape: 13.7605\n",
      "Epoch 83/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0241 - mae: 0.0239 - mape: 1845.6031\n",
      "Epoch 00083: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0241 - mae: 0.0239 - mape: 1845.0184 - val_loss: 0.0359 - val_mae: 0.0357 - val_mape: 15.2219\n",
      "Epoch 84/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.0238 - mape: 1822.4761\n",
      "Epoch 00084: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0240 - mae: 0.0238 - mape: 1821.8984 - val_loss: 0.0354 - val_mae: 0.0352 - val_mape: 15.3322\n",
      "Epoch 85/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0239 - mae: 0.0238 - mape: 1835.6436\n",
      "Epoch 00085: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0239 - mae: 0.0238 - mape: 1834.3721 - val_loss: 0.0310 - val_mae: 0.0308 - val_mape: 13.3518\n",
      "Epoch 86/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0239 - mae: 0.0237 - mape: 1855.1670\n",
      "Epoch 00086: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 68s 26ms/step - loss: 0.0239 - mae: 0.0237 - mape: 1853.8812 - val_loss: 0.0333 - val_mae: 0.0331 - val_mape: 14.2014\n",
      "Epoch 87/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0239 - mae: 0.0237 - mape: 1846.1528\n",
      "Epoch 00087: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0239 - mae: 0.0237 - mape: 1844.8741 - val_loss: 0.0324 - val_mae: 0.0322 - val_mape: 14.3157\n",
      "Epoch 88/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.0236 - mape: 1811.1903\n",
      "Epoch 00088: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0238 - mae: 0.0236 - mape: 1810.6174 - val_loss: 0.0351 - val_mae: 0.0349 - val_mape: 14.8407\n",
      "Epoch 89/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0238 - mae: 0.0236 - mape: 1797.8461\n",
      "Epoch 00089: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0238 - mae: 0.0236 - mape: 1797.8461 - val_loss: 0.0325 - val_mae: 0.0324 - val_mape: 14.0482\n",
      "Epoch 90/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0235 - mape: 1818.4917- ETA: 0s - loss: 0.0237 - mae: 0.0235 - ma\n",
      "Epoch 00090: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0237 - mae: 0.0235 - mape: 1817.9150 - val_loss: 0.0333 - val_mae: 0.0331 - val_mape: 14.1285\n",
      "Epoch 91/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0237 - mae: 0.0235 - mape: 1808.7642\n",
      "Epoch 00091: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0237 - mae: 0.0235 - mape: 1808.7642 - val_loss: 0.0329 - val_mae: 0.0327 - val_mape: 14.1315\n",
      "Epoch 92/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0237 - mae: 0.0235 - mape: 1804.1843\n",
      "Epoch 00092: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0237 - mae: 0.0235 - mape: 1804.1843 - val_loss: 0.0329 - val_mae: 0.0327 - val_mape: 14.4414\n",
      "Epoch 93/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0234 - mape: 1845.1113\n",
      "Epoch 00093: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0236 - mae: 0.0234 - mape: 1843.8326 - val_loss: 0.0352 - val_mae: 0.0350 - val_mape: 14.9656\n",
      "Epoch 94/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0234 - mape: 1799.6573\n",
      "Epoch 00094: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0236 - mae: 0.0234 - mape: 1799.0872 - val_loss: 0.0348 - val_mae: 0.0346 - val_mape: 14.8960\n",
      "Epoch 95/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0236 - mae: 0.0234 - mape: 1792.8949\n",
      "Epoch 00095: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0236 - mae: 0.0234 - mape: 1792.8949 - val_loss: 0.0323 - val_mae: 0.0321 - val_mape: 13.6764\n",
      "Epoch 96/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0235 - mae: 0.0233 - mape: 1823.3892\n",
      "Epoch 00096: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0235 - mae: 0.0233 - mape: 1823.3892 - val_loss: 0.0335 - val_mae: 0.0333 - val_mape: 14.9543\n",
      "Epoch 97/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0235 - mae: 0.0233 - mape: 1801.0228\n",
      "Epoch 00097: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0235 - mae: 0.0233 - mape: 1801.0228 - val_loss: 0.0331 - val_mae: 0.0329 - val_mape: 14.3170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0233 - mape: 1809.5364\n",
      "Epoch 00098: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 69s 26ms/step - loss: 0.0235 - mae: 0.0233 - mape: 1808.2832 - val_loss: 0.0337 - val_mae: 0.0335 - val_mape: 14.4708\n",
      "Epoch 99/100\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0232 - mape: 1758.6747\n",
      "Epoch 00099: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 68s 26ms/step - loss: 0.0234 - mae: 0.0232 - mape: 1758.1176 - val_loss: 0.0329 - val_mae: 0.0327 - val_mape: 14.0103\n",
      "Epoch 100/100\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0234 - mae: 0.0232 - mape: 1790.7820\n",
      "Epoch 00100: val_loss did not improve from 0.03083\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.0234 - mae: 0.0232 - mape: 1790.7820 - val_loss: 0.0327 - val_mae: 0.0325 - val_mape: 13.9731\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oS6zzoUoPL-A",
    "outputId": "ce1a0f3b-6b76-4980-d717-d3542c18a37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 491.16\n",
      "Median Absolute Error (MedAE): 332.87\n",
      "Mean Squared Error (MSE): 480929.26\n",
      "Root Mean Squared Error (RMSE): 693.49\n",
      "Mean Absolute Percentage Error (MAPE): 3.28 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.33 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\6\\\\E1-cp-0052-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7y39sVKwLMm"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AVN1WhyzuWFl"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\5\\\\E2-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\5\\\\E1-cp-0045-loss0.03.h5'\n",
    "start_epoch= 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WBeDTA9lu5Qm"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2j6fmkOvA2O",
    "outputId": "7fc1b23b-4d93-4b54-a4df-921c557b9a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E1-cp-0045-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rTCJW6IzvLU4",
    "outputId": "f810e1ca-98cc-4de7-a4a3-465e83705c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0248 - mean_absolute_error: 0.0248 - mean_absolute_percentage_error: 1898.2002\n",
      "Epoch 00001: val_loss improved from inf to 0.03001, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E2-cp-0001-loss0.03.h5\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0248 - mean_absolute_error: 0.0248 - mean_absolute_percentage_error: 1896.8859 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300 - val_mean_absolute_percentage_error: 12.9522\n",
      "Epoch 2/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0247 - mean_absolute_error: 0.0247 - mean_absolute_percentage_error: 1878.4425\n",
      "Epoch 00002: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0247 - mean_absolute_error: 0.0247 - mean_absolute_percentage_error: 1877.8473 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300 - val_mean_absolute_percentage_error: 12.9457\n",
      "Epoch 3/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1863.4044\n",
      "Epoch 00003: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1862.8136 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1510\n",
      "Epoch 4/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1870.9830\n",
      "Epoch 00004: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1870.3900 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1811\n",
      "Epoch 5/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1866.6724\n",
      "Epoch 00005: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1866.6724 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304 - val_mean_absolute_percentage_error: 13.2401\n",
      "Epoch 6/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1867.6958\n",
      "Epoch 00006: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1867.1040 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0163\n",
      "Epoch 7/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1849.6239\n",
      "Epoch 00007: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1849.0377 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9824\n",
      "Epoch 8/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1852.4467\n",
      "Epoch 00008: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1852.4467 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304 - val_mean_absolute_percentage_error: 13.1703\n",
      "Epoch 9/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1853.4146\n",
      "Epoch 00009: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1852.1316 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0106\n",
      "Epoch 10/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1847.4153- ETA: 1s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percent\n",
      "Epoch 00010: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1846.1372 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0279\n",
      "Epoch 11/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1839.2570- ETA: 1s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute\n",
      "Epoch 00011: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1839.2570 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.1317\n",
      "Epoch 12/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1832.5651\n",
      "Epoch 00012: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1831.2968 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9474\n",
      "Epoch 13/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1835.7986\n",
      "Epoch 00013: val_loss did not improve from 0.03001\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1835.2163 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300 - val_mean_absolute_percentage_error: 12.8533\n",
      "Epoch 14/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1827.0063\n",
      "Epoch 00014: val_loss improved from 0.03001 to 0.02984, saving model to D:\\Checkpoints\\1_ieee_industrial info\\rewiev\\1_ModifiedGoogLeNet\\5\\E2-cp-0014-loss0.03.h5\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1827.0063 - val_loss: 0.0298 - val_mean_absolute_error: 0.0298 - val_mean_absolute_percentage_error: 12.7396\n",
      "Epoch 15/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1840.8979\n",
      "Epoch 00015: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1839.6233 - val_loss: 0.0301 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 12.9270\n",
      "Epoch 16/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1830.7869\n",
      "Epoch 00016: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 22ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1830.2061 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.0077\n",
      "Epoch 17/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1823.9042\n",
      "Epoch 00017: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1823.3262 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 13.0050\n",
      "Epoch 18/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1821.5284- ETA: 3s - loss: 0.0244 - mean_absolut\n",
      "Epoch 00018: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 62s 24ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1820.9518 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.0704\n",
      "Epoch 19/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1823.4219- ETA: \n",
      "Epoch 00019: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 63s 24ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1822.1588 - val_loss: 0.0301 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 12.8642\n",
      "Epoch 20/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1816.8361\n",
      "Epoch 00020: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1816.8361 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9048\n",
      "Epoch 21/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1814.0159\n",
      "Epoch 00021: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1814.0159 - val_loss: 0.0301 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 12.7949\n",
      "Epoch 22/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1820.1229\n",
      "Epoch 00022: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1820.1229 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.0084\n",
      "Epoch 23/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1807.3566\n",
      "Epoch 00023: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1806.1049 - val_loss: 0.0305 - val_mean_absolute_error: 0.0305 - val_mean_absolute_percentage_error: 13.1951\n",
      "Epoch 24/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1804.7067\n",
      "Epoch 00024: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1804.1354 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300 - val_mean_absolute_percentage_error: 12.7502\n",
      "Epoch 25/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1797.2174- ETA: 9s - loss: 0.0243 - mean_absolute_error\n",
      "Epoch 00025: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1795.9727 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9298\n",
      "Epoch 26/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1806.2993\n",
      "Epoch 00026: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1805.7277 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304 - val_mean_absolute_percentage_error: 13.1903\n",
      "Epoch 27/60\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1800.3931- ETA: 4s - loss: 0.0243 - m\n",
      "Epoch 00027: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1800.3931 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300 - val_mean_absolute_percentage_error: 12.7915\n",
      "Epoch 28/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1803.5500\n",
      "Epoch 00028: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1802.3015 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304 - val_mean_absolute_percentage_error: 13.1215\n",
      "Epoch 29/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1805.6102\n",
      "Epoch 00029: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1804.3599 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9977\n",
      "Epoch 30/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1792.7385\n",
      "Epoch 00030: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1791.4976 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9227\n",
      "Epoch 31/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1790.1373\n",
      "Epoch 00031: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 62s 23ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1789.5696 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302 - val_mean_absolute_percentage_error: 12.9248\n",
      "Epoch 32/60\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1791.7062- ETA: 0s - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1797.7\n",
      "Epoch 00032: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1791.1382 - val_loss: 0.0301 - val_mean_absolute_error: 0.0301 - val_mean_absolute_percentage_error: 12.8711\n",
      "Epoch 33/60\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1785.5836\n",
      "Epoch 00033: val_loss did not improve from 0.02984\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1784.3464 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303 - val_mean_absolute_percentage_error: 13.0029\n",
      "Epoch 34/60\n",
      "1179/2652 [============>.................] - ETA: 30s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1438.4391"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10576\\4001373639.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalidation_X_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_X\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y_load\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X_load,train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ([validation_X_load, validation_X], validation_y_load),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qj8HRsBXvf0M",
    "outputId": "084e6fbf-b9cc-46e8-e4d7-808e9031da87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 485.07\n",
      "Median Absolute Error (MedAE): 325.85\n",
      "Mean Squared Error (MSE): 475778.52\n",
      "Root Mean Squared Error (RMSE): 689.77\n",
      "Mean Absolute Percentage Error (MAPE): 3.22 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.28 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\rewiev\\\\1_ModifiedGoogLeNet\\\\5\\\\E2-cp-0014-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X_load,test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAuKvjoOG-fA"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QZI2LOvEw6Qq"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E2-cp-0006-loss0.04.h5'\n",
    "start_epoch= 59+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qPy-9EGRw6eR"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beh0i6Vww6pP",
    "outputId": "41024e1d-abfa-4ec6-a3ed-cf5e5fdef9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E2-cp-0006-loss0.04.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y-9y1xTeYAaB",
    "outputId": "3e59db7d-d1fa-41c7-a91e-cf2063e855da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0231 - mape: 8601.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03575, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0001-loss0.04.h5\n",
      "32614/32614 [==============================] - 7s 210us/sample - loss: 0.0235 - mae: 0.0231 - mape: 8729.3066 - val_loss: 0.0358 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0230 - mape: 8563.2227\n",
      "Epoch 00002: val_loss improved from 0.03575 to 0.03574, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0002-loss0.04.h5\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0235 - mae: 0.0230 - mape: 8710.7080 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1938\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8942.2402\n",
      "Epoch 00003: val_loss improved from 0.03574 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0003-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 115us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8703.8408 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8589.9561\n",
      "Epoch 00004: val_loss improved from 0.03573 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 114us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8711.1074 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1949\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8953.2998\n",
      "Epoch 00005: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8714.5996 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8595.0498\n",
      "Epoch 00006: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8716.8604 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8175.1738\n",
      "Epoch 00007: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8717.2090 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8577.0596\n",
      "Epoch 00008: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8716.0957 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1954\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2109\n",
      "Epoch 00009: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.7314 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2744\n",
      "Epoch 00010: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.3086 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.7139\n",
      "Epoch 00011: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.5303 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.2705\n",
      "Epoch 00012: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8717.8525 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1962\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2910\n",
      "Epoch 00013: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.2773 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8968.6553\n",
      "Epoch 00014: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.5400 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1970\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8580.6602\n",
      "Epoch 00015: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8719.8223 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1967\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8623\n",
      "Epoch 00016: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.7832 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1963\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8877\n",
      "Epoch 00017: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.8203 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8606.8154\n",
      "Epoch 00018: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0547 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2119\n",
      "Epoch 00019: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.2393 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8973.6885\n",
      "Epoch 00020: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8734.4512 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8217.7236  ETA: 2s - loss: 0.0233 -\n",
      "Epoch 00021: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8730.0449 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8967.8350\n",
      "Epoch 00022: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8728.7363 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8965.8945\n",
      "Epoch 00023: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.8516 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1969\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8607.8672\n",
      "Epoch 00024: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0010 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1964\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8588.1279\n",
      "Epoch 00025: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8735.5576 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8230.3066\n",
      "Epoch 00026: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8738.8379 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 27/200\n",
      "13312/32614 [===========>..................] - ETA: 1s - loss: 0.0233 - mae: 0.0229 - mape: 7589.0488"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-05065ff65640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y, \n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9fPE-jZuQ0r",
    "outputId": "aca09565-e9df-4067-d473-32dd8826a738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 541.71\n",
      "Median Absolute Error (MedAE): 449.47\n",
      "Mean Squared Error (MSE): 473684.02\n",
      "Root Mean Squared Error (RMSE): 688.25\n",
      "Mean Absolute Percentage Error (MAPE): 5.4 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.39 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-0004-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7x0O-u0GbLy"
   },
   "source": [
    "# E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rdBroUOqIeW6"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(200, activation='relu')(x)\n",
    " #       x = Dense(10, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AQaM-IuPIXqN"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QwUKEvSfIW09"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NHgO7mJ4IQVp"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TIsLEDoIN0C",
    "outputId": "3ef77929-b334-4953-faab-b938f376c306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpLkyqBmIKIw",
    "outputId": "3d701f4a-270d-4efc-b52a-cd26d12d4dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.2038 - mae: 0.0831 - mape: 10284.1670\n",
      "Epoch 00001: val_loss improved from inf to 0.10131, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0001-loss0.10.h5\n",
      "32614/32614 [==============================] - 19s 581us/sample - loss: 0.2036 - mae: 0.0830 - mape: 10262.1201 - val_loss: 0.1013 - val_mae: 0.0675 - val_mape: 11.9466\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0654 - mae: 0.0492 - mape: 9617.4668\n",
      "Epoch 00002: val_loss improved from 0.10131 to 0.07428, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0002-loss0.07.h5\n",
      "32614/32614 [==============================] - 14s 420us/sample - loss: 0.0654 - mae: 0.0492 - mape: 9596.8438 - val_loss: 0.0743 - val_mae: 0.0671 - val_mape: 11.8373\n",
      "Epoch 3/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0511 - mae: 0.0461 - mape: 9812.7764\n",
      "Epoch 00003: val_loss improved from 0.07428 to 0.06341, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0003-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0510 - mae: 0.0460 - mape: 9762.8730 - val_loss: 0.0634 - val_mae: 0.0599 - val_mape: 10.4427\n",
      "Epoch 4/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0450 - mae: 0.0419 - mape: 9616.9199\n",
      "Epoch 00004: val_loss did not improve from 0.06341\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0450 - mae: 0.0419 - mape: 9596.3027 - val_loss: 0.0706 - val_mae: 0.0680 - val_mape: 11.4199\n",
      "Epoch 5/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0441 - mae: 0.0414 - mape: 9613.5029\n",
      "Epoch 00005: val_loss improved from 0.06341 to 0.05454, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0005-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0441 - mae: 0.0414 - mape: 9611.7354 - val_loss: 0.0545 - val_mae: 0.0519 - val_mape: 9.2312\n",
      "Epoch 6/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0418 - mae: 0.0397 - mape: 9602.1924\n",
      "Epoch 00006: val_loss did not improve from 0.05454\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0418 - mae: 0.0397 - mape: 9572.1885 - val_loss: 0.0587 - val_mae: 0.0570 - val_mape: 9.8649\n",
      "Epoch 7/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0406 - mae: 0.0389 - mape: 9195.3799\n",
      "Epoch 00007: val_loss improved from 0.05454 to 0.04682, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0007-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0406 - mae: 0.0389 - mape: 9554.0049 - val_loss: 0.0468 - val_mae: 0.0450 - val_mape: 8.7847\n",
      "Epoch 8/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0399 - mae: 0.0384 - mape: 9416.9941\n",
      "Epoch 00008: val_loss did not improve from 0.04682\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0399 - mae: 0.0384 - mape: 9396.7998 - val_loss: 0.0475 - val_mae: 0.0462 - val_mape: 8.9737\n",
      "Epoch 9/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0397 - mae: 0.0383 - mape: 9665.3730\n",
      "Epoch 00009: val_loss improved from 0.04682 to 0.04522, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0009-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0397 - mae: 0.0383 - mape: 9644.6445 - val_loss: 0.0452 - val_mae: 0.0439 - val_mape: 8.5116\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0377 - mape: 9573.1260\n",
      "Epoch 00010: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0389 - mae: 0.0377 - mape: 9533.8271 - val_loss: 0.0940 - val_mae: 0.0928 - val_mape: 15.8112\n",
      "Epoch 11/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0390 - mae: 0.0378 - mape: 9537.1748\n",
      "Epoch 00011: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0390 - mae: 0.0378 - mape: 9535.4209 - val_loss: 0.0501 - val_mae: 0.0490 - val_mape: 8.8788\n",
      "Epoch 12/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0381 - mae: 0.0371 - mape: 9512.4170\n",
      "Epoch 00012: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0381 - mae: 0.0371 - mape: 9492.0146 - val_loss: 0.0511 - val_mae: 0.0502 - val_mape: 9.2328\n",
      "Epoch 13/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0377 - mae: 0.0368 - mape: 9480.7461\n",
      "Epoch 00013: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0377 - mae: 0.0368 - mape: 9479.0029 - val_loss: 0.0494 - val_mae: 0.0485 - val_mape: 9.0546\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.0370 - mape: 9640.1309\n",
      "Epoch 00014: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0379 - mae: 0.0370 - mape: 9600.5547 - val_loss: 0.0475 - val_mae: 0.0466 - val_mape: 8.6730\n",
      "Epoch 15/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0375 - mae: 0.0367 - mape: 9567.5752\n",
      "Epoch 00015: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0375 - mae: 0.0367 - mape: 9565.8154 - val_loss: 0.0497 - val_mae: 0.0489 - val_mape: 9.3880\n",
      "Epoch 16/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0374 - mae: 0.0366 - mape: 9551.2051\n",
      "Epoch 00016: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0374 - mae: 0.0366 - mape: 9540.0859 - val_loss: 0.0464 - val_mae: 0.0456 - val_mape: 8.5940\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0362 - mape: 9563.9082\n",
      "Epoch 00017: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0369 - mae: 0.0362 - mape: 9562.1514 - val_loss: 0.0475 - val_mae: 0.0468 - val_mape: 8.6608\n",
      "Epoch 18/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0361 - mape: 9577.6992\n",
      "Epoch 00018: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0368 - mae: 0.0361 - mape: 9566.5488 - val_loss: 0.0627 - val_mae: 0.0620 - val_mape: 10.5263\n",
      "Epoch 19/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0366 - mae: 0.0359 - mape: 8800.0615\n",
      "Epoch 00019: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0366 - mae: 0.0359 - mape: 9530.1943 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 8.7106\n",
      "Epoch 20/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9487.9727\n",
      "Epoch 00020: val_loss improved from 0.04522 to 0.04432, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0020-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9476.9248 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.3836\n",
      "Epoch 21/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9175.1748\n",
      "Epoch 00021: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9548.0811 - val_loss: 0.0444 - val_mae: 0.0438 - val_mape: 8.4635\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0356 - mape: 9438.7158\n",
      "Epoch 00022: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0362 - mae: 0.0356 - mape: 9427.7275 - val_loss: 0.0463 - val_mae: 0.0456 - val_mape: 8.8022\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0363 - mae: 0.0356 - mape: 9494.2676\n",
      "Epoch 00023: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0363 - mae: 0.0356 - mape: 9492.5234 - val_loss: 0.0502 - val_mae: 0.0496 - val_mape: 8.8956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0360 - mae: 0.0354 - mape: 9481.0664\n",
      "Epoch 00024: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0360 - mae: 0.0354 - mape: 9479.3242 - val_loss: 0.0482 - val_mae: 0.0476 - val_mape: 8.8535\n",
      "Epoch 25/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9452.0479\n",
      "Epoch 00025: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9422.5107 - val_loss: 0.0450 - val_mae: 0.0444 - val_mape: 8.3675\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9389.7109\n",
      "Epoch 00026: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9360.3701 - val_loss: 0.0505 - val_mae: 0.0499 - val_mape: 8.9818\n",
      "Epoch 27/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9449.0146\n",
      "Epoch 00027: val_loss improved from 0.04432 to 0.04404, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0027-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9419.4844 - val_loss: 0.0440 - val_mae: 0.0434 - val_mape: 8.4639\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9357.8389\n",
      "Epoch 00028: val_loss improved from 0.04404 to 0.04385, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0028-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9346.9443 - val_loss: 0.0438 - val_mae: 0.0433 - val_mape: 8.3336\n",
      "Epoch 29/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0350 - mape: 9348.0967- ETA: 2s - loss: 0.\n",
      "Epoch 00029: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0356 - mae: 0.0350 - mape: 9337.2129 - val_loss: 0.0453 - val_mae: 0.0447 - val_mape: 8.8223\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0355 - mae: 0.0349 - mape: 9328.2656\n",
      "Epoch 00030: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0355 - mae: 0.0349 - mape: 9299.1152 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.4327\n",
      "Epoch 31/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0348 - mape: 9367.9561\n",
      "Epoch 00031: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0354 - mae: 0.0348 - mape: 9329.5146 - val_loss: 0.0556 - val_mae: 0.0550 - val_mape: 9.5869\n",
      "Epoch 32/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0346 - mae: 0.0341 - mape: 9331.7041\n",
      "Epoch 00032: val_loss improved from 0.04385 to 0.03952, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0032-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0346 - mae: 0.0341 - mape: 9293.3955 - val_loss: 0.0395 - val_mae: 0.0389 - val_mape: 7.3175\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0313 - mae: 0.0307 - mape: 8705.7988\n",
      "Epoch 00033: val_loss did not improve from 0.03952\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0313 - mae: 0.0307 - mape: 9014.9424 - val_loss: 0.0402 - val_mae: 0.0396 - val_mape: 6.9964\n",
      "Epoch 34/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0310 - mae: 0.0304 - mape: 8956.5234\n",
      "Epoch 00034: val_loss improved from 0.03952 to 0.03736, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0034-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 427us/sample - loss: 0.0310 - mae: 0.0304 - mape: 8937.3135 - val_loss: 0.0374 - val_mae: 0.0368 - val_mape: 6.7796\n",
      "Epoch 35/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0308 - mae: 0.0302 - mape: 9005.4199\n",
      "Epoch 00035: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0308 - mae: 0.0302 - mape: 9003.7646 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.5172\n",
      "Epoch 36/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 8986.6465\n",
      "Epoch 00036: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0307 - mae: 0.0301 - mape: 8984.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.7190\n",
      "Epoch 37/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 9036.8193\n",
      "Epoch 00037: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0307 - mae: 0.0301 - mape: 9017.4414 - val_loss: 0.0397 - val_mae: 0.0391 - val_mape: 6.9239\n",
      "Epoch 38/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0304 - mae: 0.0298 - mape: 8871.0625\n",
      "Epoch 00038: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0305 - mae: 0.0299 - mape: 8852.0391 - val_loss: 0.0418 - val_mae: 0.0413 - val_mape: 7.1686\n",
      "Epoch 39/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8882.9082\n",
      "Epoch 00039: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8881.2744 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.0621\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8929.6758\n",
      "Epoch 00040: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8910.5225 - val_loss: 0.0390 - val_mae: 0.0384 - val_mape: 6.7820\n",
      "Epoch 41/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8876.4180\n",
      "Epoch 00041: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8857.3779 - val_loss: 0.0391 - val_mae: 0.0385 - val_mape: 6.9458\n",
      "Epoch 42/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8984.8389\n",
      "Epoch 00042: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8956.7549 - val_loss: 0.0413 - val_mae: 0.0407 - val_mape: 7.6354\n",
      "Epoch 43/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0301 - mae: 0.0295 - mape: 8793.3711\n",
      "Epoch 00043: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0301 - mae: 0.0295 - mape: 8783.1338 - val_loss: 0.0435 - val_mae: 0.0429 - val_mape: 7.4288\n",
      "Epoch 44/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8822.2676- ETA: 6s - loss: 0.0301 - mae: 0.0 - ETA: 1s - loss: 0.0302 - mae:\n",
      "Epoch 00044: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8811.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.9922\n",
      "Epoch 45/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0298 - mae: 0.0292 - mape: 8836.8652\n",
      "Epoch 00045: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0298 - mae: 0.0292 - mape: 8800.5830 - val_loss: 0.0593 - val_mae: 0.0587 - val_mape: 11.2009\n",
      "Epoch 46/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0287 - mae: 0.0281 - mape: 8818.6738\n",
      "Epoch 00046: val_loss improved from 0.03736 to 0.03728, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0046-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0287 - mae: 0.0281 - mape: 8791.1104 - val_loss: 0.0373 - val_mae: 0.0367 - val_mape: 6.3479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0269 - mape: 8903.1064\n",
      "Epoch 00047: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0276 - mae: 0.0269 - mape: 8866.5479 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 7.3343\n",
      "Epoch 48/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0264 - mape: 8938.7715\n",
      "Epoch 00048: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0270 - mae: 0.0264 - mape: 8919.5977 - val_loss: 0.0419 - val_mae: 0.0413 - val_mape: 6.8349\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0265 - mape: 8954.8320\n",
      "Epoch 00049: val_loss improved from 0.03728 to 0.03701, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0049-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0272 - mae: 0.0265 - mape: 8926.8408 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2637\n",
      "Epoch 50/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0263 - mape: 8925.1689\n",
      "Epoch 00050: val_loss did not improve from 0.03701\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0269 - mae: 0.0263 - mape: 8914.7754 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2835\n",
      "Epoch 51/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0262 - mape: 8973.1758\n",
      "Epoch 00051: val_loss improved from 0.03701 to 0.03644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0051-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0269 - mae: 0.0262 - mape: 8936.3379 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3367\n",
      "Epoch 52/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0267 - mae: 0.0260 - mape: 8984.7070\n",
      "Epoch 00052: val_loss improved from 0.03644 to 0.03603, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0267 - mae: 0.0260 - mape: 8947.8154 - val_loss: 0.0360 - val_mae: 0.0354 - val_mape: 6.2870\n",
      "Epoch 53/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.0259 - mape: 8895.4316\n",
      "Epoch 00053: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0265 - mae: 0.0259 - mape: 8893.7969 - val_loss: 0.0433 - val_mae: 0.0426 - val_mape: 6.9243\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0258 - mape: 8529.0977\n",
      "Epoch 00054: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0264 - mae: 0.0258 - mape: 8883.7363 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.3396\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8902.2764\n",
      "Epoch 00055: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8883.1807 - val_loss: 0.0377 - val_mae: 0.0371 - val_mape: 6.5213\n",
      "Epoch 56/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8834.2520\n",
      "Epoch 00056: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8806.6387 - val_loss: 0.0378 - val_mae: 0.0372 - val_mape: 6.4096\n",
      "Epoch 57/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0255 - mape: 8899.8281\n",
      "Epoch 00057: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0261 - mae: 0.0255 - mape: 8889.4648 - val_loss: 0.0365 - val_mae: 0.0358 - val_mape: 6.4844\n",
      "Epoch 58/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0257 - mape: 8935.8350\n",
      "Epoch 00058: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0263 - mae: 0.0257 - mape: 8899.1426 - val_loss: 0.0389 - val_mae: 0.0382 - val_mape: 6.9754\n",
      "Epoch 59/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0255 - mape: 8985.3438\n",
      "Epoch 00059: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0262 - mae: 0.0255 - mape: 8966.0693 - val_loss: 0.0448 - val_mae: 0.0442 - val_mape: 7.1970\n",
      "Epoch 60/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0254 - mape: 8698.2939\n",
      "Epoch 00060: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0261 - mae: 0.0254 - mape: 9026.8086 - val_loss: 0.0371 - val_mae: 0.0365 - val_mape: 6.4728\n",
      "Epoch 61/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0256 - mape: 8944.7266\n",
      "Epoch 00061: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0262 - mae: 0.0256 - mape: 8943.0820 - val_loss: 0.0376 - val_mae: 0.0369 - val_mape: 6.4816\n",
      "Epoch 62/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0253 - mape: 8861.0674\n",
      "Epoch 00062: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0259 - mae: 0.0253 - mape: 8833.3691 - val_loss: 0.0376 - val_mae: 0.0370 - val_mape: 6.7046\n",
      "Epoch 63/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0252 - mape: 8937.2539\n",
      "Epoch 00063: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0259 - mae: 0.0252 - mape: 8909.3174 - val_loss: 0.0441 - val_mae: 0.0434 - val_mape: 7.0393\n",
      "Epoch 64/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0251 - mape: 8836.8535\n",
      "Epoch 00064: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0257 - mae: 0.0251 - mape: 8826.5635 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.1966\n",
      "Epoch 65/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8864.3887\n",
      "Epoch 00065: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8845.3740 - val_loss: 0.0403 - val_mae: 0.0396 - val_mape: 6.6836\n",
      "Epoch 66/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0251 - mape: 8864.2441\n",
      "Epoch 00066: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0258 - mae: 0.0251 - mape: 8853.9238 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.2001\n",
      "Epoch 67/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8919.5137- ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8391\n",
      "Epoch 00067: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0256 - mae: 0.0249 - mape: 8900.3799 - val_loss: 0.0435 - val_mae: 0.0428 - val_mape: 6.9965\n",
      "Epoch 68/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0249 - mape: 8921.0537\n",
      "Epoch 00068: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0255 - mae: 0.0249 - mape: 8910.6650 - val_loss: 0.0368 - val_mae: 0.0361 - val_mape: 6.4016\n",
      "Epoch 69/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8904.1221\n",
      "Epoch 00069: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8885.0215 - val_loss: 0.0366 - val_mae: 0.0359 - val_mape: 6.4782\n",
      "Epoch 70/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0248 - mape: 8509.3799\n",
      "Epoch 00070: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0255 - mae: 0.0248 - mape: 8848.4863 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0247 - mape: 8894.0742\n",
      "Epoch 00071: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0254 - mae: 0.0247 - mape: 8874.9961 - val_loss: 0.0447 - val_mae: 0.0440 - val_mape: 7.2170\n",
      "Epoch 72/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0246 - mape: 8940.8789\n",
      "Epoch 00072: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0253 - mae: 0.0246 - mape: 8921.6992 - val_loss: 0.0399 - val_mae: 0.0393 - val_mape: 6.6394\n",
      "Epoch 73/80\n",
      "31840/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0246 - mape: 8697.8066"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d15c63a445f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BNTzwnub2Eh-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 577.97\n",
      "Median Absolute Error (MedAE): 474.07\n",
      "Mean Squared Error (MSE): 554533.54\n",
      "Root Mean Squared Error (RMSE): 744.67\n",
      "Mean Absolute Percentage Error (MAPE): 5.86 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.58 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tYU2gaBI4Yl"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fz9kU3b6IvyO"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5'\n",
    "start_epoch= 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2bMbPx8RJKL4"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5T4Ho3YJJKTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "14t41QwoJKZ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0241 - mae: 0.0235 - mape: 8787.5186\n",
      "Epoch 00001: val_loss improved from inf to 0.03371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 57s 2ms/sample - loss: 0.0241 - mae: 0.0235 - mape: 8785.9023 - val_loss: 0.0337 - val_mae: 0.0331 - val_mape: 5.6219\n",
      "Epoch 2/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8818.7939\n",
      "Epoch 00002: val_loss improved from 0.03371 to 0.03268, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 8808.5244 - val_loss: 0.0327 - val_mae: 0.0321 - val_mape: 5.5019\n",
      "Epoch 3/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 8832.4248\n",
      "Epoch 00003: val_loss improved from 0.03268 to 0.03238, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 8830.8018 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4341\n",
      "Epoch 4/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 8807.1309\n",
      "Epoch 00004: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 8805.5117 - val_loss: 0.0330 - val_mae: 0.0324 - val_mape: 5.4931\n",
      "Epoch 5/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0212 - mape: 8834.0068\n",
      "Epoch 00005: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0212 - mape: 8832.3828 - val_loss: 0.0333 - val_mae: 0.0328 - val_mape: 5.5382\n",
      "Epoch 6/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8771.0771\n",
      "Epoch 00006: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8769.4648 - val_loss: 0.0326 - val_mae: 0.0321 - val_mape: 5.4661\n",
      "Epoch 7/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8793.5381\n",
      "Epoch 00007: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8783.2979 - val_loss: 0.0325 - val_mae: 0.0320 - val_mape: 5.4550\n",
      "Epoch 8/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8746.4922\n",
      "Epoch 00008: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8744.8848 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.4740\n",
      "Epoch 9/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8799.6211\n",
      "Epoch 00009: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8789.3721 - val_loss: 0.0332 - val_mae: 0.0327 - val_mape: 5.5137\n",
      "Epoch 10/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8783.9893\n",
      "Epoch 00010: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8782.3740 - val_loss: 0.0327 - val_mae: 0.0322 - val_mape: 5.4856\n",
      "Epoch 11/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8814.5137\n",
      "Epoch 00011: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8804.2490 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.4683\n",
      "Epoch 12/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0208 - mape: 8829.3320\n",
      "Epoch 00012: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0213 - mae: 0.0208 - mape: 8819.0498 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4762\n",
      "Epoch 13/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8766.2891\n",
      "Epoch 00013: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8764.6777 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.6098\n",
      "Epoch 14/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0208 - mape: 8807.6914\n",
      "Epoch 00014: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0208 - mape: 8797.4355 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4454\n",
      "Epoch 15/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8763.3545\n",
      "Epoch 00015: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8761.7432 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4502\n",
      "Epoch 16/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0207 - mape: 8800.9727\n",
      "Epoch 00016: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0207 - mape: 8790.7227 - val_loss: 0.0336 - val_mae: 0.0331 - val_mape: 5.5508\n",
      "Epoch 17/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8805.7637\n",
      "Epoch 00017: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 8804.1465 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5136\n",
      "Epoch 18/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8785.8975\n",
      "Epoch 00018: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8775.6660 - val_loss: 0.0324 - val_mae: 0.0320 - val_mape: 5.4366\n",
      "Epoch 19/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8766.3740\n",
      "Epoch 00019: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8756.1650 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5053\n",
      "Epoch 20/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8869.6514- ETA: 1s - loss: 0.0209 - mae: 0.0205\n",
      "Epoch 00020: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8859.3301 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5394\n",
      "Epoch 21/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8719.9990\n",
      "Epoch 00021: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8718.3955 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.4912\n",
      "Epoch 22/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8816.5059\n",
      "Epoch 00022: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8814.8848 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5017\n",
      "Epoch 23/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8851.2988\n",
      "Epoch 00023: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8840.9902 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5257\n",
      "Epoch 24/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8849.9473\n",
      "Epoch 00024: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8839.6475 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8417.9062\n",
      "Epoch 00025: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8762.0635 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5165\n",
      "Epoch 26/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8781.5459\n",
      "Epoch 00026: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8771.3203 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5305\n",
      "Epoch 27/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8805.2725\n",
      "Epoch 00027: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8795.0176 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5373\n",
      "Epoch 28/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8814.1201\n",
      "Epoch 00028: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8803.8555 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5812\n",
      "Epoch 29/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8835.2373\n",
      "Epoch 00029: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8833.6133 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.5439\n",
      "Epoch 30/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8756.9062\n",
      "Epoch 00030: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8746.7100 - val_loss: 0.0335 - val_mae: 0.0331 - val_mape: 5.5782\n",
      "Epoch 31/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8848.8691\n",
      "Epoch 00031: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8847.2422 - val_loss: 0.0345 - val_mae: 0.0342 - val_mape: 5.6903\n",
      "Epoch 32/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8786.9775\n",
      "Epoch 00032: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8785.3623 - val_loss: 0.0348 - val_mae: 0.0344 - val_mape: 5.7179\n",
      "Epoch 33/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8790.1523\n",
      "Epoch 00033: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8779.9150 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5215\n",
      "Epoch 34/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8812.7725\n",
      "Epoch 00034: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8811.1523 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5479\n",
      "Epoch 35/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8778.3477-\n",
      "Epoch 00035: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8776.7334 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5697\n",
      "Epoch 36/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8766.7051\n",
      "Epoch 00036: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8756.4961 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.6073\n",
      "Epoch 37/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0200 - mape: 8769.1572\n",
      "Epoch 00037: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0200 - mape: 8767.5449 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5441\n",
      "Epoch 38/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0201 - mape: 8747.3877\n",
      "Epoch 00038: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0201 - mape: 8745.7793 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5797\n",
      "Epoch 39/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0200 - mape: 8836.6064\n",
      "Epoch 00039: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0200 - mape: 8834.9814 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5334\n",
      "Epoch 40/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8814.8398\n",
      "Epoch 00040: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8804.5742 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5985\n",
      "Epoch 41/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8839.9746\n",
      "Epoch 00041: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8829.6797 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5734\n",
      "Epoch 42/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0198 - mape: 8831.3848\n",
      "Epoch 00042: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0202 - mae: 0.0198 - mape: 8829.7607 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5997\n",
      "Epoch 43/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0201 - mae: 0.0198 - mape: 8783.8525\n",
      "Epoch 00043: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0201 - mae: 0.0198 - mape: 8782.2383 - val_loss: 0.0337 - val_mae: 0.0334 - val_mape: 5.6261\n",
      "Epoch 44/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0197 - mape: 8822.6426\n",
      "Epoch 00044: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0197 - mape: 8821.0205 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.6350\n",
      "Epoch 45/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8898.9219\n",
      "Epoch 00045: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8888.5576 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5640\n",
      "Epoch 46/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8848.5547\n",
      "Epoch 00046: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8846.9277 - val_loss: 0.0326 - val_mae: 0.0323 - val_mape: 5.5130\n",
      "Epoch 47/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8915.1211\n",
      "Epoch 00047: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8904.7383 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5152\n",
      "Epoch 48/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8884.6523\n",
      "Epoch 00048: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8874.3057 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5568\n",
      "Epoch 49/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0195 - mape: 8901.6914\n",
      "Epoch 00049: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0195 - mape: 8891.3252 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8912.0732\n",
      "Epoch 00050: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8901.6934 - val_loss: 0.0326 - val_mae: 0.0322 - val_mape: 5.5117\n",
      "Epoch 51/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0194 - mape: 8897.9141\n",
      "Epoch 00051: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0194 - mape: 8896.2773 - val_loss: 0.0339 - val_mae: 0.0336 - val_mape: 5.6620\n",
      "Epoch 52/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8820.4697\n",
      "Epoch 00052: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8810.1973 - val_loss: 0.0336 - val_mae: 0.0333 - val_mape: 5.6221\n",
      "Epoch 53/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8908.1738\n",
      "Epoch 00053: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8906.5361 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5511\n",
      "Epoch 54/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8871.0322\n",
      "Epoch 00054: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8869.4014 - val_loss: 0.0333 - val_mae: 0.0330 - val_mape: 5.5471\n",
      "Epoch 55/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8876.8662\n",
      "Epoch 00055: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8866.5283 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5071\n",
      "Epoch 56/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8917.7432- ETA: 2s - loss: 0.0196 - ma\n",
      "Epoch 00056: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8916.1035 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.5076\n",
      "Epoch 57/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8886.0176\n",
      "Epoch 00057: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8884.3838 - val_loss: 0.0330 - val_mae: 0.0327 - val_mape: 5.6687\n",
      "Epoch 58/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8843.5547\n",
      "Epoch 00058: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0195 - mae: 0.0192 - mape: 8833.2559 - val_loss: 0.0340 - val_mae: 0.0337 - val_mape: 5.6366\n",
      "Epoch 59/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8897.1895\n",
      "Epoch 00059: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8886.8271 - val_loss: 0.0344 - val_mae: 0.0340 - val_mape: 5.7448\n",
      "Epoch 60/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8878.5566\n",
      "Epoch 00060: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0192 - mape: 8868.2227 - val_loss: 0.0338 - val_mae: 0.0334 - val_mape: 5.6100\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.92\n",
      "Median Absolute Error (MedAE): 341.57\n",
      "Mean Squared Error (MSE): 329961.29\n",
      "Root Mean Squared Error (RMSE): 574.42\n",
      "Mean Absolute Percentage Error (MAPE): 4.31 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.33 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPZ0OMjDJhnO"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UZgZB020JqhL"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5'\n",
    "start_epoch= 53+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ssm3plQeJn6F"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "u-3UhfYuJlge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "9b_pJN2ZJKff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9133.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03257, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 9s 283us/sample - loss: 0.0214 - mae: 0.0208 - mape: 8890.3867 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4449\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8733.1416\n",
      "Epoch 00002: val_loss improved from 0.03257 to 0.03256, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8869.8643 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4437\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9107.9521\n",
      "Epoch 00003: val_loss improved from 0.03256 to 0.03242, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8865.1064 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8707.8369\n",
      "Epoch 00004: val_loss improved from 0.03242 to 0.03233, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8851.4385 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4285\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9089.2246\n",
      "Epoch 00005: val_loss improved from 0.03233 to 0.03228, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8846.9043 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8326.9922\n",
      "Epoch 00006: val_loss improved from 0.03228 to 0.03226, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8841.4912 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8705.7549\n",
      "Epoch 00007: val_loss improved from 0.03226 to 0.03223, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8839.4424 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4216\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9068.8096\n",
      "Epoch 00008: val_loss improved from 0.03223 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8827.0107 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4208\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8693.2275  ETA: 1s - loss: 0.0213 - mae: 0.020\n",
      "Epoch 00009: val_loss improved from 0.03222 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8828.1250 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8320.9189\n",
      "Epoch 00010: val_loss improved from 0.03222 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8823.5527 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8678.0664\n",
      "Epoch 00011: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8818.6221 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9058.5664\n",
      "Epoch 00012: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8817.0547 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.8018\n",
      "Epoch 00013: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8819.4121 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9055.2490\n",
      "Epoch 00014: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8813.7979 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9056.6045\n",
      "Epoch 00015: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8815.1211 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4180\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9046.7842\n",
      "Epoch 00016: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8805.6162 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.0303\n",
      "Epoch 00017: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8811.1191 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4184\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8288.7080\n",
      "Epoch 00018: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8806.9727 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9051.8076\n",
      "Epoch 00019: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.4658 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8302.6875\n",
      "Epoch 00020: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.8389 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4196\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.1914\n",
      "Epoch 00021: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8803.5537 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8289.9629\n",
      "Epoch 00022: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8808.2617 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.5645\n",
      "Epoch 00023: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8807.9199 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8657.6855\n",
      "Epoch 00024: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.0996 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8660.9229- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00025: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4287 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4194\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.9697\n",
      "Epoch 00026: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.0215 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4190\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.5674- ETA: 1s - loss: 0.0211 - mae: 0.0205\n",
      "Epoch 00027: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.4775 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.4648\n",
      "Epoch 00028: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4198\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8674.4756\n",
      "Epoch 00029: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.5635 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7906.3037\n",
      "Epoch 00030: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.4629 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.0000\n",
      "Epoch 00031: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.7725 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4202\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9045.3105\n",
      "Epoch 00032: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.1299 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.4668  ETA: 2s - loss: 0.0211 - m\n",
      "Epoch 00033: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.4404 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8285.9170\n",
      "Epoch 00034: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.4795 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4204\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.9082\n",
      "Epoch 00035: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6709 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8681.3047- ETA: 2s - loss: 0.0210 - mae: 0.\n",
      "Epoch 00036: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.3027 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1055- ETA: 2s - loss: 0.0211 - mae: \n",
      "Epoch 00037: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.6387 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.3789\n",
      "Epoch 00038: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4212\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7911.5845\n",
      "Epoch 00039: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.6006 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4207\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7988\n",
      "Epoch 00040: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.7549 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4209\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.4160\n",
      "Epoch 00041: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8797.7119 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4218\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.8086\n",
      "Epoch 00042: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.4561 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9040.8076- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8763.4\n",
      "Epoch 00043: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.7510 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8671.4062\n",
      "Epoch 00044: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9043.8984\n",
      "Epoch 00045: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.7754 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.5801\n",
      "Epoch 00046: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4297 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4199\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.4580\n",
      "Epoch 00047: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.2539 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4226\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8656.3926\n",
      "Epoch 00048: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8795.1895 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4227\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.8350\n",
      "Epoch 00049: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6064 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9053.7900\n",
      "Epoch 00050: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8812.3799 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4222\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.2178\n",
      "Epoch 00051: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.1719 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4229\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.2744\n",
      "Epoch 00052: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.0586 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4240\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7900\n",
      "Epoch 00053: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.5859 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8664.6738\n",
      "Epoch 00054: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.6367 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4230\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8658.2246\n",
      "Epoch 00055: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.2617 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4231\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9039.0703\n",
      "Epoch 00056: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.0605 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.2510\n",
      "Epoch 00057: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.1816 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4238\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.6846\n",
      "Epoch 00058: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.4893 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4235\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.6758\n",
      "Epoch 00059: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4250\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.4639\n",
      "Epoch 00060: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.2031 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.5479\n",
      "Epoch 00061: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8801.4482 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.7363\n",
      "Epoch 00062: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.5029 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1113- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7955. - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8069\n",
      "Epoch 00063: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8803.5488 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.6602\n",
      "Epoch 00064: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8809.3477 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8292.0371\n",
      "Epoch 00065: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8800.8613 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4243\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8661.9170\n",
      "Epoch 00066: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.5088 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4233\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.0117\n",
      "Epoch 00067: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.7666 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.6230- ETA: 2s - loss: 0.0212 - mae:\n",
      "Epoch 00068: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.3867 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4254\n",
      "Epoch 69/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8285.9189\n",
      "Epoch 00069: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.3457 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4237\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.5889\n",
      "Epoch 00070: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.3418 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.3438\n",
      "Epoch 00071: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8803.1846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4249\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.1455\n",
      "Epoch 00072: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.9678 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8665.8174\n",
      "Epoch 00073: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.1689 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.3008\n",
      "Epoch 00074: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.6846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.2236\n",
      "Epoch 00075: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.9883 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4261\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.5234  ETA: 2s - loss: 0.0209 - \n",
      "Epoch 00076: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4239\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9045.4512\n",
      "Epoch 00077: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.2715 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4265\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.2568\n",
      "Epoch 00078: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0527 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4256\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9055.4580\n",
      "Epoch 00079: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.0088 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4271\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9054.0859\n",
      "Epoch 00080: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8812.6738 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9052.4775\n",
      "Epoch 00081: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.1074 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4263\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.4385\n",
      "Epoch 00082: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.1035 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.2695\n",
      "Epoch 00083: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.9912 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8680.7773\n",
      "Epoch 00084: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8681.9316\n",
      "Epoch 00085: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.9023 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4258\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.9326\n",
      "Epoch 00086: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.5898 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.4629\n",
      "Epoch 00087: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.1172 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4259\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8666.3701\n",
      "Epoch 00088: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2188 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8664.1152\n",
      "Epoch 00089: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.5791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4258\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.7578\n",
      "Epoch 00090: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4267\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8674.5449\n",
      "Epoch 00091: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.6641 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4264\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.3330\n",
      "Epoch 00092: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.8057 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8680.4805\n",
      "Epoch 00093: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8807.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4268\n",
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8669.4492\n",
      "Epoch 00094: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0020 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8670.0195\n",
      "Epoch 00095: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.6592 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.8623\n",
      "Epoch 00096: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8808.5645 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4274\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8297.3535\n",
      "Epoch 00097: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8814.3789 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.8828\n",
      "Epoch 00098: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8803.7236 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8692.5449\n",
      "Epoch 00099: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8821.6729 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4253\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.0117  ETA: 2s - loss: 0.0212 - mae: 0.0207 - mape - ETA: 1s - loss: 0.0211 - mae: 0.0206 - mape: 13501.83 - ETA: 1s - loss: 0.0211 - mae: 0.0\n",
      "Epoch 00100: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.5918 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8291.2773\n",
      "Epoch 00101: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.7295 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4274\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.1973\n",
      "Epoch 00102: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.7637 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4269\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.2109\n",
      "Epoch 00103: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.1260 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.0146\n",
      "Epoch 00104: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.8604 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8309.9893\n",
      "Epoch 00105: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.9658 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8300.4561\n",
      "Epoch 00106: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.9160 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8687.0166\n",
      "Epoch 00107: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8820.6562 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4257\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.9922  ETA: 1s - loss: 0.021\n",
      "Epoch 00108: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8818.7070 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7918.0552\n",
      "Epoch 00109: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8806.1182 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4266\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9054.2461\n",
      "Epoch 00110: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8812.8340 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 111/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.2012\n",
      "Epoch 00111: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8809.0146 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 112/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.0439\n",
      "Epoch 00112: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.5566 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 113/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8305.8838\n",
      "Epoch 00113: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.7695 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4286\n",
      "Epoch 114/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9051.7646\n",
      "Epoch 00114: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.4189 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4268\n",
      "Epoch 115/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8671.9346\n",
      "Epoch 00115: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.5635 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4283\n",
      "Epoch 116/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.4092  ETA: 2s - loss: \n",
      "Epoch 00116: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.9219 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 117/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8685.8936- ETA: 1s - loss: 0.0209 - mae: 0.02\n",
      "Epoch 00117: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8817.3330 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 118/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8302.0400\n",
      "Epoch 00118: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8812.6797 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4707\n",
      "Epoch 00119: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.0967 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 120/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8304.8125\n",
      "Epoch 00120: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8818.3047 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4265\n",
      "Epoch 121/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9058.0273\n",
      "Epoch 00121: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5293 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4270\n",
      "Epoch 122/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8682.1084  ETA: 1s - loss: 0.0208 - mae:\n",
      "Epoch 00122: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.7100 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 123/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9050.8926\n",
      "Epoch 00123: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.5742 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4287\n",
      "Epoch 124/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.2852- ETA: 2s - loss: 0.0209 - mae:\n",
      "Epoch 00124: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.8037 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4277\n",
      "Epoch 125/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.9121\n",
      "Epoch 00125: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.4541 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 126/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7553.2466\n",
      "Epoch 00126: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.8301 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 127/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.8857\n",
      "Epoch 00127: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.4443 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 128/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.3145\n",
      "Epoch 00128: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.7705 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 129/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9062.5234\n",
      "Epoch 00129: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8820.9062 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 130/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8310.6270\n",
      "Epoch 00130: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.6572 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4276\n",
      "Epoch 131/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8676.7832\n",
      "Epoch 00131: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8813.8525 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 132/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.6250\n",
      "Epoch 00132: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1289 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 133/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.2627\n",
      "Epoch 00133: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.7959 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 134/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9385\n",
      "Epoch 00134: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.0283 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 135/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9990\n",
      "Epoch 00135: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.8438 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 136/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9050.7119- ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8224.\n",
      "Epoch 00136: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.3926 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 137/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9065.7979\n",
      "Epoch 00137: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.0713 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 138/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.0449\n",
      "Epoch 00138: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.1436 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4327\n",
      "Epoch 139/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.4678\n",
      "Epoch 00139: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.9834 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 140/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.8877\n",
      "Epoch 00140: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.4102 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 141/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.1895\n",
      "Epoch 00141: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.6465 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4293\n",
      "Epoch 142/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9068.6006\n",
      "Epoch 00142: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8826.8496 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4311\n",
      "Epoch 143/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.5254\n",
      "Epoch 00143: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.0791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8303.7139\n",
      "Epoch 00144: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.9434 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 145/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9057.6035\n",
      "Epoch 00145: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1113 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 146/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8295.9824\n",
      "Epoch 00146: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 147/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8299.4932- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00147: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.4980 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 148/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8677.7822\n",
      "Epoch 00148: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.8496 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 149/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4521\n",
      "Epoch 00149: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8807.3672 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4317\n",
      "Epoch 150/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8687.6270\n",
      "Epoch 00150: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.2520 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 151/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7550.1221\n",
      "Epoch 00151: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5537 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4318\n",
      "Epoch 152/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8665.8770\n",
      "Epoch 00152: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.2012 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4307\n",
      "Epoch 153/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8305.3477\n",
      "Epoch 00153: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.7168 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 154/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8664.9102\n",
      "Epoch 00154: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8803.5449 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 155/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.1807\n",
      "Epoch 00155: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.1172 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4323\n",
      "Epoch 156/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9061.2051\n",
      "Epoch 00156: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8819.6104 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 157/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9058.3799\n",
      "Epoch 00157: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.8613 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4316\n",
      "Epoch 158/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9053.1445\n",
      "Epoch 00158: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8811.7646 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 159/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.0205\n",
      "Epoch 00159: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8813.5781 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 160/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8673.6445\n",
      "Epoch 00160: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8812.8994 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4306\n",
      "Epoch 161/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.0654\n",
      "Epoch 00161: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8804.1348 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 162/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8678.7412\n",
      "Epoch 00162: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8817.2510 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 163/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9058.5049  ETA: 1s - loss: 0.0209 - mae: 0.0204 - map\n",
      "Epoch 00163: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.9990 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4288\n",
      "Epoch 164/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8671.3730\n",
      "Epoch 00164: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8807.5947 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 165/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8304.8350\n",
      "Epoch 00165: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8825.1436 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4294\n",
      "Epoch 166/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7930.7822\n",
      "Epoch 00166: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.1318 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 167/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8676.1992  ETA: 1s - loss: 0.0209 - mae: 0.0203 - mape:  - ETA: 1s - loss: 0.0208 - mae: 0.0202 - mape: - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9\n",
      "Epoch 00167: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.6123 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8682.7363\n",
      "Epoch 00168: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.8896 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 169/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9065.5938\n",
      "Epoch 00169: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8823.9004 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 170/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9051.8340\n",
      "Epoch 00170: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.4756 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 171/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8678.6602- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8967.800\n",
      "Epoch 00171: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8815.0049 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 172/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.9189\n",
      "Epoch 00172: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.3682 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 173/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9055.6904\n",
      "Epoch 00173: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2402 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 174/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.9043- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8588.017\n",
      "Epoch 00174: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.3857 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 175/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.7666\n",
      "Epoch 00175: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3809 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 176/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8673.9277\n",
      "Epoch 00176: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.3027 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4343\n",
      "Epoch 177/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.8848\n",
      "Epoch 00177: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2305 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4321\n",
      "Epoch 178/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9054.9736  ETA: 1s - loss: 0.0208 - mae: 0.0203 \n",
      "Epoch 00178: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.5439 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4330\n",
      "Epoch 179/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8675.4736\n",
      "Epoch 00179: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8809.1797 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4327\n",
      "Epoch 180/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9061.8086\n",
      "Epoch 00180: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2021 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4329\n",
      "Epoch 181/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9060.0869\n",
      "Epoch 00181: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5107 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4319\n",
      "Epoch 182/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.6191\n",
      "Epoch 00182: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5000 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 183/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.7178\n",
      "Epoch 00183: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2656 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4324\n",
      "Epoch 184/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8291.2705\n",
      "Epoch 00184: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.1533 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4331\n",
      "Epoch 185/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.7324\n",
      "Epoch 00185: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.8145 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 186/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.2549\n",
      "Epoch 00186: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8817.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4305\n",
      "Epoch 187/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.2168\n",
      "Epoch 00187: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8819.0938 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4315\n",
      "Epoch 188/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.7939\n",
      "Epoch 00188: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.2949 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 189/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.9834\n",
      "Epoch 00189: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.4678 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 190/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.2012\n",
      "Epoch 00190: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7588 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4332\n",
      "Epoch 191/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8679.1182\n",
      "Epoch 00191: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.3721 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4325\n",
      "Epoch 192/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8303.0508\n",
      "Epoch 00192: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7334 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 193/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8677.2891\n",
      "Epoch 00193: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.0742 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4315\n",
      "Epoch 194/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9051.3916\n",
      "Epoch 00194: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.0596 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4353\n",
      "Epoch 195/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 7911.0254\n",
      "Epoch 00195: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.6299 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 196/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9053.1436\n",
      "Epoch 00196: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.7627 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4345\n",
      "Epoch 197/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9056.7646- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 73\n",
      "Epoch 00197: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.2822 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 198/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8293.9717\n",
      "Epoch 00198: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3867 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4346\n",
      "Epoch 199/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.1162\n",
      "Epoch 00199: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.7646 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4328\n",
      "Epoch 200/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.9668- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 796\n",
      "Epoch 00200: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8808.9336 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4344\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.57\n",
      "Median Absolute Error (MedAE): 338.68\n",
      "Mean Squared Error (MSE): 332645.96\n",
      "Root Mean Squared Error (RMSE): 576.75\n",
      "Mean Absolute Percentage Error (MAPE): 4.3 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.3 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-0013-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aW5pp6LD-1"
   },
   "source": [
    "# E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "j9RoOmzuLgHR"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B2H5iBwQLfhj"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "k06fUuK7LerZ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mB2kXVVCLRBA"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sizKJbygLNBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Q6uDSkmdLJlR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.1973 - mae: 0.0794 - mape: 9692.2637\n",
      "Epoch 00001: val_loss improved from inf to 0.10644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0001-loss0.11.h5\n",
      "32614/32614 [==============================] - 19s 584us/sample - loss: 0.1972 - mae: 0.0794 - mape: 9680.9932 - val_loss: 0.1064 - val_mae: 0.0766 - val_mape: 12.9892\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0557 - mae: 0.0421 - mape: 9249.9512\n",
      "Epoch 00002: val_loss improved from 0.10644 to 0.05840, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0002-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0556 - mae: 0.0420 - mape: 9230.1133 - val_loss: 0.0584 - val_mae: 0.0523 - val_mape: 8.7301\n",
      "Epoch 3/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0416 - mae: 0.0375 - mape: 9262.3770\n",
      "Epoch 00003: val_loss improved from 0.05840 to 0.05401, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0003-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0416 - mae: 0.0375 - mape: 9224.3594 - val_loss: 0.0540 - val_mae: 0.0503 - val_mape: 8.7703\n",
      "Epoch 4/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0398 - mae: 0.0367 - mape: 9258.6729\n",
      "Epoch 00004: val_loss improved from 0.05401 to 0.04371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 431us/sample - loss: 0.0398 - mae: 0.0367 - mape: 9247.8994 - val_loss: 0.0437 - val_mae: 0.0407 - val_mape: 6.7483\n",
      "Epoch 5/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0336 - mape: 9132.0303  ETA: 4s - \n",
      "Epoch 00005: val_loss did not improve from 0.04371\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0362 - mae: 0.0336 - mape: 9121.3975 - val_loss: 0.0456 - val_mae: 0.0433 - val_mape: 7.2883\n",
      "Epoch 6/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0332 - mape: 9247.4834\n",
      "Epoch 00006: val_loss improved from 0.04371 to 0.03429, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0358 - mae: 0.0332 - mape: 9227.6475 - val_loss: 0.0343 - val_mae: 0.0317 - val_mape: 5.5244\n",
      "Epoch 7/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0334 - mae: 0.0314 - mape: 8832.6689\n",
      "Epoch 00007: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0334 - mae: 0.0314 - mape: 9193.4805 - val_loss: 0.0770 - val_mae: 0.0752 - val_mape: 10.8461\n",
      "Epoch 8/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0309 - mape: 9279.7900\n",
      "Epoch 00008: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0326 - mae: 0.0309 - mape: 9241.6865 - val_loss: 0.0911 - val_mae: 0.0895 - val_mape: 12.6461\n",
      "Epoch 9/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0301 - mape: 9240.0088\n",
      "Epoch 00009: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0315 - mae: 0.0301 - mape: 9211.1299 - val_loss: 0.0497 - val_mae: 0.0483 - val_mape: 7.3868\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0289 - mape: 8889.9570\n",
      "Epoch 00010: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0303 - mae: 0.0289 - mape: 9243.3398 - val_loss: 0.0727 - val_mae: 0.0714 - val_mape: 10.6575\n",
      "Epoch 11/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0290 - mape: 9401.6436\n",
      "Epoch 00011: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0303 - mae: 0.0290 - mape: 9372.2676 - val_loss: 0.0601 - val_mae: 0.0589 - val_mape: 8.9566\n",
      "Epoch 12/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0292 - mae: 0.0280 - mape: 9176.0518\n",
      "Epoch 00012: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0292 - mae: 0.0280 - mape: 9147.3730 - val_loss: 0.0464 - val_mae: 0.0452 - val_mape: 7.1787\n",
      "Epoch 13/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0291 - mae: 0.0279 - mape: 9234.7246\n",
      "Epoch 00013: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0291 - mae: 0.0279 - mape: 9214.9170 - val_loss: 0.0398 - val_mae: 0.0387 - val_mape: 6.3188\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.0278 - mape: 9259.1611\n",
      "Epoch 00014: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0290 - mae: 0.0278 - mape: 9221.1396 - val_loss: 0.0613 - val_mae: 0.0602 - val_mape: 8.9035\n",
      "Epoch 15/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0285 - mae: 0.0274 - mape: 9332.2979\n",
      "Epoch 00015: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0285 - mae: 0.0274 - mape: 9293.9756 - val_loss: 0.0448 - val_mae: 0.0438 - val_mape: 6.8179\n",
      "Epoch 16/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9270.8516- ETA: 0s - loss: 0.0278 - mae: 0.0268 - ma\n",
      "Epoch 00016: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9269.1475 - val_loss: 0.0455 - val_mae: 0.0446 - val_mape: 7.0034\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9212.3809\n",
      "Epoch 00017: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9210.6865 - val_loss: 0.0670 - val_mae: 0.0661 - val_mape: 9.5463\n",
      "Epoch 18/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9337.9092\n",
      "Epoch 00018: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9308.7227 - val_loss: 0.0456 - val_mae: 0.0447 - val_mape: 7.0638\n",
      "Epoch 19/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9224.4463\n",
      "Epoch 00019: val_loss improved from 0.03429 to 0.03249, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0019-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9213.7041 - val_loss: 0.0325 - val_mae: 0.0316 - val_mape: 5.3572\n",
      "Epoch 20/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9302.4258\n",
      "Epoch 00020: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9264.2266 - val_loss: 0.0382 - val_mae: 0.0373 - val_mape: 6.0539\n",
      "Epoch 21/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9244.2197\n",
      "Epoch 00021: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 436us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9206.2666 - val_loss: 0.0409 - val_mae: 0.0400 - val_mape: 6.2748\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0260 - mape: 9348.6699\n",
      "Epoch 00022: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0269 - mae: 0.0260 - mape: 9337.7842 - val_loss: 0.0328 - val_mae: 0.0319 - val_mape: 5.4575\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9254.0684\n",
      "Epoch 00023: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0266 - mae: 0.0257 - mape: 9252.3672 - val_loss: 0.0552 - val_mae: 0.0543 - val_mape: 8.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9298.8604\n",
      "Epoch 00024: val_loss improved from 0.03249 to 0.03114, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0024-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0265 - mae: 0.0257 - mape: 9260.6738 - val_loss: 0.0311 - val_mae: 0.0303 - val_mape: 5.1846\n",
      "Epoch 25/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0254 - mape: 9225.4922\n",
      "Epoch 00025: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0262 - mae: 0.0254 - mape: 9223.7969 - val_loss: 0.0519 - val_mae: 0.0511 - val_mape: 7.9640\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0254 - mape: 9168.1641\n",
      "Epoch 00026: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0263 - mae: 0.0254 - mape: 9139.5078 - val_loss: 0.0324 - val_mae: 0.0315 - val_mape: 5.4029\n",
      "Epoch 27/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0252 - mape: 9292.8506\n",
      "Epoch 00027: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 435us/sample - loss: 0.0261 - mae: 0.0252 - mape: 9282.0293 - val_loss: 0.0368 - val_mae: 0.0359 - val_mape: 5.8316\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0250 - mape: 9257.2148\n",
      "Epoch 00028: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0259 - mae: 0.0250 - mape: 9246.4346 - val_loss: 0.0322 - val_mae: 0.0313 - val_mape: 5.3347\n",
      "Epoch 29/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0249 - mape: 9201.8809\n",
      "Epoch 00029: val_loss improved from 0.03114 to 0.03017, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0257 - mae: 0.0249 - mape: 9173.1152 - val_loss: 0.0302 - val_mae: 0.0293 - val_mape: 5.0982\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0246 - mape: 9224.1299\n",
      "Epoch 00030: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0255 - mae: 0.0246 - mape: 9195.2969 - val_loss: 0.0316 - val_mae: 0.0308 - val_mape: 5.3475\n",
      "Epoch 31/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0247 - mape: 9221.6504\n",
      "Epoch 00031: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0256 - mae: 0.0247 - mape: 9219.9551 - val_loss: 0.0383 - val_mae: 0.0375 - val_mape: 6.0447\n",
      "Epoch 32/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9185.3818\n",
      "Epoch 00032: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0253 - mae: 0.0244 - mape: 9156.6689 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.6061\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9151.5020\n",
      "Epoch 00033: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0252 - mae: 0.0243 - mape: 9104.9512 - val_loss: 0.0435 - val_mae: 0.0426 - val_mape: 6.6821\n",
      "Epoch 34/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0240 - mape: 9209.8779\n",
      "Epoch 00034: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0248 - mae: 0.0240 - mape: 9163.0439 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 5.9090\n",
      "Epoch 35/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0243 - mape: 9190.4863\n",
      "Epoch 00035: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0251 - mae: 0.0243 - mape: 9170.7715 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.4763\n",
      "Epoch 36/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0241 - mape: 9201.8359\n",
      "Epoch 00036: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0250 - mae: 0.0241 - mape: 9182.0967 - val_loss: 0.0393 - val_mae: 0.0384 - val_mape: 6.1283\n",
      "Epoch 37/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0247 - mae: 0.0238 - mape: 9064.8945\n",
      "Epoch 00037: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0247 - mae: 0.0238 - mape: 9063.2285 - val_loss: 0.0322 - val_mae: 0.0314 - val_mape: 5.4603\n",
      "Epoch 38/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0236 - mape: 9153.4014\n",
      "Epoch 00038: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0245 - mae: 0.0236 - mape: 9142.7412 - val_loss: 0.0396 - val_mae: 0.0387 - val_mape: 6.1510\n",
      "Epoch 39/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9212.5186\n",
      "Epoch 00039: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9183.7207 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5752\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0237 - mape: 9298.7363\n",
      "Epoch 00040: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0246 - mae: 0.0237 - mape: 9278.7891 - val_loss: 0.0358 - val_mae: 0.0349 - val_mape: 5.7883\n",
      "Epoch 41/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9290.7383\n",
      "Epoch 00041: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9252.5859 - val_loss: 0.0359 - val_mae: 0.0350 - val_mape: 6.1253\n",
      "Epoch 42/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0244 - mae: 0.0235 - mape: 9100.7422\n",
      "Epoch 00042: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0244 - mae: 0.0235 - mape: 9099.0762 - val_loss: 0.0365 - val_mae: 0.0356 - val_mape: 5.8232\n",
      "Epoch 43/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.0231 - mape: 9206.7373\n",
      "Epoch 00043: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0240 - mae: 0.0231 - mape: 9186.9854 - val_loss: 0.0428 - val_mae: 0.0419 - val_mape: 6.5949\n",
      "Epoch 44/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.0229 - mape: 9111.2773\n",
      "Epoch 00044: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0238 - mae: 0.0229 - mape: 9109.6025 - val_loss: 0.0335 - val_mae: 0.0326 - val_mape: 5.5624\n",
      "Epoch 45/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0229 - mape: 9251.8613\n",
      "Epoch 00045: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0229 - mape: 9250.1602 - val_loss: 0.0383 - val_mae: 0.0374 - val_mape: 6.0213\n",
      "Epoch 46/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0228 - mape: 9249.3652\n",
      "Epoch 00046: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9229.5244 - val_loss: 0.0470 - val_mae: 0.0461 - val_mape: 7.1372\n",
      "Epoch 47/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0228 - mape: 9240.2764\n",
      "Epoch 00047: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9220.4561 - val_loss: 0.0355 - val_mae: 0.0346 - val_mape: 5.8071\n",
      "Epoch 48/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0227 - mape: 9228.7842\n",
      "Epoch 00048: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0236 - mae: 0.0227 - mape: 9227.0869 - val_loss: 0.0335 - val_mae: 0.0327 - val_mape: 5.7051\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0225 - mape: 9308.8867\n",
      "Epoch 00049: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0234 - mae: 0.0225 - mape: 9279.7959 - val_loss: 0.0391 - val_mae: 0.0382 - val_mape: 6.2820\n",
      "Epoch 50/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0223 - mape: 8910.0205\n",
      "Epoch 00050: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0232 - mae: 0.0223 - mape: 9253.5254 - val_loss: 0.0347 - val_mae: 0.0338 - val_mape: 5.7545\n",
      "Epoch 51/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0224 - mape: 9279.0811\n",
      "Epoch 00051: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0233 - mae: 0.0224 - mape: 9277.3750 - val_loss: 0.0390 - val_mae: 0.0381 - val_mape: 6.1803\n",
      "Epoch 52/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0222 - mape: 9221.8008\n",
      "Epoch 00052: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0231 - mae: 0.0222 - mape: 9202.0244 - val_loss: 0.0355 - val_mae: 0.0347 - val_mape: 5.8992\n",
      "Epoch 53/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0218 - mape: 9313.6279\n",
      "Epoch 00053: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0227 - mae: 0.0219 - mape: 9284.5176 - val_loss: 0.0346 - val_mae: 0.0337 - val_mape: 5.8458\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0221 - mape: 9235.9512\n",
      "Epoch 00054: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0230 - mae: 0.0221 - mape: 9207.0791 - val_loss: 0.0399 - val_mae: 0.0391 - val_mape: 6.3575\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.0219 - mape: 9211.0430\n",
      "Epoch 00055: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0228 - mae: 0.0219 - mape: 9191.2910 - val_loss: 0.0397 - val_mae: 0.0388 - val_mape: 6.2600\n",
      "Epoch 56/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0217 - mape: 9251.5020\n",
      "Epoch 00056: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0226 - mae: 0.0217 - mape: 9240.7275 - val_loss: 0.0405 - val_mae: 0.0397 - val_mape: 6.3565\n",
      "Epoch 57/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 9213.3115\n",
      "Epoch 00057: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9166.4385 - val_loss: 0.0343 - val_mae: 0.0334 - val_mape: 5.7472\n",
      "Epoch 58/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0216 - mape: 9254.9170\n",
      "Epoch 00058: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9253.2207 - val_loss: 0.0376 - val_mae: 0.0367 - val_mape: 6.0893\n",
      "Epoch 59/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0220 - mape: 9354.7930\n",
      "Epoch 00059: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0230 - mae: 0.0220 - mape: 9325.5488 - val_loss: 0.0341 - val_mae: 0.0332 - val_mape: 5.7046\n",
      "Epoch 60/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0214 - mape: 9235.1680\n",
      "Epoch 00060: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0222 - mae: 0.0214 - mape: 9188.1836 - val_loss: 0.0366 - val_mae: 0.0358 - val_mape: 5.9825\n",
      "Epoch 61/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0215 - mape: 9196.2959\n",
      "Epoch 00061: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0223 - mae: 0.0215 - mape: 9167.5479 - val_loss: 0.0399 - val_mae: 0.0390 - val_mape: 6.2796\n",
      "Epoch 62/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0213 - mape: 9107.4619\n",
      "Epoch 00062: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0222 - mae: 0.0213 - mape: 9087.9248 - val_loss: 0.0385 - val_mae: 0.0376 - val_mape: 6.1628\n",
      "Epoch 63/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0213 - mape: 9210.2637\n",
      "Epoch 00063: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0221 - mae: 0.0213 - mape: 9172.4463 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5972\n",
      "Epoch 64/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9124.6074\n",
      "Epoch 00064: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9122.9297 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 6.0281\n",
      "Epoch 65/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0212 - mape: 9226.6611\n",
      "Epoch 00065: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0220 - mae: 0.0212 - mape: 9188.7695 - val_loss: 0.0377 - val_mae: 0.0369 - val_mape: 6.2115\n",
      "Epoch 66/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9052.7314\n",
      "Epoch 00066: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9051.0664 - val_loss: 0.0405 - val_mae: 0.0396 - val_mape: 6.4286\n",
      "Epoch 67/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0209 - mape: 9083.4209\n",
      "Epoch 00067: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0218 - mae: 0.0209 - mape: 9072.8418 - val_loss: 0.0365 - val_mae: 0.0357 - val_mape: 6.0204\n",
      "Epoch 68/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9076.5205\n",
      "Epoch 00068: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9039.2510 - val_loss: 0.0342 - val_mae: 0.0334 - val_mape: 5.6559\n",
      "Epoch 69/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0208 - mape: 9033.5391\n",
      "Epoch 00069: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0217 - mae: 0.0208 - mape: 9005.3018 - val_loss: 0.0396 - val_mae: 0.0388 - val_mape: 6.4502\n",
      "Epoch 70/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9120.9199\n",
      "Epoch 00070: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9083.4629 - val_loss: 0.0361 - val_mae: 0.0353 - val_mape: 5.8008\n",
      "Epoch 71/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9097.3145\n",
      "Epoch 00071: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9095.6416 - val_loss: 0.0361 - val_mae: 0.0352 - val_mape: 5.9642\n",
      "Epoch 72/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0206 - mape: 9088.2021\n",
      "Epoch 00072: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0206 - mape: 9086.5312 - val_loss: 0.0380 - val_mae: 0.0372 - val_mape: 6.1542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9105.6396\n",
      "Epoch 00073: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9059.3135 - val_loss: 0.0434 - val_mae: 0.0426 - val_mape: 6.7060\n",
      "Epoch 74/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0205 - mape: 9066.7041\n",
      "Epoch 00074: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0205 - mape: 9065.0361 - val_loss: 0.0346 - val_mae: 0.0338 - val_mape: 5.7416\n",
      "Epoch 75/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0205 - mape: 8978.4404\n",
      "Epoch 00075: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0213 - mae: 0.0205 - mape: 8967.9941 - val_loss: 0.0348 - val_mae: 0.0340 - val_mape: 5.9216\n",
      "Epoch 76/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9057.9648\n",
      "Epoch 00076: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0210 - mae: 0.0202 - mape: 9056.2988 - val_loss: 0.0398 - val_mae: 0.0390 - val_mape: 6.3300\n",
      "Epoch 77/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0203 - mape: 8981.2236\n",
      "Epoch 00077: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0212 - mae: 0.0203 - mape: 8979.5723 - val_loss: 0.0451 - val_mae: 0.0444 - val_mape: 7.0400\n",
      "Epoch 78/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9038.3955\n",
      "Epoch 00078: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0210 - mae: 0.0202 - mape: 8992.4121 - val_loss: 0.0360 - val_mae: 0.0353 - val_mape: 5.9399\n",
      "Epoch 79/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0201 - mape: 8948.2764\n",
      "Epoch 00079: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0209 - mae: 0.0201 - mape: 8920.3037 - val_loss: 0.0350 - val_mae: 0.0342 - val_mape: 5.7982\n",
      "Epoch 80/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0200 - mape: 8990.1953\n",
      "Epoch 00080: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0208 - mae: 0.0200 - mape: 8962.0898 - val_loss: 0.0358 - val_mae: 0.0350 - val_mape: 5.8451\n"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "cW3NWXtXLDA-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 420.56\n",
      "Median Absolute Error (MedAE): 333.06\n",
      "Mean Squared Error (MSE): 313586.16\n",
      "Root Mean Squared Error (RMSE): 559.99\n",
      "Mean Absolute Percentage Error (MAPE): 4.19 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.25 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5'\n",
    "start_epoch= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0228 - mape: 9138.6191- ETA: 3s - loss: 0.023\n",
      "Epoch 00001: val_loss improved from inf to 0.03148, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 56s 2ms/sample - loss: 0.0235 - mae: 0.0228 - mape: 9136.9395 - val_loss: 0.0315 - val_mae: 0.0307 - val_mape: 5.2145\n",
      "Epoch 2/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0225 - mape: 9164.0752\n",
      "Epoch 00002: val_loss improved from 0.03148 to 0.03063, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0232 - mae: 0.0225 - mape: 9162.3906 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1229\n",
      "Epoch 3/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0224 - mape: 9196.9521- ETA: 3s - loss: 0.0\n",
      "Epoch 00003: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0231 - mae: 0.0224 - mape: 9195.2607 - val_loss: 0.0314 - val_mae: 0.0307 - val_mape: 5.2105\n",
      "Epoch 4/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0223 - mape: 9131.6660\n",
      "Epoch 00004: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0230 - mae: 0.0223 - mape: 9121.0312 - val_loss: 0.0310 - val_mae: 0.0304 - val_mape: 5.1898\n",
      "Epoch 5/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.0223 - mape: 9159.3213\n",
      "Epoch 00005: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0229 - mae: 0.0223 - mape: 9148.6533 - val_loss: 0.0321 - val_mae: 0.0314 - val_mape: 5.2817\n",
      "Epoch 6/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9180.6631\n",
      "Epoch 00006: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0228 - mae: 0.0221 - mape: 9178.9824 - val_loss: 0.0309 - val_mae: 0.0303 - val_mape: 5.1679\n",
      "Epoch 7/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9175.2949\n",
      "Epoch 00007: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0227 - mae: 0.0221 - mape: 9173.6084 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.2847\n",
      "Epoch 8/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0220 - mape: 9205.9355\n",
      "Epoch 00008: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0226 - mae: 0.0220 - mape: 9195.2217 - val_loss: 0.0307 - val_mae: 0.0301 - val_mape: 5.2188\n",
      "Epoch 9/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0219 - mape: 9181.9873\n",
      "Epoch 00009: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0225 - mae: 0.0219 - mape: 9180.2988 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.2595\n",
      "Epoch 10/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0218 - mape: 9186.6875\n",
      "Epoch 00010: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0224 - mae: 0.0218 - mape: 9184.9990 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2146\n",
      "Epoch 11/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0217 - mape: 9203.6816\n",
      "Epoch 00011: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0223 - mae: 0.0217 - mape: 9192.9629 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.2609\n",
      "Epoch 12/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9241.5742\n",
      "Epoch 00012: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 9239.8750 - val_loss: 0.0311 - val_mae: 0.0305 - val_mape: 5.2300\n",
      "Epoch 13/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0216 - mape: 9247.9932\n",
      "Epoch 00013: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0216 - mape: 9237.2227 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4177\n",
      "Epoch 14/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0215 - mape: 9297.9727\n",
      "Epoch 00014: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0215 - mape: 9296.2627 - val_loss: 0.0307 - val_mae: 0.0302 - val_mape: 5.2173\n",
      "Epoch 15/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0214 - mape: 9263.5205\n",
      "Epoch 00015: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0220 - mae: 0.0214 - mape: 9261.8164 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2651\n",
      "Epoch 16/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 9198.6846\n",
      "Epoch 00016: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 9196.9941 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.3174\n",
      "Epoch 17/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 9260.9404\n",
      "Epoch 00017: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 9250.1543 - val_loss: 0.0309 - val_mae: 0.0304 - val_mape: 5.2860\n",
      "Epoch 18/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0212 - mape: 9216.3271\n",
      "Epoch 00018: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0212 - mape: 9214.6318 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3829\n",
      "Epoch 19/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0211 - mape: 9211.0586\n",
      "Epoch 00019: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0211 - mape: 9200.3320 - val_loss: 0.0341 - val_mae: 0.0336 - val_mape: 5.5767\n",
      "Epoch 20/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0209 - mape: 9201.0938\n",
      "Epoch 00020: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0215 - mae: 0.0209 - mape: 9190.3779 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3684\n",
      "Epoch 21/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9219.9883\n",
      "Epoch 00021: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0214 - mae: 0.0208 - mape: 9218.2930 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3618\n",
      "Epoch 22/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 9187.7080\n",
      "Epoch 00022: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 9186.0186 - val_loss: 0.0316 - val_mae: 0.0310 - val_mape: 5.3197\n",
      "Epoch 23/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9196.0430\n",
      "Epoch 00023: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0206 - mape: 9194.3525 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2367\n",
      "Epoch 24/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9184.2910\n",
      "Epoch 00024: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 9173.5947 - val_loss: 0.0315 - val_mae: 0.0309 - val_mape: 5.2920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9166.1699\n",
      "Epoch 00025: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0210 - mae: 0.0205 - mape: 9155.4951 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.3492\n",
      "Epoch 26/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9205.6270\n",
      "Epoch 00026: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9203.9385 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2379\n",
      "Epoch 27/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9117.5986\n",
      "Epoch 00027: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9115.9219 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4364\n",
      "Epoch 28/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9204.3340\n",
      "Epoch 00028: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9193.6143 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2468\n",
      "Epoch 29/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9128.1553\n",
      "Epoch 00029: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9126.4766 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3819\n",
      "Epoch 30/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0202 - mape: 9126.3975\n",
      "Epoch 00030: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0207 - mae: 0.0202 - mape: 9124.7197 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.3127\n",
      "Epoch 31/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9127.3574\n",
      "Epoch 00031: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9125.6787 - val_loss: 0.0314 - val_mae: 0.0308 - val_mape: 5.3096\n",
      "Epoch 32/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9139.1289\n",
      "Epoch 00032: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9137.4482 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3143\n",
      "Epoch 33/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0200 - mape: 9129.0029\n",
      "Epoch 00033: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0205 - mae: 0.0200 - mape: 9127.3242 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3023\n",
      "Epoch 34/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9085.1152\n",
      "Epoch 00034: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9083.4443 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3421\n",
      "Epoch 35/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9161.0391\n",
      "Epoch 00035: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9159.3545 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2771\n",
      "Epoch 36/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0198 - mape: 9147.9355\n",
      "Epoch 00036: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0203 - mae: 0.0198 - mape: 9137.2803 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3513\n",
      "Epoch 37/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9170.7051- ETA: 4s - loss: 0. - ETA: 2s - loss: 0.0203\n",
      "Epoch 00037: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9169.0186 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2897\n",
      "Epoch 38/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9186.7959\n",
      "Epoch 00038: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9176.0967 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2613\n",
      "Epoch 39/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0195 - mape: 9121.9736\n",
      "Epoch 00039: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0200 - mae: 0.0195 - mape: 9120.2959 - val_loss: 0.0348 - val_mae: 0.0343 - val_mape: 5.6403\n",
      "Epoch 40/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0194 - mape: 9143.8574\n",
      "Epoch 00040: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0199 - mae: 0.0194 - mape: 9142.1768 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3696\n",
      "Epoch 41/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0192 - mape: 9166.4883\n",
      "Epoch 00041: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0197 - mae: 0.0192 - mape: 9155.8125 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3686\n",
      "Epoch 42/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9157.3496\n",
      "Epoch 00042: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9146.6846 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3299\n",
      "Epoch 43/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9169.7188\n",
      "Epoch 00043: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9159.0391 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2889\n",
      "Epoch 44/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0190 - mape: 9109.2627\n",
      "Epoch 00044: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0195 - mae: 0.0190 - mape: 9107.5879 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3664\n",
      "Epoch 45/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0189 - mape: 9101.7988\n",
      "Epoch 00045: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0195 - mae: 0.0189 - mape: 9100.1250 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4306\n",
      "Epoch 46/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0194 - mae: 0.0189 - mape: 9152.3506\n",
      "Epoch 00046: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0194 - mae: 0.0189 - mape: 9150.6680 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.3450\n",
      "Epoch 47/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0193 - mae: 0.0187 - mape: 9092.9639\n",
      "Epoch 00047: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0193 - mae: 0.0187 - mape: 9091.2920 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3590\n",
      "Epoch 48/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.0186 - mape: 9166.8369\n",
      "Epoch 00048: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0191 - mae: 0.0186 - mape: 9165.1514 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4518\n",
      "Epoch 49/200\n",
      "11776/32614 [=========>....................] - ETA: 32s - loss: 0.0191 - mae: 0.0186 - mape: 7337.6323"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-048f600f12b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 412.44\n",
      "Median Absolute Error (MedAE): 328.08\n",
      "Mean Squared Error (MSE): 301264.17\n",
      "Root Mean Squared Error (RMSE): 548.88\n",
      "Mean Absolute Percentage Error (MAPE): 4.12 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.19 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5'\n",
    "start_epoch= 30+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0218 - mape: 9021.4141\n",
      "Epoch 00001: val_loss improved from inf to 0.03075, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 7s 222us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9155.7246 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1276\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9392.5967\n",
      "Epoch 00002: val_loss improved from 0.03075 to 0.03067, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9142.1660 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1187\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 8595.9766\n",
      "Epoch 00003: val_loss improved from 0.03067 to 0.03054, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9143.0762 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1073\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9385.6611\n",
      "Epoch 00004: val_loss improved from 0.03054 to 0.03050, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9135.4072 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1036\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9377.5986\n",
      "Epoch 00005: val_loss improved from 0.03050 to 0.03046, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9127.5752 - val_loss: 0.0305 - val_mae: 0.0297 - val_mape: 5.0995\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8606.6309\n",
      "Epoch 00006: val_loss improved from 0.03046 to 0.03042, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9137.0928 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0965\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9375.1191\n",
      "Epoch 00007: val_loss improved from 0.03042 to 0.03041, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9125.1416 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9374.5898\n",
      "Epoch 00008: val_loss improved from 0.03041 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9124.6318 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0924\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8210.0459\n",
      "Epoch 00009: val_loss improved from 0.03038 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9126.5254 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0921\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8572.6328  ETA: 1s - loss: 0.0225 - mae: 0.0218 \n",
      "Epoch 00010: val_loss improved from 0.03038 to 0.03035, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9118.1846 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0894\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8207.2930\n",
      "Epoch 00011: val_loss did not improve from 0.03035\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9111.9365 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0895\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9357.2139\n",
      "Epoch 00012: val_loss improved from 0.03035 to 0.03034, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0012-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.7373 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0893\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8967.2090\n",
      "Epoch 00013: val_loss improved from 0.03034 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0859 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0887\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8584.7832\n",
      "Epoch 00014: val_loss improved from 0.03033 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0014-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9115.9170 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9359.8037\n",
      "Epoch 00015: val_loss improved from 0.03033 to 0.03030, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0015-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.2441 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0861\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8970.0029\n",
      "Epoch 00016: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9116.9443 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0868\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8182.7358\n",
      "Epoch 00017: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.9473 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0872\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9360.5244\n",
      "Epoch 00018: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.9551 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8964.7793\n",
      "Epoch 00019: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0879 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8972.5371\n",
      "Epoch 00020: val_loss improved from 0.03030 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0020-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9106.9902 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0856\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8572.1016\n",
      "Epoch 00021: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9105.2520 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0876\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9354.3223\n",
      "Epoch 00022: val_loss improved from 0.03028 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.9277 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0858\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9355.4004- ETA: 1s - loss: 0.0223 - mae: 0.0216 - mape: 9009 - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 869\n",
      "Epoch 00023: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9105.9697 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9358.2070\n",
      "Epoch 00024: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9108.6885 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0860\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8967.9619\n",
      "Epoch 00025: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.4453 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8956.2822\n",
      "Epoch 00026: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.1289 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0879\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9353.0088\n",
      "Epoch 00027: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9103.6240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.5430\n",
      "Epoch 00028: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1504 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0871\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9354.4570\n",
      "Epoch 00029: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.0547 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0886\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.4277\n",
      "Epoch 00030: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.0498 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2637\n",
      "Epoch 00031: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0391 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0890\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.9912\n",
      "Epoch 00032: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.1416 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0896\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8960.5801\n",
      "Epoch 00033: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7783 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0913\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9346.8809\n",
      "Epoch 00034: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9097.6689 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8965.5186\n",
      "Epoch 00035: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8569.2568\n",
      "Epoch 00036: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1602 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0915\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.2012  ETA: 2s - loss: 0.02\n",
      "Epoch 00037: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1436 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0920\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8574.7637\n",
      "Epoch 00038: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.4717 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0902\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.5205\n",
      "Epoch 00039: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.2197 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.8828\n",
      "Epoch 00040: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.5615 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.7637\n",
      "Epoch 00041: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.3633 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0908\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9359.6875\n",
      "Epoch 00042: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0914\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.9414\n",
      "Epoch 00043: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9099.0850 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3818\n",
      "Epoch 00044: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9072 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0957\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2998\n",
      "Epoch 00045: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0811 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8428- ETA: 2s - loss: 0.0223 - mae: 0\n",
      "Epoch 00046: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9570 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0959\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.5625\n",
      "Epoch 00047: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.1621 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0927\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8184\n",
      "Epoch 00048: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.2930 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0955\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8958.4678- ETA: 2s - loss: 0.0224 - mae: 0.0217 - mape: 4847. - ETA: 1s - loss: 0.0222 - mae: 0.0215 \n",
      "Epoch 00049: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.8057 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0937\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8977.0430\n",
      "Epoch 00050: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.9639 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0950\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.6865\n",
      "Epoch 00051: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.2705 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0954\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8964.1064\n",
      "Epoch 00052: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.1123 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0919\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.2607\n",
      "Epoch 00053: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.9170 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0989\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8959.4570\n",
      "Epoch 00054: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.2852 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0969\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.4199- ETA: 2s - loss: 0.0225 - mae: 0\n",
      "Epoch 00055: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.2910 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0990\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6221\n",
      "Epoch 00056: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1025 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0925\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.5264\n",
      "Epoch 00057: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8096 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1009\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8963.5762\n",
      "Epoch 00058: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.3652 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1010\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8584.6045\n",
      "Epoch 00059: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9108.2227 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0963\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.8242\n",
      "Epoch 00060: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.3945 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0996\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3730  ETA: 2s - loss: 0.0\n",
      "Epoch 00061: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8975 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1003\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.2998\n",
      "Epoch 00062: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9113.6562 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0987\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8572.1582\n",
      "Epoch 00063: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.7090 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1015\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6914\n",
      "Epoch 00064: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1013\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.9131\n",
      "Epoch 00065: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.5430 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1008\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9361.4287\n",
      "Epoch 00066: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9111.8457 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1032\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8968.2002\n",
      "Epoch 00067: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.6504 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1028\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.6250\n",
      "Epoch 00068: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7305 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8962.6729- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8456.346\n",
      "Epoch 00069: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9107.0010 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1072\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.2842\n",
      "Epoch 00070: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.8740 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1061\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.1514\n",
      "Epoch 00071: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9108.2100 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1067\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9350.1494\n",
      "Epoch 00072: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9100.8525 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.5088\n",
      "Epoch 00073: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9105.0918 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1070\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8561.8984\n",
      "Epoch 00074: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1073\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.5430\n",
      "Epoch 00075: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.3232 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1101\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.6846\n",
      "Epoch 00076: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.0889 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1018\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.6553\n",
      "Epoch 00077: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.3877 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1131\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.4307\n",
      "Epoch 00078: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.1025 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1066\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8972.5918\n",
      "Epoch 00079: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7490 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.0195\n",
      "Epoch 00080: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.7041 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1059\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.1738\n",
      "Epoch 00081: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.8281 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1086\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9359.2744\n",
      "Epoch 00082: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7461 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1125\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9355.9951\n",
      "Epoch 00083: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.5498 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1091\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.0264\n",
      "Epoch 00084: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9111.4297 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8973.9590\n",
      "Epoch 00085: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.7012 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1117\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8980.7080\n",
      "Epoch 00086: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.9180 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1181\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9362.3105\n",
      "Epoch 00087: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.6982 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1146\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.5547\n",
      "Epoch 00088: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1924 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.5771\n",
      "Epoch 00089: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.1777 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1158\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.7559\n",
      "Epoch 00090: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.4727 - val_loss: 0.0304 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9344.5049\n",
      "Epoch 00091: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9095.3486 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1201\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.3213\n",
      "Epoch 00092: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.6084 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1109\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.4619\n",
      "Epoch 00093: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1074 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9357.2207\n",
      "Epoch 00094: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.7432 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1149\n",
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.8730\n",
      "Epoch 00095: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.5840 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9366.0117\n",
      "Epoch 00096: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9116.2979 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1090\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 7807.1338- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape:\n",
      "Epoch 00097: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9097.1768 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8969.1562\n",
      "Epoch 00098: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9108.1855 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1151\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9350.9482  ETA\n",
      "Epoch 00099: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.6289 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1170\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.7568\n",
      "Epoch 00100: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.3867 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1185\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.9570\n",
      "Epoch 00101: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.5547 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.7695- ETA: 1s - loss: 0.0222 - mae: 0.0215 - ma\n",
      "Epoch 00102: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.2959 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1222\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9360.5537\n",
      "Epoch 00103: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.9805 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1187\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8967.6729\n",
      "Epoch 00104: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1299 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1163\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.5879\n",
      "Epoch 00105: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.3262 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1221\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.9297\n",
      "Epoch 00106: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9114.2559 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1136\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8575.9648\n",
      "Epoch 00107: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.5449 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1218\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.1768\n",
      "Epoch 00108: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9098.9189 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1236\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.6055\n",
      "Epoch 00109: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.2510 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1214\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0214 - mape: 9352.8691\n",
      "Epoch 00110: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0221 - mae: 0.0215 - mape: 9103.5098 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1184\n",
      "Epoch 111/200\n",
      "23552/32614 [====================>.........] - ETA: 0s - loss: 0.0220 - mae: 0.0213 - mape: 9332.0186"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-aa18ff9589c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 411.01\n",
      "Median Absolute Error (MedAE): 326.15\n",
      "Mean Squared Error (MSE): 300114.5\n",
      "Root Mean Squared Error (RMSE): 547.83\n",
      "Mean Absolute Percentage Error (MAPE): 4.11 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.17 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-0022-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
