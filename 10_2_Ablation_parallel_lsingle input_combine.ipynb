{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tWP4R-BZmaZi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\pyimagesearch')\n",
    "from timeseires.CNN1D.GoogLeNet1D import PC\n",
    "from timeseires.utils.to_split import to_split\n",
    "#from timeseires.callbacks.EpochCheckpoint import EpochCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from timeseires.callbacks.TrainingMonitor import TrainingMonitor\n",
    "from timeseires.utils.load_only_multi_step import load_only_multi_step\n",
    "from timeseires.utils.multivariate_multi_step import multivariate_multi_step\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD  \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,MaxPooling1D,Concatenate,AveragePooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "import pandas as pd\n",
    "import time, pickle\n",
    "import pydot\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Input, Reshape, Dense, Lambda\n",
    "from tensorflow.keras.layers import Layer, Flatten, LeakyReLU, concatenate\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12130, 21), (24259, 21))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tr = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_train.csv'\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values \n",
    "\n",
    "path_v = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_validation.csv'\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "\n",
    "path_te = 'C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_test.csv'\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "#.......................................................................................................\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive - Higher Education Commission\\\\HI Research\\\\Datasets\\\\AEP\\\\AEP_scaler.pkl\", 'rb'))\n",
    "df_te.shape,df_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nmYgpwTPnsf4"
   },
   "outputs": [],
   "source": [
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIYjXnV81ZHC",
    "outputId": "4520a0c1-81f1-4cf6-9b17-d15f5ae9c7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.37026524543762207 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X, train_y = multivariate_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24,ahead=0)\n",
    "validation_X, validation_y = multivariate_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24,ahead=0)\n",
    "test_X, test_y = multivariate_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24,ahead=0)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.24781274795532227 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_X_load, train_y_load = load_only_multi_step(train_set, time_steps = time_steps, target_col = 0,target_len=24)\n",
    "validation_X_load, validation_y_load = load_only_multi_step(validation_set, time_steps =time_steps ,target_col=0,target_len=24)\n",
    "test_X_load, test_y_load = load_only_multi_step(test_set, time_steps = time_steps, target_col= 0,target_len=24)\n",
    "print('Time Consumed',time.time()-start,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PbWIXWwSJN3k"
   },
   "outputs": [],
   "source": [
    "#EXP 1 STEPS 24 LOOKBACK 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_goog():   \n",
    "    inputs_combine = Input(shape=(time_steps, num_features))\n",
    "    lstm1   = LSTM(20, return_sequences = True)(inputs_combine)\n",
    "    lstm2   = LSTM(20)(lstm1)\n",
    "    #--------------------------------------------------------\n",
    "   # inputs_load = Input(shape=(time_steps, 1))\n",
    "    l = Bidirectional(LSTM(32, return_sequences=True))(inputs_combine)\n",
    "    l = Bidirectional(LSTM(32, return_sequences=True))(l)\n",
    "    \n",
    "    c11 = PC.inception_module(inputs_combine, 32, 32, 64, 8, 32, \"1a\", 0.01)\n",
    "    c12 = Concatenate(axis=-1)([inputs_combine, c11])\n",
    "    \n",
    "    c21 = PC.inception_module(c12, 64, 64, 128, 16, 64, \"2a\", 0.01)\n",
    "    c22 = Concatenate(axis=-1)([ c11, c21])\n",
    "    \n",
    "    c31 = PC.inception_module(c22, 64, 64, 128, 16, 64, \"3a\", 0.01)\n",
    "    #---------------------------------------------------------\n",
    "    y = Concatenate(axis=-1)([inputs_combine, l, c21, c31])\n",
    "\n",
    "    #y= PC.conv_module(y, 128, 3, 1)\n",
    "    x = Flatten()(y)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([x, lstm2])\n",
    "    \n",
    "    x = Dense(24, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs_combine, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = review_goog()\n",
    "tensorflow.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 24, 21)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_conv (Conv1D)        (None, 24, 32)       704         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_conv (Conv1D)         (None, 24, 8)        176         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_bn (BatchNormalizati (None, 24, 32)       128         1a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_bn (BatchNormalizatio (None, 24, 8)        32          1a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_second1_act (Activation)     (None, 24, 32)       0           1a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third1_act (Activation)      (None, 24, 8)        0           1a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_conv (Conv1D)          (None, 24, 32)       704         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_conv (Conv1D)        (None, 24, 64)       6208        1a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_conv (Conv1D)         (None, 24, 32)       1312        1a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_bn (BatchNormalization (None, 24, 32)       128         1a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_bn (BatchNormalizati (None, 24, 64)       256         1a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_bn (BatchNormalizatio (None, 24, 32)       128         1a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "1a_first_act (Activation)       (None, 24, 32)       0           1a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "1a_second2_act (Activation)     (None, 24, 64)       0           1a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "1a_third2_act (Activation)      (None, 24, 32)       0           1a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "1a_mixed (Concatenate)          (None, 24, 128)      0           1a_first_act[0][0]               \n",
      "                                                                 1a_second2_act[0][0]             \n",
      "                                                                 1a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 24, 149)      0           input_1[0][0]                    \n",
      "                                                                 1a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_conv (Conv1D)        (None, 24, 64)       9600        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_conv (Conv1D)         (None, 24, 16)       2400        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_bn (BatchNormalizati (None, 24, 64)       256         2a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_bn (BatchNormalizatio (None, 24, 16)       64          2a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_second1_act (Activation)     (None, 24, 64)       0           2a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third1_act (Activation)      (None, 24, 16)       0           2a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_conv (Conv1D)          (None, 24, 64)       9600        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_conv (Conv1D)        (None, 24, 128)      24704       2a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_conv (Conv1D)         (None, 24, 64)       5184        2a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_bn (BatchNormalization (None, 24, 64)       256         2a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_bn (BatchNormalizati (None, 24, 128)      512         2a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_bn (BatchNormalizatio (None, 24, 64)       256         2a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "2a_first_act (Activation)       (None, 24, 64)       0           2a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "2a_second2_act (Activation)     (None, 24, 128)      0           2a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "2a_third2_act (Activation)      (None, 24, 64)       0           2a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "2a_mixed (Concatenate)          (None, 24, 256)      0           2a_first_act[0][0]               \n",
      "                                                                 2a_second2_act[0][0]             \n",
      "                                                                 2a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24, 384)      0           1a_mixed[0][0]                   \n",
      "                                                                 2a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_conv (Conv1D)        (None, 24, 64)       24640       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_conv (Conv1D)         (None, 24, 16)       6160        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_bn (BatchNormalizati (None, 24, 64)       256         3a_second1_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_bn (BatchNormalizatio (None, 24, 16)       64          3a_third1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_second1_act (Activation)     (None, 24, 64)       0           3a_second1_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third1_act (Activation)      (None, 24, 16)       0           3a_third1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_conv (Conv1D)          (None, 24, 64)       24640       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_conv (Conv1D)        (None, 24, 128)      24704       3a_second1_act[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_conv (Conv1D)         (None, 24, 64)       5184        3a_third1_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_bn (BatchNormalization (None, 24, 64)       256         3a_first_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_bn (BatchNormalizati (None, 24, 128)      512         3a_second2_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_bn (BatchNormalizatio (None, 24, 64)       256         3a_third2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 24, 64)       13824       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "3a_first_act (Activation)       (None, 24, 64)       0           3a_first_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "3a_second2_act (Activation)     (None, 24, 128)      0           3a_second2_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_third2_act (Activation)      (None, 24, 64)       0           3a_third2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 64)       24832       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3a_mixed (Concatenate)          (None, 24, 256)      0           3a_first_act[0][0]               \n",
      "                                                                 3a_second2_act[0][0]             \n",
      "                                                                 3a_third2_act[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 24, 597)      0           input_1[0][0]                    \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 2a_mixed[0][0]                   \n",
      "                                                                 3a_mixed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 24, 20)       3360        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 14328)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20)           3280        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 14348)        0           flatten[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 24)           344376      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 538,952\n",
      "Trainable params: 537,272\n",
      "Non-trainable params: 1,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9rQrz9B0JWvR"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\2_single_input\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\2_single_input'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "h6NbczAlOpSQ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=24\n",
    "num_features=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NkF7jg_XO346"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-rIEZj0PCPF",
    "outputId": "95edda02-84de-4f8a-83a2-b4d53387049d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = review_goog()\n",
    "    opt = Adam(0.001)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 0.0001)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-o0qgceiPFdI",
    "outputId": "ebb2c588-e73f-4182-baad-2f4c28f3db95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.2189 - mae: 0.0727 - mape: 2331.1746- ETA: 0s - loss: 0.2208 - mae: 0.0731 - mape:\n",
      "Epoch 00001: val_loss improved from inf to 0.04411, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\2_single_input\\E1-cp-0001-loss0.04.h5\n",
      "2652/2652 [==============================] - 63s 24ms/step - loss: 0.2187 - mae: 0.0727 - mape: 2329.5608 - val_loss: 0.0441 - val_mae: 0.0439 - val_mape: 20.6187\n",
      "Epoch 2/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0372 - mae: 0.0370 - mape: 2375.4377- ETA: 3s - loss: 0.0373 - mae: 0.0370  - ETA: 2s -\n",
      "Epoch 00002: val_loss improved from 0.04411 to 0.03916, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\2_single_input\\E1-cp-0002-loss0.04.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0372 - mae: 0.0370 - mape: 2374.6860 - val_loss: 0.0392 - val_mae: 0.0390 - val_mape: 16.9655\n",
      "Epoch 3/40\n",
      "2649/2652 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0352 - mape: 2295.7095\n",
      "Epoch 00003: val_loss improved from 0.03916 to 0.03607, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\2_single_input\\E1-cp-0003-loss0.04.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0353 - mae: 0.0352 - mape: 2293.2598 - val_loss: 0.0361 - val_mae: 0.0360 - val_mape: 16.1074\n",
      "Epoch 4/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0341 - mae: 0.0340 - mape: 2369.7910\n",
      "Epoch 00004: val_loss did not improve from 0.03607\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0341 - mae: 0.0340 - mape: 2369.0400 - val_loss: 0.0373 - val_mae: 0.0372 - val_mape: 16.9927\n",
      "Epoch 5/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0330 - mae: 0.0329 - mape: 2269.0266\n",
      "Epoch 00005: val_loss improved from 0.03607 to 0.03515, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\2_single_input\\E1-cp-0005-loss0.04.h5\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0330 - mae: 0.0329 - mape: 2267.4565 - val_loss: 0.0352 - val_mae: 0.0351 - val_mape: 15.3239\n",
      "Epoch 6/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0322 - mae: 0.0322 - mape: 2273.4102\n",
      "Epoch 00006: val_loss improved from 0.03515 to 0.03452, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\2_single_input\\E1-cp-0006-loss0.03.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0322 - mae: 0.0322 - mape: 2271.8372 - val_loss: 0.0345 - val_mae: 0.0345 - val_mape: 14.6945\n",
      "Epoch 7/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0316 - mae: 0.0316 - mape: 2273.7795\n",
      "Epoch 00007: val_loss improved from 0.03452 to 0.03319, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\2_single_input\\E1-cp-0007-loss0.03.h5\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0316 - mae: 0.0316 - mape: 2272.2048 - val_loss: 0.0332 - val_mae: 0.0332 - val_mape: 14.1310\n",
      "Epoch 8/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0312 - mae: 0.0311 - mape: 2289.0569\n",
      "Epoch 00008: val_loss did not improve from 0.03319\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0312 - mae: 0.0311 - mape: 2288.3313 - val_loss: 0.0385 - val_mae: 0.0385 - val_mape: 16.5111\n",
      "Epoch 9/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0308 - mae: 0.0308 - mape: 2292.1943\n",
      "Epoch 00009: val_loss did not improve from 0.03319\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0308 - mae: 0.0308 - mape: 2290.6067 - val_loss: 0.0357 - val_mae: 0.0357 - val_mape: 15.6686\n",
      "Epoch 10/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0303 - mape: 2154.6985\n",
      "Epoch 00010: val_loss did not improve from 0.03319\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0303 - mae: 0.0303 - mape: 2153.2080 - val_loss: 0.0360 - val_mae: 0.0360 - val_mape: 16.0576\n",
      "Epoch 11/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0300 - mae: 0.0300 - mape: 2129.7000\n",
      "Epoch 00011: val_loss did not improve from 0.03319\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0300 - mae: 0.0300 - mape: 2129.7000 - val_loss: 0.0362 - val_mae: 0.0362 - val_mape: 16.0661\n",
      "Epoch 12/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0296 - mae: 0.0296 - mape: 2090.1682\n",
      "Epoch 00012: val_loss did not improve from 0.03319\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0296 - mae: 0.0296 - mape: 2090.1682 - val_loss: 0.0356 - val_mae: 0.0356 - val_mape: 15.3871\n",
      "Epoch 13/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0293 - mae: 0.0293 - mape: 1893.6888\n",
      "Epoch 00013: val_loss did not improve from 0.03319\n",
      "2652/2652 [==============================] - 59s 22ms/step - loss: 0.0293 - mae: 0.0293 - mape: 1892.3790 - val_loss: 0.0339 - val_mae: 0.0339 - val_mape: 14.1589\n",
      "Epoch 14/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0291 - mae: 0.0291 - mape: 1802.1509\n",
      "Epoch 00014: val_loss did not improve from 0.03319\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0291 - mae: 0.0291 - mape: 1802.1509 - val_loss: 0.0351 - val_mae: 0.0351 - val_mape: 14.9155\n",
      "Epoch 15/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0288 - mae: 0.0288 - mape: 1668.2585\n",
      "Epoch 00015: val_loss improved from 0.03319 to 0.03308, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\2_single_input\\E1-cp-0015-loss0.03.h5\n",
      "2652/2652 [==============================] - 61s 23ms/step - loss: 0.0288 - mae: 0.0288 - mape: 1668.2585 - val_loss: 0.0331 - val_mae: 0.0331 - val_mape: 13.7958\n",
      "Epoch 16/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0285 - mae: 0.0285 - mape: 1515.4951\n",
      "Epoch 00016: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0285 - mae: 0.0285 - mape: 1515.0154 - val_loss: 0.0335 - val_mae: 0.0335 - val_mape: 14.1562\n",
      "Epoch 17/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0282 - mae: 0.0282 - mape: 1141.6399\n",
      "Epoch 00017: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 60s 23ms/step - loss: 0.0282 - mae: 0.0282 - mape: 1141.6399 - val_loss: 0.0356 - val_mae: 0.0356 - val_mape: 15.1321\n",
      "Epoch 18/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0280 - mae: 0.0280 - mape: 946.4149\n",
      "Epoch 00018: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 64s 24ms/step - loss: 0.0280 - mae: 0.0280 - mape: 946.4149 - val_loss: 0.0348 - val_mae: 0.0348 - val_mape: 15.0113\n",
      "Epoch 19/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0278 - mae: 0.0278 - mape: 776.3769\n",
      "Epoch 00019: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 63s 24ms/step - loss: 0.0278 - mae: 0.0278 - mape: 776.3769 - val_loss: 0.0337 - val_mae: 0.0337 - val_mape: 13.8145\n",
      "Epoch 20/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0276 - mape: 675.2916\n",
      "Epoch 00020: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 57s 21ms/step - loss: 0.0276 - mae: 0.0276 - mape: 675.0791 - val_loss: 0.0352 - val_mae: 0.0352 - val_mape: 14.7041\n",
      "Epoch 21/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0274 - mae: 0.0274 - mape: 646.6312\n",
      "Epoch 00021: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 57s 21ms/step - loss: 0.0274 - mae: 0.0274 - mape: 646.6312 - val_loss: 0.0342 - val_mae: 0.0342 - val_mape: 14.6070\n",
      "Epoch 22/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0272 - mape: 543.3492\n",
      "Epoch 00022: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 57s 22ms/step - loss: 0.0272 - mae: 0.0272 - mape: 542.9767 - val_loss: 0.0345 - val_mae: 0.0345 - val_mape: 14.2959\n",
      "Epoch 23/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0270 - mae: 0.0270 - mape: 449.0716\n",
      "Epoch 00023: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0270 - mae: 0.0270 - mape: 449.0716 - val_loss: 0.0348 - val_mae: 0.0348 - val_mape: 14.7883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0268 - mae: 0.0268 - mape: 457.3810\n",
      "Epoch 00024: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0268 - mae: 0.0268 - mape: 457.2378 - val_loss: 0.0344 - val_mae: 0.0344 - val_mape: 14.1266\n",
      "Epoch 25/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0266 - mape: 463.4755 ETA: 0s - loss: 0.0266 - ma\n",
      "Epoch 00025: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0266 - mae: 0.0266 - mape: 463.3311 - val_loss: 0.0344 - val_mae: 0.0344 - val_mape: 14.3008\n",
      "Epoch 26/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0264 - mape: 362.4839\n",
      "Epoch 00026: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 57s 22ms/step - loss: 0.0264 - mae: 0.0264 - mape: 362.2378 - val_loss: 0.0369 - val_mae: 0.0369 - val_mape: 15.8472\n",
      "Epoch 27/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0263 - mape: 386.7776\n",
      "Epoch 00027: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 57s 22ms/step - loss: 0.0263 - mae: 0.0263 - mape: 386.6568 - val_loss: 0.0352 - val_mae: 0.0352 - val_mape: 14.9238\n",
      "Epoch 28/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0260 - mae: 0.0260 - mape: 359.6589\n",
      "Epoch 00028: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 56s 21ms/step - loss: 0.0260 - mae: 0.0260 - mape: 359.4146 - val_loss: 0.0331 - val_mae: 0.0331 - val_mape: 13.8704\n",
      "Epoch 29/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0259 - mape: 330.9469\n",
      "Epoch 00029: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 57s 22ms/step - loss: 0.0259 - mae: 0.0259 - mape: 330.7223 - val_loss: 0.0368 - val_mae: 0.0368 - val_mape: 16.0116\n",
      "Epoch 30/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0258 - mape: 324.7923\n",
      "Epoch 00030: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 57s 22ms/step - loss: 0.0258 - mae: 0.0258 - mape: 341.0161 - val_loss: 0.0355 - val_mae: 0.0355 - val_mape: 14.8650\n",
      "Epoch 31/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0256 - mae: 0.0256 - mape: 340.2726\n",
      "Epoch 00031: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 57s 21ms/step - loss: 0.0256 - mae: 0.0256 - mape: 340.2726 - val_loss: 0.0359 - val_mae: 0.0359 - val_mape: 15.7657\n",
      "Epoch 32/40\n",
      "2651/2652 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0254 - mape: 346.7607\n",
      "Epoch 00032: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 57s 21ms/step - loss: 0.0254 - mae: 0.0254 - mape: 346.6537 - val_loss: 0.0390 - val_mae: 0.0390 - val_mape: 16.9924\n",
      "Epoch 33/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0252 - mape: 387.6132\n",
      "Epoch 00033: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 57s 21ms/step - loss: 0.0252 - mae: 0.0252 - mape: 387.3486 - val_loss: 0.0356 - val_mae: 0.0356 - val_mape: 15.1286\n",
      "Epoch 34/40\n",
      "2652/2652 [==============================] - ETA: 0s - loss: 0.0250 - mae: 0.0250 - mape: 368.8808\n",
      "Epoch 00034: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0250 - mae: 0.0250 - mape: 368.8808 - val_loss: 0.0355 - val_mae: 0.0355 - val_mape: 14.7553\n",
      "Epoch 35/40\n",
      "2650/2652 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0248 - mape: 413.3428\n",
      "Epoch 00035: val_loss did not improve from 0.03308\n",
      "2652/2652 [==============================] - 58s 22ms/step - loss: 0.0248 - mae: 0.0248 - mape: 413.0603 - val_loss: 0.0360 - val_mae: 0.0360 - val_mape: 15.2191\n",
      "Epoch 36/40\n",
      "1981/2652 [=====================>........] - ETA: 14s - loss: 0.0247 - mae: 0.0247 - mape: 284.8521- ETA: 17s - loss: 0.0246 - mae: 0.0246 - m - ETA: 16s - l"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13076\\3648322731.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(x= [train_X],\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X, validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oS6zzoUoPL-A",
    "outputId": "ce1a0f3b-6b76-4980-d717-d3542c18a37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 532.14\n",
      "Median Absolute Error (MedAE): 375.44\n",
      "Mean Squared Error (MSE): 542791.65\n",
      "Root Mean Squared Error (RMSE): 736.74\n",
      "Mean Absolute Percentage Error (MAPE): 3.55 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.63 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\2_single_input\\\\E1-cp-0015-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7y39sVKwLMm"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AVN1WhyzuWFl"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\2_single_input\\\\E2-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\2_single_input\\\\E1-cp-0015-loss0.03.h5'\n",
    "start_epoch= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WBeDTA9lu5Qm"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2j6fmkOvA2O",
    "outputId": "7fc1b23b-4d93-4b54-a4df-921c557b9a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\2_single_input\\E1-cp-0015-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = review_goog()\n",
    "    opt = Adam(0.001)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 0.0001)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rTCJW6IzvLU4",
    "outputId": "f810e1ca-98cc-4de7-a4a3-465e83705c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0252 - mean_absolute_error: 0.0252 - mean_absolute_percentage_error: 1491.6442\n",
      "Epoch 00001: val_loss improved from inf to 0.03182, saving model to D:\\Checkpoints\\2_ieee_access\\Hybrid\\Ablation\\2_single_input\\E2-cp-0001-loss0.03.h5\n",
      "166/166 [==============================] - 66s 398ms/step - loss: 0.0252 - mean_absolute_error: 0.0252 - mean_absolute_percentage_error: 1491.6442 - val_loss: 0.0318 - val_mean_absolute_error: 0.0318 - val_mean_absolute_percentage_error: 13.5519\n",
      "Epoch 2/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1467.5135\n",
      "Epoch 00002: val_loss did not improve from 0.03182\n",
      "166/166 [==============================] - 64s 388ms/step - loss: 0.0246 - mean_absolute_error: 0.0246 - mean_absolute_percentage_error: 1467.5135 - val_loss: 0.0319 - val_mean_absolute_error: 0.0319 - val_mean_absolute_percentage_error: 13.6650\n",
      "Epoch 3/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1420.0828\n",
      "Epoch 00003: val_loss did not improve from 0.03182\n",
      "166/166 [==============================] - 64s 383ms/step - loss: 0.0245 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1420.0828 - val_loss: 0.0320 - val_mean_absolute_error: 0.0320 - val_mean_absolute_percentage_error: 13.6621\n",
      "Epoch 4/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1377.2728\n",
      "Epoch 00004: val_loss did not improve from 0.03182\n",
      "166/166 [==============================] - 63s 381ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - mean_absolute_percentage_error: 1377.2728 - val_loss: 0.0319 - val_mean_absolute_error: 0.0319 - val_mean_absolute_percentage_error: 13.6295\n",
      "Epoch 5/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1332.9370\n",
      "Epoch 00005: val_loss did not improve from 0.03182\n",
      "166/166 [==============================] - 65s 390ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1332.9370 - val_loss: 0.0320 - val_mean_absolute_error: 0.0320 - val_mean_absolute_percentage_error: 13.6823\n",
      "Epoch 6/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1307.2440\n",
      "Epoch 00006: val_loss did not improve from 0.03182\n",
      "166/166 [==============================] - 63s 381ms/step - loss: 0.0243 - mean_absolute_error: 0.0243 - mean_absolute_percentage_error: 1307.2440 - val_loss: 0.0322 - val_mean_absolute_error: 0.0322 - val_mean_absolute_percentage_error: 13.8211\n",
      "Epoch 7/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1293.2292\n",
      "Epoch 00007: val_loss did not improve from 0.03182\n",
      "166/166 [==============================] - 63s 377ms/step - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1293.2292 - val_loss: 0.0321 - val_mean_absolute_error: 0.0321 - val_mean_absolute_percentage_error: 13.7271\n",
      "Epoch 8/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1258.4352\n",
      "Epoch 00008: val_loss did not improve from 0.03182\n",
      "166/166 [==============================] - 68s 411ms/step - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1258.4352 - val_loss: 0.0321 - val_mean_absolute_error: 0.0321 - val_mean_absolute_percentage_error: 13.7118\n",
      "Epoch 9/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1252.3811\n",
      "Epoch 00009: val_loss did not improve from 0.03182\n",
      "166/166 [==============================] - 66s 396ms/step - loss: 0.0242 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1252.3811 - val_loss: 0.0322 - val_mean_absolute_error: 0.0322 - val_mean_absolute_percentage_error: 13.7893\n",
      "Epoch 10/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0241 - mean_absolute_error: 0.0241 - mean_absolute_percentage_error: 1214.6481\n",
      "Epoch 00010: val_loss did not improve from 0.03182\n",
      "166/166 [==============================] - 65s 393ms/step - loss: 0.0241 - mean_absolute_error: 0.0241 - mean_absolute_percentage_error: 1214.6481 - val_loss: 0.0321 - val_mean_absolute_error: 0.0321 - val_mean_absolute_percentage_error: 13.7097\n",
      "Epoch 11/30\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0241 - mean_absolute_error: 0.0241 - mean_absolute_percentage_error: 1203.2847\n",
      "Epoch 00011: val_loss did not improve from 0.03182\n",
      "166/166 [==============================] - 66s 400ms/step - loss: 0.0241 - mean_absolute_error: 0.0241 - mean_absolute_percentage_error: 1203.2847 - val_loss: 0.0321 - val_mean_absolute_error: 0.0321 - val_mean_absolute_percentage_error: 13.6800\n",
      "Epoch 12/30\n",
      " 14/166 [=>............................] - ETA: 52s - loss: 0.0238 - mean_absolute_error: 0.0238 - mean_absolute_percentage_error: 904.4197"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13076\\3128038204.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y_load\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\time2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "verbose = 1 #0\n",
    "batch_size = 512\n",
    "History = model.fit(x= train_X,\n",
    "                        y=train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = ( validation_X, validation_y_load),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qj8HRsBXvf0M",
    "outputId": "084e6fbf-b9cc-46e8-e4d7-808e9031da87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 500.37\n",
      "Median Absolute Error (MedAE): 339.1\n",
      "Mean Squared Error (MSE): 498149.7\n",
      "Root Mean Squared Error (RMSE): 705.8\n",
      "Mean Absolute Percentage Error (MAPE): 3.34 %\n",
      "Median Absolute Percentage Error (MDAPE): 2.37 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (12082, 24)\n",
      "y_pred.shape=  (12082, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\2_ieee_access\\Hybrid\\\\Ablation\\\\2_single_input\\\\E2-cp-0001-loss0.03.h5')\n",
    "#scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict([test_X]) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAuKvjoOG-fA"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QZI2LOvEw6Qq"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E2-cp-0006-loss0.04.h5'\n",
    "start_epoch= 59+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qPy-9EGRw6eR"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beh0i6Vww6pP",
    "outputId": "41024e1d-abfa-4ec6-a3ed-cf5e5fdef9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E2-cp-0006-loss0.04.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y-9y1xTeYAaB",
    "outputId": "3e59db7d-d1fa-41c7-a91e-cf2063e855da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0231 - mape: 8601.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03575, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0001-loss0.04.h5\n",
      "32614/32614 [==============================] - 7s 210us/sample - loss: 0.0235 - mae: 0.0231 - mape: 8729.3066 - val_loss: 0.0358 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0230 - mape: 8563.2227\n",
      "Epoch 00002: val_loss improved from 0.03575 to 0.03574, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0002-loss0.04.h5\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0235 - mae: 0.0230 - mape: 8710.7080 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1938\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8942.2402\n",
      "Epoch 00003: val_loss improved from 0.03574 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0003-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 115us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8703.8408 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8589.9561\n",
      "Epoch 00004: val_loss improved from 0.03573 to 0.03573, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\1\\E3_cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 4s 114us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8711.1074 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1949\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8953.2998\n",
      "Epoch 00005: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8714.5996 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1948\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0230 - mape: 8595.0498\n",
      "Epoch 00006: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8716.8604 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1952\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8175.1738\n",
      "Epoch 00007: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0230 - mape: 8717.2090 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8577.0596\n",
      "Epoch 00008: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8716.0957 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1954\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2109\n",
      "Epoch 00009: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.7314 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2744\n",
      "Epoch 00010: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.3086 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1957\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.7139\n",
      "Epoch 00011: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.5303 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8578.2705\n",
      "Epoch 00012: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8717.8525 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1962\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8602.2910\n",
      "Epoch 00013: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8720.2773 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8968.6553\n",
      "Epoch 00014: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.5400 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1970\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8580.6602\n",
      "Epoch 00015: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8719.8223 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1967\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8623\n",
      "Epoch 00016: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.7832 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1963\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8966.8877\n",
      "Epoch 00017: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8727.8203 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8606.8154\n",
      "Epoch 00018: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0547 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8963.2119\n",
      "Epoch 00019: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8724.2393 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8973.6885\n",
      "Epoch 00020: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8734.4512 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8217.7236  ETA: 2s - loss: 0.0233 -\n",
      "Epoch 00021: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8730.0449 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1972\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8967.8350\n",
      "Epoch 00022: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8728.7363 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1966\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8965.8945\n",
      "Epoch 00023: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8726.8516 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1969\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8607.8672\n",
      "Epoch 00024: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8729.0010 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1964\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8588.1279\n",
      "Epoch 00025: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8735.5576 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1965\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0229 - mape: 8230.3066\n",
      "Epoch 00026: val_loss did not improve from 0.03573\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0234 - mae: 0.0229 - mape: 8738.8379 - val_loss: 0.0357 - val_mae: 0.0353 - val_mape: 6.1968\n",
      "Epoch 27/200\n",
      "13312/32614 [===========>..................] - ETA: 1s - loss: 0.0233 - mae: 0.0229 - mape: 7589.0488"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-05065ff65640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y, \n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9fPE-jZuQ0r",
    "outputId": "aca09565-e9df-4067-d473-32dd8826a738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 541.71\n",
      "Median Absolute Error (MedAE): 449.47\n",
      "Mean Squared Error (MSE): 473684.02\n",
      "Root Mean Squared Error (RMSE): 688.25\n",
      "Mean Absolute Percentage Error (MAPE): 5.4 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.39 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\1\\\\E3_cp-0004-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7x0O-u0GbLy"
   },
   "source": [
    "# E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rdBroUOqIeW6"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(200, activation='relu')(x)\n",
    " #       x = Dense(10, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AQaM-IuPIXqN"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QwUKEvSfIW09"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NHgO7mJ4IQVp"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TIsLEDoIN0C",
    "outputId": "3ef77929-b334-4953-faab-b938f376c306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpLkyqBmIKIw",
    "outputId": "3d701f4a-270d-4efc-b52a-cd26d12d4dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.2038 - mae: 0.0831 - mape: 10284.1670\n",
      "Epoch 00001: val_loss improved from inf to 0.10131, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0001-loss0.10.h5\n",
      "32614/32614 [==============================] - 19s 581us/sample - loss: 0.2036 - mae: 0.0830 - mape: 10262.1201 - val_loss: 0.1013 - val_mae: 0.0675 - val_mape: 11.9466\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0654 - mae: 0.0492 - mape: 9617.4668\n",
      "Epoch 00002: val_loss improved from 0.10131 to 0.07428, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0002-loss0.07.h5\n",
      "32614/32614 [==============================] - 14s 420us/sample - loss: 0.0654 - mae: 0.0492 - mape: 9596.8438 - val_loss: 0.0743 - val_mae: 0.0671 - val_mape: 11.8373\n",
      "Epoch 3/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0511 - mae: 0.0461 - mape: 9812.7764\n",
      "Epoch 00003: val_loss improved from 0.07428 to 0.06341, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0003-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0510 - mae: 0.0460 - mape: 9762.8730 - val_loss: 0.0634 - val_mae: 0.0599 - val_mape: 10.4427\n",
      "Epoch 4/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0450 - mae: 0.0419 - mape: 9616.9199\n",
      "Epoch 00004: val_loss did not improve from 0.06341\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0450 - mae: 0.0419 - mape: 9596.3027 - val_loss: 0.0706 - val_mae: 0.0680 - val_mape: 11.4199\n",
      "Epoch 5/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0441 - mae: 0.0414 - mape: 9613.5029\n",
      "Epoch 00005: val_loss improved from 0.06341 to 0.05454, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0005-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0441 - mae: 0.0414 - mape: 9611.7354 - val_loss: 0.0545 - val_mae: 0.0519 - val_mape: 9.2312\n",
      "Epoch 6/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0418 - mae: 0.0397 - mape: 9602.1924\n",
      "Epoch 00006: val_loss did not improve from 0.05454\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0418 - mae: 0.0397 - mape: 9572.1885 - val_loss: 0.0587 - val_mae: 0.0570 - val_mape: 9.8649\n",
      "Epoch 7/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0406 - mae: 0.0389 - mape: 9195.3799\n",
      "Epoch 00007: val_loss improved from 0.05454 to 0.04682, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0007-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0406 - mae: 0.0389 - mape: 9554.0049 - val_loss: 0.0468 - val_mae: 0.0450 - val_mape: 8.7847\n",
      "Epoch 8/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0399 - mae: 0.0384 - mape: 9416.9941\n",
      "Epoch 00008: val_loss did not improve from 0.04682\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0399 - mae: 0.0384 - mape: 9396.7998 - val_loss: 0.0475 - val_mae: 0.0462 - val_mape: 8.9737\n",
      "Epoch 9/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0397 - mae: 0.0383 - mape: 9665.3730\n",
      "Epoch 00009: val_loss improved from 0.04682 to 0.04522, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0009-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0397 - mae: 0.0383 - mape: 9644.6445 - val_loss: 0.0452 - val_mae: 0.0439 - val_mape: 8.5116\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0377 - mape: 9573.1260\n",
      "Epoch 00010: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0389 - mae: 0.0377 - mape: 9533.8271 - val_loss: 0.0940 - val_mae: 0.0928 - val_mape: 15.8112\n",
      "Epoch 11/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0390 - mae: 0.0378 - mape: 9537.1748\n",
      "Epoch 00011: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0390 - mae: 0.0378 - mape: 9535.4209 - val_loss: 0.0501 - val_mae: 0.0490 - val_mape: 8.8788\n",
      "Epoch 12/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0381 - mae: 0.0371 - mape: 9512.4170\n",
      "Epoch 00012: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0381 - mae: 0.0371 - mape: 9492.0146 - val_loss: 0.0511 - val_mae: 0.0502 - val_mape: 9.2328\n",
      "Epoch 13/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0377 - mae: 0.0368 - mape: 9480.7461\n",
      "Epoch 00013: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0377 - mae: 0.0368 - mape: 9479.0029 - val_loss: 0.0494 - val_mae: 0.0485 - val_mape: 9.0546\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.0370 - mape: 9640.1309\n",
      "Epoch 00014: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0379 - mae: 0.0370 - mape: 9600.5547 - val_loss: 0.0475 - val_mae: 0.0466 - val_mape: 8.6730\n",
      "Epoch 15/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0375 - mae: 0.0367 - mape: 9567.5752\n",
      "Epoch 00015: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0375 - mae: 0.0367 - mape: 9565.8154 - val_loss: 0.0497 - val_mae: 0.0489 - val_mape: 9.3880\n",
      "Epoch 16/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0374 - mae: 0.0366 - mape: 9551.2051\n",
      "Epoch 00016: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0374 - mae: 0.0366 - mape: 9540.0859 - val_loss: 0.0464 - val_mae: 0.0456 - val_mape: 8.5940\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0369 - mae: 0.0362 - mape: 9563.9082\n",
      "Epoch 00017: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0369 - mae: 0.0362 - mape: 9562.1514 - val_loss: 0.0475 - val_mae: 0.0468 - val_mape: 8.6608\n",
      "Epoch 18/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0368 - mae: 0.0361 - mape: 9577.6992\n",
      "Epoch 00018: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0368 - mae: 0.0361 - mape: 9566.5488 - val_loss: 0.0627 - val_mae: 0.0620 - val_mape: 10.5263\n",
      "Epoch 19/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0366 - mae: 0.0359 - mape: 8800.0615\n",
      "Epoch 00019: val_loss did not improve from 0.04522\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0366 - mae: 0.0359 - mape: 9530.1943 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 8.7106\n",
      "Epoch 20/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9487.9727\n",
      "Epoch 00020: val_loss improved from 0.04522 to 0.04432, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0020-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9476.9248 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.3836\n",
      "Epoch 21/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0364 - mae: 0.0357 - mape: 9175.1748\n",
      "Epoch 00021: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0364 - mae: 0.0357 - mape: 9548.0811 - val_loss: 0.0444 - val_mae: 0.0438 - val_mape: 8.4635\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0356 - mape: 9438.7158\n",
      "Epoch 00022: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0362 - mae: 0.0356 - mape: 9427.7275 - val_loss: 0.0463 - val_mae: 0.0456 - val_mape: 8.8022\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0363 - mae: 0.0356 - mape: 9494.2676\n",
      "Epoch 00023: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0363 - mae: 0.0356 - mape: 9492.5234 - val_loss: 0.0502 - val_mae: 0.0496 - val_mape: 8.8956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0360 - mae: 0.0354 - mape: 9481.0664\n",
      "Epoch 00024: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0360 - mae: 0.0354 - mape: 9479.3242 - val_loss: 0.0482 - val_mae: 0.0476 - val_mape: 8.8535\n",
      "Epoch 25/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9452.0479\n",
      "Epoch 00025: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9422.5107 - val_loss: 0.0450 - val_mae: 0.0444 - val_mape: 8.3675\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9389.7109\n",
      "Epoch 00026: val_loss did not improve from 0.04432\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9360.3701 - val_loss: 0.0505 - val_mae: 0.0499 - val_mape: 8.9818\n",
      "Epoch 27/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0353 - mape: 9449.0146\n",
      "Epoch 00027: val_loss improved from 0.04432 to 0.04404, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0027-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0359 - mae: 0.0353 - mape: 9419.4844 - val_loss: 0.0440 - val_mae: 0.0434 - val_mape: 8.4639\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0352 - mape: 9357.8389\n",
      "Epoch 00028: val_loss improved from 0.04404 to 0.04385, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0028-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0358 - mae: 0.0352 - mape: 9346.9443 - val_loss: 0.0438 - val_mae: 0.0433 - val_mape: 8.3336\n",
      "Epoch 29/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0356 - mae: 0.0350 - mape: 9348.0967- ETA: 2s - loss: 0.\n",
      "Epoch 00029: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0356 - mae: 0.0350 - mape: 9337.2129 - val_loss: 0.0453 - val_mae: 0.0447 - val_mape: 8.8223\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0355 - mae: 0.0349 - mape: 9328.2656\n",
      "Epoch 00030: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0355 - mae: 0.0349 - mape: 9299.1152 - val_loss: 0.0443 - val_mae: 0.0437 - val_mape: 8.4327\n",
      "Epoch 31/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0348 - mape: 9367.9561\n",
      "Epoch 00031: val_loss did not improve from 0.04385\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0354 - mae: 0.0348 - mape: 9329.5146 - val_loss: 0.0556 - val_mae: 0.0550 - val_mape: 9.5869\n",
      "Epoch 32/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0346 - mae: 0.0341 - mape: 9331.7041\n",
      "Epoch 00032: val_loss improved from 0.04385 to 0.03952, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0032-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0346 - mae: 0.0341 - mape: 9293.3955 - val_loss: 0.0395 - val_mae: 0.0389 - val_mape: 7.3175\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0313 - mae: 0.0307 - mape: 8705.7988\n",
      "Epoch 00033: val_loss did not improve from 0.03952\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0313 - mae: 0.0307 - mape: 9014.9424 - val_loss: 0.0402 - val_mae: 0.0396 - val_mape: 6.9964\n",
      "Epoch 34/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0310 - mae: 0.0304 - mape: 8956.5234\n",
      "Epoch 00034: val_loss improved from 0.03952 to 0.03736, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0034-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 427us/sample - loss: 0.0310 - mae: 0.0304 - mape: 8937.3135 - val_loss: 0.0374 - val_mae: 0.0368 - val_mape: 6.7796\n",
      "Epoch 35/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0308 - mae: 0.0302 - mape: 9005.4199\n",
      "Epoch 00035: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0308 - mae: 0.0302 - mape: 9003.7646 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.5172\n",
      "Epoch 36/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 8986.6465\n",
      "Epoch 00036: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0307 - mae: 0.0301 - mape: 8984.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.7190\n",
      "Epoch 37/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0307 - mae: 0.0301 - mape: 9036.8193\n",
      "Epoch 00037: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0307 - mae: 0.0301 - mape: 9017.4414 - val_loss: 0.0397 - val_mae: 0.0391 - val_mape: 6.9239\n",
      "Epoch 38/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0304 - mae: 0.0298 - mape: 8871.0625\n",
      "Epoch 00038: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0305 - mae: 0.0299 - mape: 8852.0391 - val_loss: 0.0418 - val_mae: 0.0413 - val_mape: 7.1686\n",
      "Epoch 39/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8882.9082\n",
      "Epoch 00039: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8881.2744 - val_loss: 0.0411 - val_mae: 0.0405 - val_mape: 7.0621\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0306 - mae: 0.0300 - mape: 8929.6758\n",
      "Epoch 00040: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0306 - mae: 0.0300 - mape: 8910.5225 - val_loss: 0.0390 - val_mae: 0.0384 - val_mape: 6.7820\n",
      "Epoch 41/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8876.4180\n",
      "Epoch 00041: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8857.3779 - val_loss: 0.0391 - val_mae: 0.0385 - val_mape: 6.9458\n",
      "Epoch 42/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8984.8389\n",
      "Epoch 00042: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8956.7549 - val_loss: 0.0413 - val_mae: 0.0407 - val_mape: 7.6354\n",
      "Epoch 43/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0301 - mae: 0.0295 - mape: 8793.3711\n",
      "Epoch 00043: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0301 - mae: 0.0295 - mape: 8783.1338 - val_loss: 0.0435 - val_mae: 0.0429 - val_mape: 7.4288\n",
      "Epoch 44/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0296 - mape: 8822.2676- ETA: 6s - loss: 0.0301 - mae: 0.0 - ETA: 1s - loss: 0.0302 - mae:\n",
      "Epoch 00044: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0302 - mae: 0.0296 - mape: 8811.9941 - val_loss: 0.0384 - val_mae: 0.0378 - val_mape: 6.9922\n",
      "Epoch 45/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0298 - mae: 0.0292 - mape: 8836.8652\n",
      "Epoch 00045: val_loss did not improve from 0.03736\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0298 - mae: 0.0292 - mape: 8800.5830 - val_loss: 0.0593 - val_mae: 0.0587 - val_mape: 11.2009\n",
      "Epoch 46/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0287 - mae: 0.0281 - mape: 8818.6738\n",
      "Epoch 00046: val_loss improved from 0.03736 to 0.03728, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0046-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0287 - mae: 0.0281 - mape: 8791.1104 - val_loss: 0.0373 - val_mae: 0.0367 - val_mape: 6.3479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0276 - mae: 0.0269 - mape: 8903.1064\n",
      "Epoch 00047: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0276 - mae: 0.0269 - mape: 8866.5479 - val_loss: 0.0456 - val_mae: 0.0449 - val_mape: 7.3343\n",
      "Epoch 48/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0264 - mape: 8938.7715\n",
      "Epoch 00048: val_loss did not improve from 0.03728\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0270 - mae: 0.0264 - mape: 8919.5977 - val_loss: 0.0419 - val_mae: 0.0413 - val_mape: 6.8349\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0272 - mae: 0.0265 - mape: 8954.8320\n",
      "Epoch 00049: val_loss improved from 0.03728 to 0.03701, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0049-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 419us/sample - loss: 0.0272 - mae: 0.0265 - mape: 8926.8408 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2637\n",
      "Epoch 50/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0263 - mape: 8925.1689\n",
      "Epoch 00050: val_loss did not improve from 0.03701\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0269 - mae: 0.0263 - mape: 8914.7754 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.2835\n",
      "Epoch 51/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0262 - mape: 8973.1758\n",
      "Epoch 00051: val_loss improved from 0.03701 to 0.03644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0051-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0269 - mae: 0.0262 - mape: 8936.3379 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3367\n",
      "Epoch 52/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0267 - mae: 0.0260 - mape: 8984.7070\n",
      "Epoch 00052: val_loss improved from 0.03644 to 0.03603, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0267 - mae: 0.0260 - mape: 8947.8154 - val_loss: 0.0360 - val_mae: 0.0354 - val_mape: 6.2870\n",
      "Epoch 53/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.0259 - mape: 8895.4316\n",
      "Epoch 00053: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0265 - mae: 0.0259 - mape: 8893.7969 - val_loss: 0.0433 - val_mae: 0.0426 - val_mape: 6.9243\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0258 - mape: 8529.0977\n",
      "Epoch 00054: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0264 - mae: 0.0258 - mape: 8883.7363 - val_loss: 0.0370 - val_mae: 0.0364 - val_mape: 6.3396\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8902.2764\n",
      "Epoch 00055: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8883.1807 - val_loss: 0.0377 - val_mae: 0.0371 - val_mape: 6.5213\n",
      "Epoch 56/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0264 - mae: 0.0257 - mape: 8834.2520\n",
      "Epoch 00056: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0264 - mae: 0.0257 - mape: 8806.6387 - val_loss: 0.0378 - val_mae: 0.0372 - val_mape: 6.4096\n",
      "Epoch 57/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0255 - mape: 8899.8281\n",
      "Epoch 00057: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 421us/sample - loss: 0.0261 - mae: 0.0255 - mape: 8889.4648 - val_loss: 0.0365 - val_mae: 0.0358 - val_mape: 6.4844\n",
      "Epoch 58/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0257 - mape: 8935.8350\n",
      "Epoch 00058: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 413us/sample - loss: 0.0263 - mae: 0.0257 - mape: 8899.1426 - val_loss: 0.0389 - val_mae: 0.0382 - val_mape: 6.9754\n",
      "Epoch 59/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0255 - mape: 8985.3438\n",
      "Epoch 00059: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 412us/sample - loss: 0.0262 - mae: 0.0255 - mape: 8966.0693 - val_loss: 0.0448 - val_mae: 0.0442 - val_mape: 7.1970\n",
      "Epoch 60/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0254 - mape: 8698.2939\n",
      "Epoch 00060: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0261 - mae: 0.0254 - mape: 9026.8086 - val_loss: 0.0371 - val_mae: 0.0365 - val_mape: 6.4728\n",
      "Epoch 61/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0256 - mape: 8944.7266\n",
      "Epoch 00061: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0262 - mae: 0.0256 - mape: 8943.0820 - val_loss: 0.0376 - val_mae: 0.0369 - val_mape: 6.4816\n",
      "Epoch 62/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0253 - mape: 8861.0674\n",
      "Epoch 00062: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0259 - mae: 0.0253 - mape: 8833.3691 - val_loss: 0.0376 - val_mae: 0.0370 - val_mape: 6.7046\n",
      "Epoch 63/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0252 - mape: 8937.2539\n",
      "Epoch 00063: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 414us/sample - loss: 0.0259 - mae: 0.0252 - mape: 8909.3174 - val_loss: 0.0441 - val_mae: 0.0434 - val_mape: 7.0393\n",
      "Epoch 64/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0251 - mape: 8836.8535\n",
      "Epoch 00064: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0257 - mae: 0.0251 - mape: 8826.5635 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.1966\n",
      "Epoch 65/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8864.3887\n",
      "Epoch 00065: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 13s 411us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8845.3740 - val_loss: 0.0403 - val_mae: 0.0396 - val_mape: 6.6836\n",
      "Epoch 66/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0258 - mae: 0.0251 - mape: 8864.2441\n",
      "Epoch 00066: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0258 - mae: 0.0251 - mape: 8853.9238 - val_loss: 0.0362 - val_mae: 0.0355 - val_mape: 6.2001\n",
      "Epoch 67/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8919.5137- ETA: 0s - loss: 0.0256 - mae: 0.0249 - mape: 8391\n",
      "Epoch 00067: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 425us/sample - loss: 0.0256 - mae: 0.0249 - mape: 8900.3799 - val_loss: 0.0435 - val_mae: 0.0428 - val_mape: 6.9965\n",
      "Epoch 68/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0249 - mape: 8921.0537\n",
      "Epoch 00068: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0255 - mae: 0.0249 - mape: 8910.6650 - val_loss: 0.0368 - val_mae: 0.0361 - val_mape: 6.4016\n",
      "Epoch 69/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0250 - mape: 8904.1221\n",
      "Epoch 00069: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0256 - mae: 0.0250 - mape: 8885.0215 - val_loss: 0.0366 - val_mae: 0.0359 - val_mape: 6.4782\n",
      "Epoch 70/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0248 - mape: 8509.3799\n",
      "Epoch 00070: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0255 - mae: 0.0248 - mape: 8848.4863 - val_loss: 0.0364 - val_mae: 0.0358 - val_mape: 6.3512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0254 - mae: 0.0247 - mape: 8894.0742\n",
      "Epoch 00071: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0254 - mae: 0.0247 - mape: 8874.9961 - val_loss: 0.0447 - val_mae: 0.0440 - val_mape: 7.2170\n",
      "Epoch 72/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0253 - mae: 0.0246 - mape: 8940.8789\n",
      "Epoch 00072: val_loss did not improve from 0.03603\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0253 - mae: 0.0246 - mape: 8921.6992 - val_loss: 0.0399 - val_mae: 0.0393 - val_mape: 6.6394\n",
      "Epoch 73/80\n",
      "31840/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0246 - mape: 8697.8066"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d15c63a445f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BNTzwnub2Eh-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 577.97\n",
      "Median Absolute Error (MedAE): 474.07\n",
      "Mean Squared Error (MSE): 554533.54\n",
      "Root Mean Squared Error (RMSE): 744.67\n",
      "Mean Absolute Percentage Error (MAPE): 5.86 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.58 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tYU2gaBI4Yl"
   },
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fz9kU3b6IvyO"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E1-cp-0052-loss0.04.h5'\n",
    "start_epoch= 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2bMbPx8RJKL4"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5T4Ho3YJJKTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E1-cp-0052-loss0.04.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "14t41QwoJKZ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0241 - mae: 0.0235 - mape: 8787.5186\n",
      "Epoch 00001: val_loss improved from inf to 0.03371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 57s 2ms/sample - loss: 0.0241 - mae: 0.0235 - mape: 8785.9023 - val_loss: 0.0337 - val_mae: 0.0331 - val_mape: 5.6219\n",
      "Epoch 2/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8818.7939\n",
      "Epoch 00002: val_loss improved from 0.03371 to 0.03268, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 8808.5244 - val_loss: 0.0327 - val_mae: 0.0321 - val_mape: 5.5019\n",
      "Epoch 3/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 8832.4248\n",
      "Epoch 00003: val_loss improved from 0.03268 to 0.03238, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 8830.8018 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4341\n",
      "Epoch 4/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 8807.1309\n",
      "Epoch 00004: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 8805.5117 - val_loss: 0.0330 - val_mae: 0.0324 - val_mape: 5.4931\n",
      "Epoch 5/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0212 - mape: 8834.0068\n",
      "Epoch 00005: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0212 - mape: 8832.3828 - val_loss: 0.0333 - val_mae: 0.0328 - val_mape: 5.5382\n",
      "Epoch 6/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8771.0771\n",
      "Epoch 00006: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8769.4648 - val_loss: 0.0326 - val_mae: 0.0321 - val_mape: 5.4661\n",
      "Epoch 7/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0211 - mape: 8793.5381\n",
      "Epoch 00007: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0216 - mae: 0.0211 - mape: 8783.2979 - val_loss: 0.0325 - val_mae: 0.0320 - val_mape: 5.4550\n",
      "Epoch 8/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8746.4922\n",
      "Epoch 00008: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8744.8848 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.4740\n",
      "Epoch 9/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0210 - mape: 8799.6211\n",
      "Epoch 00009: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0215 - mae: 0.0210 - mape: 8789.3721 - val_loss: 0.0332 - val_mae: 0.0327 - val_mape: 5.5137\n",
      "Epoch 10/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8783.9893\n",
      "Epoch 00010: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8782.3740 - val_loss: 0.0327 - val_mae: 0.0322 - val_mape: 5.4856\n",
      "Epoch 11/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0209 - mape: 8814.5137\n",
      "Epoch 00011: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0214 - mae: 0.0209 - mape: 8804.2490 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.4683\n",
      "Epoch 12/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0208 - mape: 8829.3320\n",
      "Epoch 00012: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0213 - mae: 0.0208 - mape: 8819.0498 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4762\n",
      "Epoch 13/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8766.2891\n",
      "Epoch 00013: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8764.6777 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.6098\n",
      "Epoch 14/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0208 - mape: 8807.6914\n",
      "Epoch 00014: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0208 - mape: 8797.4355 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4454\n",
      "Epoch 15/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 8763.3545\n",
      "Epoch 00015: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 8761.7432 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.4502\n",
      "Epoch 16/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0207 - mape: 8800.9727\n",
      "Epoch 00016: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0207 - mape: 8790.7227 - val_loss: 0.0336 - val_mae: 0.0331 - val_mape: 5.5508\n",
      "Epoch 17/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8805.7637\n",
      "Epoch 00017: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 8804.1465 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5136\n",
      "Epoch 18/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8785.8975\n",
      "Epoch 00018: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8775.6660 - val_loss: 0.0324 - val_mae: 0.0320 - val_mape: 5.4366\n",
      "Epoch 19/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0206 - mape: 8766.3740\n",
      "Epoch 00019: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0210 - mae: 0.0206 - mape: 8756.1650 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5053\n",
      "Epoch 20/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8869.6514- ETA: 1s - loss: 0.0209 - mae: 0.0205\n",
      "Epoch 00020: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8859.3301 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5394\n",
      "Epoch 21/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8719.9990\n",
      "Epoch 00021: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8718.3955 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.4912\n",
      "Epoch 22/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0205 - mape: 8816.5059\n",
      "Epoch 00022: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0209 - mae: 0.0205 - mape: 8814.8848 - val_loss: 0.0329 - val_mae: 0.0324 - val_mape: 5.5017\n",
      "Epoch 23/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8851.2988\n",
      "Epoch 00023: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8840.9902 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5257\n",
      "Epoch 24/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8849.9473\n",
      "Epoch 00024: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8839.6475 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0204 - mape: 8417.9062\n",
      "Epoch 00025: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0208 - mae: 0.0204 - mape: 8762.0635 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5165\n",
      "Epoch 26/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8781.5459\n",
      "Epoch 00026: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8771.3203 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5305\n",
      "Epoch 27/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8805.2725\n",
      "Epoch 00027: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8795.0176 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5373\n",
      "Epoch 28/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0203 - mape: 8814.1201\n",
      "Epoch 00028: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0207 - mae: 0.0203 - mape: 8803.8555 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5812\n",
      "Epoch 29/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8835.2373\n",
      "Epoch 00029: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8833.6133 - val_loss: 0.0327 - val_mae: 0.0323 - val_mape: 5.5439\n",
      "Epoch 30/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8756.9062\n",
      "Epoch 00030: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8746.7100 - val_loss: 0.0335 - val_mae: 0.0331 - val_mape: 5.5782\n",
      "Epoch 31/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0202 - mape: 8848.8691\n",
      "Epoch 00031: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0206 - mae: 0.0202 - mape: 8847.2422 - val_loss: 0.0345 - val_mae: 0.0342 - val_mape: 5.6903\n",
      "Epoch 32/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8786.9775\n",
      "Epoch 00032: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8785.3623 - val_loss: 0.0348 - val_mae: 0.0344 - val_mape: 5.7179\n",
      "Epoch 33/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8790.1523\n",
      "Epoch 00033: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8779.9150 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5215\n",
      "Epoch 34/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0202 - mape: 8812.7725\n",
      "Epoch 00034: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0202 - mape: 8811.1523 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5479\n",
      "Epoch 35/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8778.3477-\n",
      "Epoch 00035: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8776.7334 - val_loss: 0.0329 - val_mae: 0.0325 - val_mape: 5.5697\n",
      "Epoch 36/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0201 - mape: 8766.7051\n",
      "Epoch 00036: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0205 - mae: 0.0201 - mape: 8756.4961 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.6073\n",
      "Epoch 37/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0200 - mape: 8769.1572\n",
      "Epoch 00037: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0200 - mape: 8767.5449 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5441\n",
      "Epoch 38/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0201 - mape: 8747.3877\n",
      "Epoch 00038: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0204 - mae: 0.0201 - mape: 8745.7793 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5797\n",
      "Epoch 39/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0200 - mape: 8836.6064\n",
      "Epoch 00039: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0200 - mape: 8834.9814 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5334\n",
      "Epoch 40/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8814.8398\n",
      "Epoch 00040: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8804.5742 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5985\n",
      "Epoch 41/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0199 - mape: 8839.9746\n",
      "Epoch 00041: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0203 - mae: 0.0199 - mape: 8829.6797 - val_loss: 0.0331 - val_mae: 0.0327 - val_mape: 5.5734\n",
      "Epoch 42/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0198 - mape: 8831.3848\n",
      "Epoch 00042: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0202 - mae: 0.0198 - mape: 8829.7607 - val_loss: 0.0335 - val_mae: 0.0332 - val_mape: 5.5997\n",
      "Epoch 43/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0201 - mae: 0.0198 - mape: 8783.8525\n",
      "Epoch 00043: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0201 - mae: 0.0198 - mape: 8782.2383 - val_loss: 0.0337 - val_mae: 0.0334 - val_mape: 5.6261\n",
      "Epoch 44/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0197 - mape: 8822.6426\n",
      "Epoch 00044: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0197 - mape: 8821.0205 - val_loss: 0.0338 - val_mae: 0.0335 - val_mape: 5.6350\n",
      "Epoch 45/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8898.9219\n",
      "Epoch 00045: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8888.5576 - val_loss: 0.0330 - val_mae: 0.0326 - val_mape: 5.5640\n",
      "Epoch 46/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0196 - mape: 8848.5547\n",
      "Epoch 00046: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0200 - mae: 0.0196 - mape: 8846.9277 - val_loss: 0.0326 - val_mae: 0.0323 - val_mape: 5.5130\n",
      "Epoch 47/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8915.1211\n",
      "Epoch 00047: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8904.7383 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5152\n",
      "Epoch 48/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0196 - mape: 8884.6523\n",
      "Epoch 00048: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0196 - mape: 8874.3057 - val_loss: 0.0334 - val_mae: 0.0330 - val_mape: 5.5568\n",
      "Epoch 49/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0195 - mape: 8901.6914\n",
      "Epoch 00049: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0199 - mae: 0.0195 - mape: 8891.3252 - val_loss: 0.0333 - val_mae: 0.0329 - val_mape: 5.5512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8912.0732\n",
      "Epoch 00050: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8901.6934 - val_loss: 0.0326 - val_mae: 0.0322 - val_mape: 5.5117\n",
      "Epoch 51/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0194 - mape: 8897.9141\n",
      "Epoch 00051: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0194 - mape: 8896.2773 - val_loss: 0.0339 - val_mae: 0.0336 - val_mape: 5.6620\n",
      "Epoch 52/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0198 - mae: 0.0195 - mape: 8820.4697\n",
      "Epoch 00052: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0198 - mae: 0.0195 - mape: 8810.1973 - val_loss: 0.0336 - val_mae: 0.0333 - val_mape: 5.6221\n",
      "Epoch 53/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8908.1738\n",
      "Epoch 00053: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8906.5361 - val_loss: 0.0331 - val_mae: 0.0328 - val_mape: 5.5511\n",
      "Epoch 54/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0194 - mape: 8871.0322\n",
      "Epoch 00054: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0194 - mape: 8869.4014 - val_loss: 0.0333 - val_mae: 0.0330 - val_mape: 5.5471\n",
      "Epoch 55/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8876.8662\n",
      "Epoch 00055: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8866.5283 - val_loss: 0.0328 - val_mae: 0.0324 - val_mape: 5.5071\n",
      "Epoch 56/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8917.7432- ETA: 2s - loss: 0.0196 - ma\n",
      "Epoch 00056: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8916.1035 - val_loss: 0.0325 - val_mae: 0.0321 - val_mape: 5.5076\n",
      "Epoch 57/60\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0193 - mape: 8886.0176\n",
      "Epoch 00057: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0197 - mae: 0.0193 - mape: 8884.3838 - val_loss: 0.0330 - val_mae: 0.0327 - val_mape: 5.6687\n",
      "Epoch 58/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8843.5547\n",
      "Epoch 00058: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0195 - mae: 0.0192 - mape: 8833.2559 - val_loss: 0.0340 - val_mae: 0.0337 - val_mape: 5.6366\n",
      "Epoch 59/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0193 - mape: 8897.1895\n",
      "Epoch 00059: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0193 - mape: 8886.8271 - val_loss: 0.0344 - val_mae: 0.0340 - val_mape: 5.7448\n",
      "Epoch 60/60\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0192 - mape: 8878.5566\n",
      "Epoch 00060: val_loss did not improve from 0.03238\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0196 - mae: 0.0192 - mape: 8868.2227 - val_loss: 0.0338 - val_mae: 0.0334 - val_mape: 5.6100\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.92\n",
      "Median Absolute Error (MedAE): 341.57\n",
      "Mean Squared Error (MSE): 329961.29\n",
      "Root Mean Squared Error (RMSE): 574.42\n",
      "Mean Absolute Percentage Error (MAPE): 4.31 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.33 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPZ0OMjDJhnO"
   },
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UZgZB020JqhL"
   },
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E2_cp-0003-loss0.03.h5'\n",
    "start_epoch= 53+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ssm3plQeJn6F"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "u-3UhfYuJlge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E2_cp-0003-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "9b_pJN2ZJKff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9133.9141\n",
      "Epoch 00001: val_loss improved from inf to 0.03257, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 9s 283us/sample - loss: 0.0214 - mae: 0.0208 - mape: 8890.3867 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4449\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8733.1416\n",
      "Epoch 00002: val_loss improved from 0.03257 to 0.03256, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8869.8643 - val_loss: 0.0326 - val_mae: 0.0320 - val_mape: 5.4437\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9107.9521\n",
      "Epoch 00003: val_loss improved from 0.03256 to 0.03242, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8865.1064 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8707.8369\n",
      "Epoch 00004: val_loss improved from 0.03242 to 0.03233, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8851.4385 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4285\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9089.2246\n",
      "Epoch 00005: val_loss improved from 0.03233 to 0.03228, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8846.9043 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8326.9922\n",
      "Epoch 00006: val_loss improved from 0.03228 to 0.03226, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8841.4912 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8705.7549\n",
      "Epoch 00007: val_loss improved from 0.03226 to 0.03223, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0212 - mae: 0.0206 - mape: 8839.4424 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4216\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9068.8096\n",
      "Epoch 00008: val_loss improved from 0.03223 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8827.0107 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4208\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8693.2275  ETA: 1s - loss: 0.0213 - mae: 0.020\n",
      "Epoch 00009: val_loss improved from 0.03222 to 0.03222, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8828.1250 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8320.9189\n",
      "Epoch 00010: val_loss improved from 0.03222 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8823.5527 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 8678.0664\n",
      "Epoch 00011: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8818.6221 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9058.5664\n",
      "Epoch 00012: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8817.0547 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.8018\n",
      "Epoch 00013: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8819.4121 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9055.2490\n",
      "Epoch 00014: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8813.7979 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4186\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9056.6045\n",
      "Epoch 00015: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8815.1211 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4180\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9046.7842\n",
      "Epoch 00016: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8805.6162 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8683.0303\n",
      "Epoch 00017: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8811.1191 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4184\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8288.7080\n",
      "Epoch 00018: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8806.9727 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4185\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9051.8076\n",
      "Epoch 00019: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.4658 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8302.6875\n",
      "Epoch 00020: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8810.8389 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4196\n",
      "Epoch 21/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.1914\n",
      "Epoch 00021: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8803.5537 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 8289.9629\n",
      "Epoch 00022: val_loss improved from 0.03219 to 0.03219, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\3\\E3p-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8808.2617 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4189\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.5645\n",
      "Epoch 00023: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0206 - mape: 8807.9199 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8657.6855\n",
      "Epoch 00024: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.0996 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4191\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8660.9229- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00025: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4287 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4194\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.9697\n",
      "Epoch 00026: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.0215 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4190\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.5674- ETA: 1s - loss: 0.0211 - mae: 0.0205\n",
      "Epoch 00027: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.4775 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4188\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.4648\n",
      "Epoch 00028: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4198\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8674.4756\n",
      "Epoch 00029: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.5635 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7906.3037\n",
      "Epoch 00030: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.4629 - val_loss: 0.0322 - val_mae: 0.0316 - val_mape: 5.4194\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.0000\n",
      "Epoch 00031: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.7725 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4202\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9045.3105\n",
      "Epoch 00032: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.1299 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.4668  ETA: 2s - loss: 0.0211 - m\n",
      "Epoch 00033: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.4404 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4200\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8285.9170\n",
      "Epoch 00034: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.4795 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4204\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.9082\n",
      "Epoch 00035: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6709 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8681.3047- ETA: 2s - loss: 0.0210 - mae: 0.\n",
      "Epoch 00036: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8808.3027 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1055- ETA: 2s - loss: 0.0211 - mae: \n",
      "Epoch 00037: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.6387 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4205\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.3789\n",
      "Epoch 00038: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4212\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7911.5845\n",
      "Epoch 00039: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.6006 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4207\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7988\n",
      "Epoch 00040: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.7549 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4209\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.4160\n",
      "Epoch 00041: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8797.7119 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4218\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8670.8086\n",
      "Epoch 00042: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.4561 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9040.8076- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8763.4\n",
      "Epoch 00043: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8799.7510 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4211\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8671.4062\n",
      "Epoch 00044: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.1455 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4214\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9043.8984\n",
      "Epoch 00045: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.7754 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8666.5801\n",
      "Epoch 00046: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8805.4297 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4199\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.4580\n",
      "Epoch 00047: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.2539 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4226\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8656.3926\n",
      "Epoch 00048: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8795.1895 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4227\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.8350\n",
      "Epoch 00049: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.6064 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9053.7900\n",
      "Epoch 00050: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8812.3799 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4222\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8662.2178\n",
      "Epoch 00051: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8800.1719 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4229\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9047.2744\n",
      "Epoch 00052: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8806.0586 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4240\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8665.7900\n",
      "Epoch 00053: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.5859 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4234\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8664.6738\n",
      "Epoch 00054: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.6367 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4230\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8658.2246\n",
      "Epoch 00055: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8802.2617 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4231\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9039.0703\n",
      "Epoch 00056: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8798.0605 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.2510\n",
      "Epoch 00057: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.1816 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4238\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8288.6846\n",
      "Epoch 00058: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8804.4893 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4235\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.6758\n",
      "Epoch 00059: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8801.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4250\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.4639\n",
      "Epoch 00060: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8807.2031 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 9042.5479\n",
      "Epoch 00061: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8801.4482 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.7363\n",
      "Epoch 00062: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.5029 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4241\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8668.1113- ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 7955. - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8069\n",
      "Epoch 00063: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8803.5488 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.6602\n",
      "Epoch 00064: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0211 - mae: 0.0205 - mape: 8809.3477 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0205 - mape: 8292.0371\n",
      "Epoch 00065: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8800.8613 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4243\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8661.9170\n",
      "Epoch 00066: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.5088 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4233\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.0117\n",
      "Epoch 00067: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.7666 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9047.6230- ETA: 2s - loss: 0.0212 - mae:\n",
      "Epoch 00068: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.3867 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4254\n",
      "Epoch 69/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8285.9189\n",
      "Epoch 00069: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.3457 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4237\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.5889\n",
      "Epoch 00070: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.3418 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.3438\n",
      "Epoch 00071: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8803.1846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4249\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.1455\n",
      "Epoch 00072: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.9678 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4251\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8665.8174\n",
      "Epoch 00073: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.1689 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.3008\n",
      "Epoch 00074: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.6846 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9048.2236\n",
      "Epoch 00075: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8806.9883 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4261\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.5234  ETA: 2s - loss: 0.0209 - \n",
      "Epoch 00076: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2354 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4239\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9045.4512\n",
      "Epoch 00077: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8804.2715 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4265\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9046.2568\n",
      "Epoch 00078: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0527 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4256\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9055.4580\n",
      "Epoch 00079: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.0088 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4271\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9054.0859\n",
      "Epoch 00080: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8812.6738 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4247\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9052.4775\n",
      "Epoch 00081: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.1074 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4263\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8671.4385\n",
      "Epoch 00082: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.1035 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4255\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9050.2695\n",
      "Epoch 00083: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.9912 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8680.7773\n",
      "Epoch 00084: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8807.5732 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4245\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8681.9316\n",
      "Epoch 00085: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8814.9023 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4258\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.9326\n",
      "Epoch 00086: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.5898 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4248\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9051.4629\n",
      "Epoch 00087: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8810.1172 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4259\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8666.3701\n",
      "Epoch 00088: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.2188 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8664.1152\n",
      "Epoch 00089: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.5791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4258\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.7578\n",
      "Epoch 00090: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8808.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4267\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8674.5449\n",
      "Epoch 00091: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8809.6641 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4264\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.3330\n",
      "Epoch 00092: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8811.8057 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8680.4805\n",
      "Epoch 00093: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8807.4814 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4268\n",
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8669.4492\n",
      "Epoch 00094: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0205 - mape: 8805.0020 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 8670.0195\n",
      "Epoch 00095: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.6592 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9049.8623\n",
      "Epoch 00096: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8808.5645 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4274\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8297.3535\n",
      "Epoch 00097: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8814.3789 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4252\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9044.8828\n",
      "Epoch 00098: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8803.7236 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8692.5449\n",
      "Epoch 00099: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8821.6729 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4253\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.0117  ETA: 2s - loss: 0.0212 - mae: 0.0207 - mape - ETA: 1s - loss: 0.0211 - mae: 0.0206 - mape: 13501.83 - ETA: 1s - loss: 0.0211 - mae: 0.0\n",
      "Epoch 00100: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.5918 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8291.2773\n",
      "Epoch 00101: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.7295 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4274\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9055.1973\n",
      "Epoch 00102: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.7637 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4269\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.2109\n",
      "Epoch 00103: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.1260 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4262\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8677.0146\n",
      "Epoch 00104: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 95us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.8604 - val_loss: 0.0322 - val_mae: 0.0317 - val_mape: 5.4262\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8309.9893\n",
      "Epoch 00105: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.9658 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4267\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8300.4561\n",
      "Epoch 00106: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8813.9160 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4260\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8687.0166\n",
      "Epoch 00107: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8820.6562 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4257\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.9922  ETA: 1s - loss: 0.021\n",
      "Epoch 00108: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8818.7070 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7918.0552\n",
      "Epoch 00109: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8806.1182 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4266\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9054.2461\n",
      "Epoch 00110: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8812.8340 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 111/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8679.2012\n",
      "Epoch 00111: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8809.0146 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 112/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.0439\n",
      "Epoch 00112: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.5566 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4272\n",
      "Epoch 113/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8305.8838\n",
      "Epoch 00113: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8816.7695 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4286\n",
      "Epoch 114/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9051.7646\n",
      "Epoch 00114: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.4189 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4268\n",
      "Epoch 115/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8671.9346\n",
      "Epoch 00115: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8811.5635 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4283\n",
      "Epoch 116/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9057.4092  ETA: 2s - loss: \n",
      "Epoch 00116: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8815.9219 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 117/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8685.8936- ETA: 1s - loss: 0.0209 - mae: 0.02\n",
      "Epoch 00117: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8817.3330 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 118/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8302.0400\n",
      "Epoch 00118: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8812.6797 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4707\n",
      "Epoch 00119: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0210 - mae: 0.0204 - mape: 8810.0967 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 120/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8304.8125\n",
      "Epoch 00120: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8818.3047 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4265\n",
      "Epoch 121/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9058.0273\n",
      "Epoch 00121: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5293 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4270\n",
      "Epoch 122/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8682.1084  ETA: 1s - loss: 0.0208 - mae:\n",
      "Epoch 00122: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.7100 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4275\n",
      "Epoch 123/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 9050.8926\n",
      "Epoch 00123: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.5742 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4287\n",
      "Epoch 124/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.2852- ETA: 2s - loss: 0.0209 - mae:\n",
      "Epoch 00124: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.8037 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4277\n",
      "Epoch 125/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.9121\n",
      "Epoch 00125: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.4541 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 126/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7553.2466\n",
      "Epoch 00126: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.8301 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 127/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.8857\n",
      "Epoch 00127: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.4443 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 128/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.3145\n",
      "Epoch 00128: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.7705 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 129/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9062.5234\n",
      "Epoch 00129: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8820.9062 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 130/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8310.6270\n",
      "Epoch 00130: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.6572 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4276\n",
      "Epoch 131/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8676.7832\n",
      "Epoch 00131: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8813.8525 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 132/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9057.6250\n",
      "Epoch 00132: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1289 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 133/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.2627\n",
      "Epoch 00133: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.7959 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4296\n",
      "Epoch 134/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9385\n",
      "Epoch 00134: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8819.0283 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4279\n",
      "Epoch 135/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8680.9990\n",
      "Epoch 00135: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.8438 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4284\n",
      "Epoch 136/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9050.7119- ETA: 0s - loss: 0.0210 - mae: 0.0204 - mape: 8224.\n",
      "Epoch 00136: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.3926 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 137/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9065.7979\n",
      "Epoch 00137: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.0713 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 138/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.0449\n",
      "Epoch 00138: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8811.1436 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4327\n",
      "Epoch 139/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.4678\n",
      "Epoch 00139: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.9834 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 140/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9056.8877\n",
      "Epoch 00140: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.4102 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 141/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9059.1895\n",
      "Epoch 00141: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.6465 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4293\n",
      "Epoch 142/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9068.6006\n",
      "Epoch 00142: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8826.8496 - val_loss: 0.0323 - val_mae: 0.0317 - val_mape: 5.4311\n",
      "Epoch 143/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.5254\n",
      "Epoch 00143: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8814.0791 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8303.7139\n",
      "Epoch 00144: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.9434 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4300\n",
      "Epoch 145/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9057.6035\n",
      "Epoch 00145: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.1113 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4280\n",
      "Epoch 146/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8295.9824\n",
      "Epoch 00146: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8815.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 147/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8299.4932- ETA: 1s - loss: 0.0209 - mae: 0.020\n",
      "Epoch 00147: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.4980 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 148/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8677.7822\n",
      "Epoch 00148: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8817.8496 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4282\n",
      "Epoch 149/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.4521\n",
      "Epoch 00149: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8807.3672 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4317\n",
      "Epoch 150/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8687.6270\n",
      "Epoch 00150: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8824.2520 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 151/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7550.1221\n",
      "Epoch 00151: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.5537 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4318\n",
      "Epoch 152/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8665.8770\n",
      "Epoch 00152: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8809.2012 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4307\n",
      "Epoch 153/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8305.3477\n",
      "Epoch 00153: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.7168 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 154/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8664.9102\n",
      "Epoch 00154: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 94us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8803.5449 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4291\n",
      "Epoch 155/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8674.1807\n",
      "Epoch 00155: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.1172 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4323\n",
      "Epoch 156/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9061.2051\n",
      "Epoch 00156: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8819.6104 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 157/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9058.3799\n",
      "Epoch 00157: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0204 - mape: 8816.8613 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4316\n",
      "Epoch 158/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9053.1445\n",
      "Epoch 00158: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8811.7646 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4289\n",
      "Epoch 159/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9055.0205\n",
      "Epoch 00159: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8813.5781 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4292\n",
      "Epoch 160/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8673.6445\n",
      "Epoch 00160: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8812.8994 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4306\n",
      "Epoch 161/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8671.0654\n",
      "Epoch 00161: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8804.1348 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 162/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 8678.7412\n",
      "Epoch 00162: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8817.2510 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 163/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9058.5049  ETA: 1s - loss: 0.0209 - mae: 0.0204 - map\n",
      "Epoch 00163: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.9990 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4288\n",
      "Epoch 164/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8671.3730\n",
      "Epoch 00164: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8807.5947 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4290\n",
      "Epoch 165/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8304.8350\n",
      "Epoch 00165: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8825.1436 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4294\n",
      "Epoch 166/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 7930.7822\n",
      "Epoch 00166: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.1318 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 167/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8676.1992  ETA: 1s - loss: 0.0209 - mae: 0.0203 - mape:  - ETA: 1s - loss: 0.0208 - mae: 0.0202 - mape: - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9\n",
      "Epoch 00167: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8814.6123 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 8682.7363\n",
      "Epoch 00168: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8816.8896 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 169/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9065.5938\n",
      "Epoch 00169: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8823.9004 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4301\n",
      "Epoch 170/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9051.8340\n",
      "Epoch 00170: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8810.4756 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4297\n",
      "Epoch 171/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8678.6602- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8967.800\n",
      "Epoch 00171: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0209 - mae: 0.0203 - mape: 8815.0049 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4299\n",
      "Epoch 172/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.9189\n",
      "Epoch 00172: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.3682 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4308\n",
      "Epoch 173/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0203 - mape: 9055.6904\n",
      "Epoch 00173: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2402 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 174/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.9043- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8588.017\n",
      "Epoch 00174: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.3857 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4302\n",
      "Epoch 175/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.7666\n",
      "Epoch 00175: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3809 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4309\n",
      "Epoch 176/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8673.9277\n",
      "Epoch 00176: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.3027 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4343\n",
      "Epoch 177/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.8848\n",
      "Epoch 00177: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2305 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4321\n",
      "Epoch 178/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9054.9736  ETA: 1s - loss: 0.0208 - mae: 0.0203 \n",
      "Epoch 00178: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.5439 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4330\n",
      "Epoch 179/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8675.4736\n",
      "Epoch 00179: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8809.1797 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4327\n",
      "Epoch 180/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9061.8086\n",
      "Epoch 00180: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8820.2021 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4329\n",
      "Epoch 181/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9060.0869\n",
      "Epoch 00181: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5107 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4319\n",
      "Epoch 182/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8685.6191\n",
      "Epoch 00182: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8818.5000 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4349\n",
      "Epoch 183/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.7178\n",
      "Epoch 00183: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8814.2656 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4324\n",
      "Epoch 184/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8291.2705\n",
      "Epoch 00184: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.1533 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4331\n",
      "Epoch 185/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.7324\n",
      "Epoch 00185: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 96us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.8145 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 186/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9059.2549\n",
      "Epoch 00186: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8817.7012 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4305\n",
      "Epoch 187/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8682.2168\n",
      "Epoch 00187: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8819.0938 - val_loss: 0.0323 - val_mae: 0.0318 - val_mape: 5.4315\n",
      "Epoch 188/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.7939\n",
      "Epoch 00188: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.2949 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 189/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9057.9834\n",
      "Epoch 00189: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8816.4678 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4330\n",
      "Epoch 190/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9055.2012\n",
      "Epoch 00190: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7588 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4332\n",
      "Epoch 191/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8679.1182\n",
      "Epoch 00191: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.3721 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4325\n",
      "Epoch 192/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8303.0508\n",
      "Epoch 00192: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.7334 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4329\n",
      "Epoch 193/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8677.2891\n",
      "Epoch 00193: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8813.0742 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4315\n",
      "Epoch 194/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9051.3916\n",
      "Epoch 00194: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.0596 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4353\n",
      "Epoch 195/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 7911.0254\n",
      "Epoch 00195: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.6299 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4324\n",
      "Epoch 196/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9053.1436\n",
      "Epoch 00196: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 99us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.7627 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4345\n",
      "Epoch 197/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9056.7646- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 73\n",
      "Epoch 00197: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8815.2822 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4338\n",
      "Epoch 198/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8293.9717\n",
      "Epoch 00198: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8811.3867 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4346\n",
      "Epoch 199/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9052.1162\n",
      "Epoch 00199: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 98us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8810.7646 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.4328\n",
      "Epoch 200/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 8680.9668- ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 796\n",
      "Epoch 00200: val_loss did not improve from 0.03219\n",
      "32614/32614 [==============================] - 3s 97us/sample - loss: 0.0208 - mae: 0.0203 - mape: 8808.9336 - val_loss: 0.0324 - val_mae: 0.0318 - val_mape: 5.4344\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 431.57\n",
      "Median Absolute Error (MedAE): 338.68\n",
      "Mean Squared Error (MSE): 332645.96\n",
      "Root Mean Squared Error (RMSE): 576.75\n",
      "Mean Absolute Percentage Error (MAPE): 4.3 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.3 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\3\\\\E3p-0013-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aW5pp6LD-1"
   },
   "source": [
    "# E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "j9RoOmzuLgHR"
   },
   "outputs": [],
   "source": [
    "class PC:\n",
    "    @staticmethod\n",
    "    def conv_module(x, num_filter, kr, stride ,padding=\"same\", reg=0.0005, name=None):\n",
    "        # initialize the CONV, BN, and RELU layer names\n",
    "        (convName, bnName, actName) = (None, None, None)\n",
    "        \n",
    "        # if a layer name was supplied, prepend it\n",
    "        if name is not None:\n",
    "            convName = name + \"_conv\"\n",
    "            bnName = name + \"_bn\"\n",
    "            actName = name + \"_act\"\n",
    "        # define a CONV => BN => RELU pattern\n",
    "        x = Conv1D(num_filter, kr, strides=stride, padding=padding, kernel_regularizer=l2(reg),name=convName)(x)\n",
    "        x = BatchNormalization(axis=-1, name=bnName)(x)\n",
    "        x = Activation(\"relu\", name=actName)(x)\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def inception_module(x, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, stage, reg=0.0005):\n",
    "        # define the first branch of the Inception module which\n",
    "        # consists of 1x1 convolutions\n",
    "        first = PC.conv_module(x, num1x1, 1, 1, reg=reg, name=stage + \"_first\")\n",
    "        \n",
    "        # define the second branch of the Inception module which\n",
    "        # consists of 1x1 and 3x3 convolutions\n",
    "        second = PC.conv_module(x, num3x3Reduce, 1, 1,  reg=reg, name=stage + \"_second1\")\n",
    "        second = PC.conv_module(second, num3x3, 3, 1, reg=reg, name=stage + \"_second2\")\n",
    "        \n",
    "        # define the third branch of the Inception module which\n",
    "        # are our 1x1 and 5x5 convolutions\n",
    "        third = PC.conv_module(x, num5x5Reduce, 1, 1,  reg=reg, name=stage + \"_third1\")\n",
    "        third = PC.conv_module(third, num5x5, 5, 1,  reg=reg, name=stage + \"_third2\")\n",
    "        \n",
    "        # concatenate across the channel dimension\n",
    "        x = concatenate([first, second, third], axis=-1 ,name=stage + \"_mixed\")\n",
    "        \n",
    "        # return the block\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def build(time_steps, num_features, reg=0.0005):\n",
    "        # initialize the input shape to be \"channels last\" and the\n",
    "        # channels dimension itself\n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape=(time_steps, num_features))\n",
    "        x = LSTM(48, return_sequences=True)(inputs)\n",
    "        x = Activation('relu')(x)\n",
    "        x = PC.conv_module(x, 32, 7, 1)\n",
    "        x = PC.conv_module(x, 64, 3, 1)\n",
    "        x = PC.inception_module(x, 32, 32, 64, 8, 32, \"2a\")\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = PC.inception_module(x, 64, 64, 128, 16, 64, \"3a\")\n",
    "#.......................................................................\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(24)(x)\n",
    "        #x = Dense(1)(x)\n",
    "        \n",
    "#.......................................................................\n",
    "        # create the model\n",
    "        model = Model(inputs, x, name=\"PC\")\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B2H5iBwQLfhj"
   },
   "outputs": [],
   "source": [
    "checkpoints = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "OUTPUT_PATH = 'D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4'\n",
    "FIG_PATH = os.path.sep.join([OUTPUT_PATH,\"history.png\"])\n",
    "JSON_PATH = os.path.sep.join([OUTPUT_PATH,\"history.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "k06fUuK7LerZ"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "start_epoch = 0\n",
    "time_steps=72\n",
    "num_features=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mB2kXVVCLRBA"
   },
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sizKJbygLNBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-3)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-3)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Q6uDSkmdLJlR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.1973 - mae: 0.0794 - mape: 9692.2637\n",
      "Epoch 00001: val_loss improved from inf to 0.10644, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0001-loss0.11.h5\n",
      "32614/32614 [==============================] - 19s 584us/sample - loss: 0.1972 - mae: 0.0794 - mape: 9680.9932 - val_loss: 0.1064 - val_mae: 0.0766 - val_mape: 12.9892\n",
      "Epoch 2/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0557 - mae: 0.0421 - mape: 9249.9512\n",
      "Epoch 00002: val_loss improved from 0.10644 to 0.05840, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0002-loss0.06.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0556 - mae: 0.0420 - mape: 9230.1133 - val_loss: 0.0584 - val_mae: 0.0523 - val_mape: 8.7301\n",
      "Epoch 3/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0416 - mae: 0.0375 - mape: 9262.3770\n",
      "Epoch 00003: val_loss improved from 0.05840 to 0.05401, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0003-loss0.05.h5\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0416 - mae: 0.0375 - mape: 9224.3594 - val_loss: 0.0540 - val_mae: 0.0503 - val_mape: 8.7703\n",
      "Epoch 4/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0398 - mae: 0.0367 - mape: 9258.6729\n",
      "Epoch 00004: val_loss improved from 0.05401 to 0.04371, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0004-loss0.04.h5\n",
      "32614/32614 [==============================] - 14s 431us/sample - loss: 0.0398 - mae: 0.0367 - mape: 9247.8994 - val_loss: 0.0437 - val_mae: 0.0407 - val_mape: 6.7483\n",
      "Epoch 5/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0362 - mae: 0.0336 - mape: 9132.0303  ETA: 4s - \n",
      "Epoch 00005: val_loss did not improve from 0.04371\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0362 - mae: 0.0336 - mape: 9121.3975 - val_loss: 0.0456 - val_mae: 0.0433 - val_mape: 7.2883\n",
      "Epoch 6/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0359 - mae: 0.0332 - mape: 9247.4834\n",
      "Epoch 00006: val_loss improved from 0.04371 to 0.03429, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0358 - mae: 0.0332 - mape: 9227.6475 - val_loss: 0.0343 - val_mae: 0.0317 - val_mape: 5.5244\n",
      "Epoch 7/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0334 - mae: 0.0314 - mape: 8832.6689\n",
      "Epoch 00007: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0334 - mae: 0.0314 - mape: 9193.4805 - val_loss: 0.0770 - val_mae: 0.0752 - val_mape: 10.8461\n",
      "Epoch 8/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0326 - mae: 0.0309 - mape: 9279.7900\n",
      "Epoch 00008: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0326 - mae: 0.0309 - mape: 9241.6865 - val_loss: 0.0911 - val_mae: 0.0895 - val_mape: 12.6461\n",
      "Epoch 9/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0315 - mae: 0.0301 - mape: 9240.0088\n",
      "Epoch 00009: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0315 - mae: 0.0301 - mape: 9211.1299 - val_loss: 0.0497 - val_mae: 0.0483 - val_mape: 7.3868\n",
      "Epoch 10/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0289 - mape: 8889.9570\n",
      "Epoch 00010: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0303 - mae: 0.0289 - mape: 9243.3398 - val_loss: 0.0727 - val_mae: 0.0714 - val_mape: 10.6575\n",
      "Epoch 11/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0290 - mape: 9401.6436\n",
      "Epoch 00011: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0303 - mae: 0.0290 - mape: 9372.2676 - val_loss: 0.0601 - val_mae: 0.0589 - val_mape: 8.9566\n",
      "Epoch 12/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0292 - mae: 0.0280 - mape: 9176.0518\n",
      "Epoch 00012: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0292 - mae: 0.0280 - mape: 9147.3730 - val_loss: 0.0464 - val_mae: 0.0452 - val_mape: 7.1787\n",
      "Epoch 13/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0291 - mae: 0.0279 - mape: 9234.7246\n",
      "Epoch 00013: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 429us/sample - loss: 0.0291 - mae: 0.0279 - mape: 9214.9170 - val_loss: 0.0398 - val_mae: 0.0387 - val_mape: 6.3188\n",
      "Epoch 14/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.0278 - mape: 9259.1611\n",
      "Epoch 00014: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0290 - mae: 0.0278 - mape: 9221.1396 - val_loss: 0.0613 - val_mae: 0.0602 - val_mape: 8.9035\n",
      "Epoch 15/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0285 - mae: 0.0274 - mape: 9332.2979\n",
      "Epoch 00015: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0285 - mae: 0.0274 - mape: 9293.9756 - val_loss: 0.0448 - val_mae: 0.0438 - val_mape: 6.8179\n",
      "Epoch 16/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9270.8516- ETA: 0s - loss: 0.0278 - mae: 0.0268 - ma\n",
      "Epoch 00016: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9269.1475 - val_loss: 0.0455 - val_mae: 0.0446 - val_mape: 7.0034\n",
      "Epoch 17/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0269 - mape: 9212.3809\n",
      "Epoch 00017: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 424us/sample - loss: 0.0279 - mae: 0.0269 - mape: 9210.6865 - val_loss: 0.0670 - val_mae: 0.0661 - val_mape: 9.5463\n",
      "Epoch 18/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9337.9092\n",
      "Epoch 00018: val_loss did not improve from 0.03429\n",
      "32614/32614 [==============================] - 14s 426us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9308.7227 - val_loss: 0.0456 - val_mae: 0.0447 - val_mape: 7.0638\n",
      "Epoch 19/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0264 - mape: 9224.4463\n",
      "Epoch 00019: val_loss improved from 0.03429 to 0.03249, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0019-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0273 - mae: 0.0264 - mape: 9213.7041 - val_loss: 0.0325 - val_mae: 0.0316 - val_mape: 5.3572\n",
      "Epoch 20/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9302.4258\n",
      "Epoch 00020: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9264.2266 - val_loss: 0.0382 - val_mae: 0.0373 - val_mape: 6.0539\n",
      "Epoch 21/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0271 - mae: 0.0262 - mape: 9244.2197\n",
      "Epoch 00021: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 436us/sample - loss: 0.0271 - mae: 0.0262 - mape: 9206.2666 - val_loss: 0.0409 - val_mae: 0.0400 - val_mape: 6.2748\n",
      "Epoch 22/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0269 - mae: 0.0260 - mape: 9348.6699\n",
      "Epoch 00022: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0269 - mae: 0.0260 - mape: 9337.7842 - val_loss: 0.0328 - val_mae: 0.0319 - val_mape: 5.4575\n",
      "Epoch 23/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9254.0684\n",
      "Epoch 00023: val_loss did not improve from 0.03249\n",
      "32614/32614 [==============================] - 14s 423us/sample - loss: 0.0266 - mae: 0.0257 - mape: 9252.3672 - val_loss: 0.0552 - val_mae: 0.0543 - val_mape: 8.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0266 - mae: 0.0257 - mape: 9298.8604\n",
      "Epoch 00024: val_loss improved from 0.03249 to 0.03114, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0024-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 437us/sample - loss: 0.0265 - mae: 0.0257 - mape: 9260.6738 - val_loss: 0.0311 - val_mae: 0.0303 - val_mape: 5.1846\n",
      "Epoch 25/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0262 - mae: 0.0254 - mape: 9225.4922\n",
      "Epoch 00025: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0262 - mae: 0.0254 - mape: 9223.7969 - val_loss: 0.0519 - val_mae: 0.0511 - val_mape: 7.9640\n",
      "Epoch 26/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0263 - mae: 0.0254 - mape: 9168.1641\n",
      "Epoch 00026: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 433us/sample - loss: 0.0263 - mae: 0.0254 - mape: 9139.5078 - val_loss: 0.0324 - val_mae: 0.0315 - val_mape: 5.4029\n",
      "Epoch 27/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0261 - mae: 0.0252 - mape: 9292.8506\n",
      "Epoch 00027: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 435us/sample - loss: 0.0261 - mae: 0.0252 - mape: 9282.0293 - val_loss: 0.0368 - val_mae: 0.0359 - val_mape: 5.8316\n",
      "Epoch 28/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0250 - mape: 9257.2148\n",
      "Epoch 00028: val_loss did not improve from 0.03114\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0259 - mae: 0.0250 - mape: 9246.4346 - val_loss: 0.0322 - val_mae: 0.0313 - val_mape: 5.3347\n",
      "Epoch 29/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0257 - mae: 0.0249 - mape: 9201.8809\n",
      "Epoch 00029: val_loss improved from 0.03114 to 0.03017, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5\n",
      "32614/32614 [==============================] - 14s 432us/sample - loss: 0.0257 - mae: 0.0249 - mape: 9173.1152 - val_loss: 0.0302 - val_mae: 0.0293 - val_mape: 5.0982\n",
      "Epoch 30/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0255 - mae: 0.0246 - mape: 9224.1299\n",
      "Epoch 00030: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0255 - mae: 0.0246 - mape: 9195.2969 - val_loss: 0.0316 - val_mae: 0.0308 - val_mape: 5.3475\n",
      "Epoch 31/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0256 - mae: 0.0247 - mape: 9221.6504\n",
      "Epoch 00031: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0256 - mae: 0.0247 - mape: 9219.9551 - val_loss: 0.0383 - val_mae: 0.0375 - val_mape: 6.0447\n",
      "Epoch 32/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9185.3818\n",
      "Epoch 00032: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 422us/sample - loss: 0.0253 - mae: 0.0244 - mape: 9156.6689 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.6061\n",
      "Epoch 33/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0244 - mape: 9151.5020\n",
      "Epoch 00033: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 418us/sample - loss: 0.0252 - mae: 0.0243 - mape: 9104.9512 - val_loss: 0.0435 - val_mae: 0.0426 - val_mape: 6.6821\n",
      "Epoch 34/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0248 - mae: 0.0240 - mape: 9209.8779\n",
      "Epoch 00034: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0248 - mae: 0.0240 - mape: 9163.0439 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 5.9090\n",
      "Epoch 35/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0251 - mae: 0.0243 - mape: 9190.4863\n",
      "Epoch 00035: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0251 - mae: 0.0243 - mape: 9170.7715 - val_loss: 0.0326 - val_mae: 0.0317 - val_mape: 5.4763\n",
      "Epoch 36/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0241 - mape: 9201.8359\n",
      "Epoch 00036: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0250 - mae: 0.0241 - mape: 9182.0967 - val_loss: 0.0393 - val_mae: 0.0384 - val_mape: 6.1283\n",
      "Epoch 37/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0247 - mae: 0.0238 - mape: 9064.8945\n",
      "Epoch 00037: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0247 - mae: 0.0238 - mape: 9063.2285 - val_loss: 0.0322 - val_mae: 0.0314 - val_mape: 5.4603\n",
      "Epoch 38/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0245 - mae: 0.0236 - mape: 9153.4014\n",
      "Epoch 00038: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0245 - mae: 0.0236 - mape: 9142.7412 - val_loss: 0.0396 - val_mae: 0.0387 - val_mape: 6.1510\n",
      "Epoch 39/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9212.5186\n",
      "Epoch 00039: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9183.7207 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5752\n",
      "Epoch 40/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0237 - mape: 9298.7363\n",
      "Epoch 00040: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0246 - mae: 0.0237 - mape: 9278.7891 - val_loss: 0.0358 - val_mae: 0.0349 - val_mape: 5.7883\n",
      "Epoch 41/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0243 - mae: 0.0234 - mape: 9290.7383\n",
      "Epoch 00041: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0243 - mae: 0.0234 - mape: 9252.5859 - val_loss: 0.0359 - val_mae: 0.0350 - val_mape: 6.1253\n",
      "Epoch 42/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0244 - mae: 0.0235 - mape: 9100.7422\n",
      "Epoch 00042: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0244 - mae: 0.0235 - mape: 9099.0762 - val_loss: 0.0365 - val_mae: 0.0356 - val_mape: 5.8232\n",
      "Epoch 43/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.0231 - mape: 9206.7373\n",
      "Epoch 00043: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0240 - mae: 0.0231 - mape: 9186.9854 - val_loss: 0.0428 - val_mae: 0.0419 - val_mape: 6.5949\n",
      "Epoch 44/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.0229 - mape: 9111.2773\n",
      "Epoch 00044: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0238 - mae: 0.0229 - mape: 9109.6025 - val_loss: 0.0335 - val_mae: 0.0326 - val_mape: 5.5624\n",
      "Epoch 45/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0229 - mape: 9251.8613\n",
      "Epoch 00045: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0229 - mape: 9250.1602 - val_loss: 0.0383 - val_mae: 0.0374 - val_mape: 6.0213\n",
      "Epoch 46/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0228 - mape: 9249.3652\n",
      "Epoch 00046: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9229.5244 - val_loss: 0.0470 - val_mae: 0.0461 - val_mape: 7.1372\n",
      "Epoch 47/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0237 - mae: 0.0228 - mape: 9240.2764\n",
      "Epoch 00047: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0237 - mae: 0.0228 - mape: 9220.4561 - val_loss: 0.0355 - val_mae: 0.0346 - val_mape: 5.8071\n",
      "Epoch 48/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0236 - mae: 0.0227 - mape: 9228.7842\n",
      "Epoch 00048: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0236 - mae: 0.0227 - mape: 9227.0869 - val_loss: 0.0335 - val_mae: 0.0327 - val_mape: 5.7051\n",
      "Epoch 49/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0234 - mae: 0.0225 - mape: 9308.8867\n",
      "Epoch 00049: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0234 - mae: 0.0225 - mape: 9279.7959 - val_loss: 0.0391 - val_mae: 0.0382 - val_mape: 6.2820\n",
      "Epoch 50/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0223 - mape: 8910.0205\n",
      "Epoch 00050: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0232 - mae: 0.0223 - mape: 9253.5254 - val_loss: 0.0347 - val_mae: 0.0338 - val_mape: 5.7545\n",
      "Epoch 51/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0224 - mape: 9279.0811\n",
      "Epoch 00051: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 430us/sample - loss: 0.0233 - mae: 0.0224 - mape: 9277.3750 - val_loss: 0.0390 - val_mae: 0.0381 - val_mape: 6.1803\n",
      "Epoch 52/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0222 - mape: 9221.8008\n",
      "Epoch 00052: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0231 - mae: 0.0222 - mape: 9202.0244 - val_loss: 0.0355 - val_mae: 0.0347 - val_mape: 5.8992\n",
      "Epoch 53/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0218 - mape: 9313.6279\n",
      "Epoch 00053: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0227 - mae: 0.0219 - mape: 9284.5176 - val_loss: 0.0346 - val_mae: 0.0337 - val_mape: 5.8458\n",
      "Epoch 54/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0221 - mape: 9235.9512\n",
      "Epoch 00054: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0230 - mae: 0.0221 - mape: 9207.0791 - val_loss: 0.0399 - val_mae: 0.0391 - val_mape: 6.3575\n",
      "Epoch 55/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0228 - mae: 0.0219 - mape: 9211.0430\n",
      "Epoch 00055: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0228 - mae: 0.0219 - mape: 9191.2910 - val_loss: 0.0397 - val_mae: 0.0388 - val_mape: 6.2600\n",
      "Epoch 56/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0217 - mape: 9251.5020\n",
      "Epoch 00056: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0226 - mae: 0.0217 - mape: 9240.7275 - val_loss: 0.0405 - val_mae: 0.0397 - val_mape: 6.3565\n",
      "Epoch 57/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 9213.3115\n",
      "Epoch 00057: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9166.4385 - val_loss: 0.0343 - val_mae: 0.0334 - val_mape: 5.7472\n",
      "Epoch 58/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0216 - mape: 9254.9170\n",
      "Epoch 00058: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9253.2207 - val_loss: 0.0376 - val_mae: 0.0367 - val_mape: 6.0893\n",
      "Epoch 59/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0220 - mape: 9354.7930\n",
      "Epoch 00059: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 417us/sample - loss: 0.0230 - mae: 0.0220 - mape: 9325.5488 - val_loss: 0.0341 - val_mae: 0.0332 - val_mape: 5.7046\n",
      "Epoch 60/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0214 - mape: 9235.1680\n",
      "Epoch 00060: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0222 - mae: 0.0214 - mape: 9188.1836 - val_loss: 0.0366 - val_mae: 0.0358 - val_mape: 5.9825\n",
      "Epoch 61/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0215 - mape: 9196.2959\n",
      "Epoch 00061: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0223 - mae: 0.0215 - mape: 9167.5479 - val_loss: 0.0399 - val_mae: 0.0390 - val_mape: 6.2796\n",
      "Epoch 62/80\n",
      "32544/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0213 - mape: 9107.4619\n",
      "Epoch 00062: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0222 - mae: 0.0213 - mape: 9087.9248 - val_loss: 0.0385 - val_mae: 0.0376 - val_mape: 6.1628\n",
      "Epoch 63/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0213 - mape: 9210.2637\n",
      "Epoch 00063: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0221 - mae: 0.0213 - mape: 9172.4463 - val_loss: 0.0337 - val_mae: 0.0328 - val_mape: 5.5972\n",
      "Epoch 64/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9124.6074\n",
      "Epoch 00064: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9122.9297 - val_loss: 0.0372 - val_mae: 0.0364 - val_mape: 6.0281\n",
      "Epoch 65/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0212 - mape: 9226.6611\n",
      "Epoch 00065: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0220 - mae: 0.0212 - mape: 9188.7695 - val_loss: 0.0377 - val_mae: 0.0369 - val_mape: 6.2115\n",
      "Epoch 66/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0211 - mape: 9052.7314\n",
      "Epoch 00066: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0219 - mae: 0.0211 - mape: 9051.0664 - val_loss: 0.0405 - val_mae: 0.0396 - val_mape: 6.4286\n",
      "Epoch 67/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0209 - mape: 9083.4209\n",
      "Epoch 00067: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0218 - mae: 0.0209 - mape: 9072.8418 - val_loss: 0.0365 - val_mae: 0.0357 - val_mape: 6.0204\n",
      "Epoch 68/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9076.5205\n",
      "Epoch 00068: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9039.2510 - val_loss: 0.0342 - val_mae: 0.0334 - val_mape: 5.6559\n",
      "Epoch 69/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0208 - mape: 9033.5391\n",
      "Epoch 00069: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0217 - mae: 0.0208 - mape: 9005.3018 - val_loss: 0.0396 - val_mae: 0.0388 - val_mape: 6.4502\n",
      "Epoch 70/80\n",
      "32480/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9120.9199\n",
      "Epoch 00070: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9083.4629 - val_loss: 0.0361 - val_mae: 0.0353 - val_mape: 5.8008\n",
      "Epoch 71/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0208 - mape: 9097.3145\n",
      "Epoch 00071: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0216 - mae: 0.0208 - mape: 9095.6416 - val_loss: 0.0361 - val_mae: 0.0352 - val_mape: 5.9642\n",
      "Epoch 72/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0206 - mape: 9088.2021\n",
      "Epoch 00072: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0206 - mape: 9086.5312 - val_loss: 0.0380 - val_mae: 0.0372 - val_mape: 6.1542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0207 - mape: 9105.6396\n",
      "Epoch 00073: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0215 - mae: 0.0207 - mape: 9059.3135 - val_loss: 0.0434 - val_mae: 0.0426 - val_mape: 6.7060\n",
      "Epoch 74/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0205 - mape: 9066.7041\n",
      "Epoch 00074: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0214 - mae: 0.0205 - mape: 9065.0361 - val_loss: 0.0346 - val_mae: 0.0338 - val_mape: 5.7416\n",
      "Epoch 75/80\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0205 - mape: 8978.4404\n",
      "Epoch 00075: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0213 - mae: 0.0205 - mape: 8967.9941 - val_loss: 0.0348 - val_mae: 0.0340 - val_mape: 5.9216\n",
      "Epoch 76/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9057.9648\n",
      "Epoch 00076: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 416us/sample - loss: 0.0210 - mae: 0.0202 - mape: 9056.2988 - val_loss: 0.0398 - val_mae: 0.0390 - val_mape: 6.3300\n",
      "Epoch 77/80\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0203 - mape: 8981.2236\n",
      "Epoch 00077: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0212 - mae: 0.0203 - mape: 8979.5723 - val_loss: 0.0451 - val_mae: 0.0444 - val_mape: 7.0400\n",
      "Epoch 78/80\n",
      "32448/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0202 - mape: 9038.3955\n",
      "Epoch 00078: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0210 - mae: 0.0202 - mape: 8992.4121 - val_loss: 0.0360 - val_mae: 0.0353 - val_mape: 5.9399\n",
      "Epoch 79/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0201 - mape: 8948.2764\n",
      "Epoch 00079: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 415us/sample - loss: 0.0209 - mae: 0.0201 - mape: 8920.3037 - val_loss: 0.0350 - val_mae: 0.0342 - val_mape: 5.7982\n",
      "Epoch 80/80\n",
      "32512/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0200 - mape: 8990.1953\n",
      "Epoch 00080: val_loss did not improve from 0.03017\n",
      "32614/32614 [==============================] - 14s 414us/sample - loss: 0.0208 - mae: 0.0200 - mape: 8962.0898 - val_loss: 0.0358 - val_mae: 0.0350 - val_mape: 5.8451\n"
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "cW3NWXtXLDA-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 420.56\n",
      "Median Absolute Error (MedAE): 333.06\n",
      "Mean Squared Error (MSE): 313586.16\n",
      "Root Mean Squared Error (RMSE): 559.99\n",
      "Mean Absolute Percentage Error (MAPE): 4.19 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.25 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E1-cp-0029-loss0.03.h5'\n",
    "start_epoch= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E1-cp-0029-loss0.03.h5...\n",
      "[INFO] old learning rate: 0.0010000000474974513\n",
      "[INFO] new learning rate: 9.999999747378752e-05\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0228 - mape: 9138.6191- ETA: 3s - loss: 0.023\n",
      "Epoch 00001: val_loss improved from inf to 0.03148, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 56s 2ms/sample - loss: 0.0235 - mae: 0.0228 - mape: 9136.9395 - val_loss: 0.0315 - val_mae: 0.0307 - val_mape: 5.2145\n",
      "Epoch 2/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0232 - mae: 0.0225 - mape: 9164.0752\n",
      "Epoch 00002: val_loss improved from 0.03148 to 0.03063, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0232 - mae: 0.0225 - mape: 9162.3906 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1229\n",
      "Epoch 3/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0231 - mae: 0.0224 - mape: 9196.9521- ETA: 3s - loss: 0.0\n",
      "Epoch 00003: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0231 - mae: 0.0224 - mape: 9195.2607 - val_loss: 0.0314 - val_mae: 0.0307 - val_mape: 5.2105\n",
      "Epoch 4/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0230 - mae: 0.0223 - mape: 9131.6660\n",
      "Epoch 00004: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0230 - mae: 0.0223 - mape: 9121.0312 - val_loss: 0.0310 - val_mae: 0.0304 - val_mape: 5.1898\n",
      "Epoch 5/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0229 - mae: 0.0223 - mape: 9159.3213\n",
      "Epoch 00005: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0229 - mae: 0.0223 - mape: 9148.6533 - val_loss: 0.0321 - val_mae: 0.0314 - val_mape: 5.2817\n",
      "Epoch 6/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9180.6631\n",
      "Epoch 00006: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0228 - mae: 0.0221 - mape: 9178.9824 - val_loss: 0.0309 - val_mae: 0.0303 - val_mape: 5.1679\n",
      "Epoch 7/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0227 - mae: 0.0221 - mape: 9175.2949\n",
      "Epoch 00007: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0227 - mae: 0.0221 - mape: 9173.6084 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.2847\n",
      "Epoch 8/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0220 - mape: 9205.9355\n",
      "Epoch 00008: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0226 - mae: 0.0220 - mape: 9195.2217 - val_loss: 0.0307 - val_mae: 0.0301 - val_mape: 5.2188\n",
      "Epoch 9/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0219 - mape: 9181.9873\n",
      "Epoch 00009: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0225 - mae: 0.0219 - mape: 9180.2988 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.2595\n",
      "Epoch 10/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0218 - mape: 9186.6875\n",
      "Epoch 00010: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0224 - mae: 0.0218 - mape: 9184.9990 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2146\n",
      "Epoch 11/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0217 - mape: 9203.6816\n",
      "Epoch 00011: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0223 - mae: 0.0217 - mape: 9192.9629 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.2609\n",
      "Epoch 12/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9241.5742\n",
      "Epoch 00012: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0222 - mae: 0.0216 - mape: 9239.8750 - val_loss: 0.0311 - val_mae: 0.0305 - val_mape: 5.2300\n",
      "Epoch 13/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0216 - mape: 9247.9932\n",
      "Epoch 00013: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0216 - mape: 9237.2227 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4177\n",
      "Epoch 14/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0215 - mape: 9297.9727\n",
      "Epoch 00014: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0221 - mae: 0.0215 - mape: 9296.2627 - val_loss: 0.0307 - val_mae: 0.0302 - val_mape: 5.2173\n",
      "Epoch 15/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0220 - mae: 0.0214 - mape: 9263.5205\n",
      "Epoch 00015: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0220 - mae: 0.0214 - mape: 9261.8164 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2651\n",
      "Epoch 16/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0214 - mape: 9198.6846\n",
      "Epoch 00016: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0219 - mae: 0.0214 - mape: 9196.9941 - val_loss: 0.0318 - val_mae: 0.0312 - val_mape: 5.3174\n",
      "Epoch 17/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0213 - mape: 9260.9404\n",
      "Epoch 00017: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0213 - mape: 9250.1543 - val_loss: 0.0309 - val_mae: 0.0304 - val_mape: 5.2860\n",
      "Epoch 18/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0218 - mae: 0.0212 - mape: 9216.3271\n",
      "Epoch 00018: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0218 - mae: 0.0212 - mape: 9214.6318 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3829\n",
      "Epoch 19/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0211 - mape: 9211.0586\n",
      "Epoch 00019: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0217 - mae: 0.0211 - mape: 9200.3320 - val_loss: 0.0341 - val_mae: 0.0336 - val_mape: 5.5767\n",
      "Epoch 20/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0215 - mae: 0.0209 - mape: 9201.0938\n",
      "Epoch 00020: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0215 - mae: 0.0209 - mape: 9190.3779 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3684\n",
      "Epoch 21/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0208 - mape: 9219.9883\n",
      "Epoch 00021: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 53s 2ms/sample - loss: 0.0214 - mae: 0.0208 - mape: 9218.2930 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3618\n",
      "Epoch 22/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0207 - mape: 9187.7080\n",
      "Epoch 00022: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0207 - mape: 9186.0186 - val_loss: 0.0316 - val_mae: 0.0310 - val_mape: 5.3197\n",
      "Epoch 23/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0212 - mae: 0.0206 - mape: 9196.0430\n",
      "Epoch 00023: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0212 - mae: 0.0206 - mape: 9194.3525 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2367\n",
      "Epoch 24/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.0206 - mape: 9184.2910\n",
      "Epoch 00024: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0211 - mae: 0.0206 - mape: 9173.5947 - val_loss: 0.0315 - val_mae: 0.0309 - val_mape: 5.2920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0210 - mae: 0.0205 - mape: 9166.1699\n",
      "Epoch 00025: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0210 - mae: 0.0205 - mape: 9155.4951 - val_loss: 0.0316 - val_mae: 0.0311 - val_mape: 5.3492\n",
      "Epoch 26/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9205.6270\n",
      "Epoch 00026: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9203.9385 - val_loss: 0.0308 - val_mae: 0.0303 - val_mape: 5.2379\n",
      "Epoch 27/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0204 - mape: 9117.5986\n",
      "Epoch 00027: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0209 - mae: 0.0204 - mape: 9115.9219 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4364\n",
      "Epoch 28/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9204.3340\n",
      "Epoch 00028: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9193.6143 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2468\n",
      "Epoch 29/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0208 - mae: 0.0203 - mape: 9128.1553\n",
      "Epoch 00029: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0208 - mae: 0.0203 - mape: 9126.4766 - val_loss: 0.0324 - val_mae: 0.0319 - val_mape: 5.3819\n",
      "Epoch 30/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0207 - mae: 0.0202 - mape: 9126.3975\n",
      "Epoch 00030: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0207 - mae: 0.0202 - mape: 9124.7197 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.3127\n",
      "Epoch 31/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9127.3574\n",
      "Epoch 00031: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9125.6787 - val_loss: 0.0314 - val_mae: 0.0308 - val_mape: 5.3096\n",
      "Epoch 32/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0201 - mape: 9139.1289\n",
      "Epoch 00032: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0206 - mae: 0.0201 - mape: 9137.4482 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3143\n",
      "Epoch 33/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0205 - mae: 0.0200 - mape: 9129.0029\n",
      "Epoch 00033: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0205 - mae: 0.0200 - mape: 9127.3242 - val_loss: 0.0313 - val_mae: 0.0308 - val_mape: 5.3023\n",
      "Epoch 34/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9085.1152\n",
      "Epoch 00034: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9083.4443 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3421\n",
      "Epoch 35/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0204 - mae: 0.0199 - mape: 9161.0391\n",
      "Epoch 00035: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0204 - mae: 0.0199 - mape: 9159.3545 - val_loss: 0.0310 - val_mae: 0.0305 - val_mape: 5.2771\n",
      "Epoch 36/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0203 - mae: 0.0198 - mape: 9147.9355\n",
      "Epoch 00036: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0203 - mae: 0.0198 - mape: 9137.2803 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3513\n",
      "Epoch 37/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9170.7051- ETA: 4s - loss: 0. - ETA: 2s - loss: 0.0203\n",
      "Epoch 00037: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9169.0186 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2897\n",
      "Epoch 38/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0202 - mae: 0.0197 - mape: 9186.7959\n",
      "Epoch 00038: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0202 - mae: 0.0197 - mape: 9176.0967 - val_loss: 0.0312 - val_mae: 0.0307 - val_mape: 5.2613\n",
      "Epoch 39/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0195 - mape: 9121.9736\n",
      "Epoch 00039: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0200 - mae: 0.0195 - mape: 9120.2959 - val_loss: 0.0348 - val_mae: 0.0343 - val_mape: 5.6403\n",
      "Epoch 40/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0194 - mape: 9143.8574\n",
      "Epoch 00040: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0199 - mae: 0.0194 - mape: 9142.1768 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3696\n",
      "Epoch 41/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0192 - mape: 9166.4883\n",
      "Epoch 00041: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0197 - mae: 0.0192 - mape: 9155.8125 - val_loss: 0.0320 - val_mae: 0.0315 - val_mape: 5.3686\n",
      "Epoch 42/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9157.3496\n",
      "Epoch 00042: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9146.6846 - val_loss: 0.0314 - val_mae: 0.0309 - val_mape: 5.3299\n",
      "Epoch 43/200\n",
      "32576/32614 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0191 - mape: 9169.7188\n",
      "Epoch 00043: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0196 - mae: 0.0191 - mape: 9159.0391 - val_loss: 0.0311 - val_mae: 0.0306 - val_mape: 5.2889\n",
      "Epoch 44/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0190 - mape: 9109.2627\n",
      "Epoch 00044: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0195 - mae: 0.0190 - mape: 9107.5879 - val_loss: 0.0321 - val_mae: 0.0316 - val_mape: 5.3664\n",
      "Epoch 45/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0189 - mape: 9101.7988\n",
      "Epoch 00045: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0195 - mae: 0.0189 - mape: 9100.1250 - val_loss: 0.0328 - val_mae: 0.0323 - val_mape: 5.4306\n",
      "Epoch 46/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0194 - mae: 0.0189 - mape: 9152.3506\n",
      "Epoch 00046: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0194 - mae: 0.0189 - mape: 9150.6680 - val_loss: 0.0317 - val_mae: 0.0311 - val_mape: 5.3450\n",
      "Epoch 47/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0193 - mae: 0.0187 - mape: 9092.9639\n",
      "Epoch 00047: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 54s 2ms/sample - loss: 0.0193 - mae: 0.0187 - mape: 9091.2920 - val_loss: 0.0318 - val_mae: 0.0313 - val_mape: 5.3590\n",
      "Epoch 48/200\n",
      "32608/32614 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.0186 - mape: 9166.8369\n",
      "Epoch 00048: val_loss did not improve from 0.03063\n",
      "32614/32614 [==============================] - 55s 2ms/sample - loss: 0.0191 - mae: 0.0186 - mape: 9165.1514 - val_loss: 0.0330 - val_mae: 0.0325 - val_mape: 5.4518\n",
      "Epoch 49/200\n",
      "11776/32614 [=========>....................] - ETA: 32s - loss: 0.0191 - mae: 0.0186 - mape: 7337.6323"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-048f600f12b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 32\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 412.44\n",
      "Median Absolute Error (MedAE): 328.08\n",
      "Mean Squared Error (MSE): 301264.17\n",
      "Root Mean Squared Error (RMSE): 548.88\n",
      "Mean Absolute Percentage Error (MAPE): 4.12 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.19 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-{epoch:04d}-loss{val_loss:.2f}.h5'\n",
    "model='D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E2-0002-loss0.03.h5'\n",
    "start_epoch= 30+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the callback to save only the *best* model to disk\n",
    "# based on the validation loss\n",
    "EpochCheckpoint1 = ModelCheckpoint(checkpoints,\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "TrainingMonitor1=TrainingMonitor(FIG_PATH, jsonPath=JSON_PATH, startAt=start_epoch)\n",
    "\n",
    "# construct the set of callbacks\n",
    "callbacks = [EpochCheckpoint1,TrainingMonitor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E2-0002-loss0.03.h5...\n",
      "[INFO] old learning rate: 9.999999747378752e-05\n",
      "[INFO] new learning rate: 9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "# if there is no specific model checkpoint supplied, then initialize\n",
    "# the network and compile the model\n",
    "if model is None:\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model = PC.build(time_steps=72, num_features=17, reg=0.0005)\n",
    "    opt = Adam(1e-2)\n",
    "    model.compile(loss= 'mae', optimizer=opt, metrics=[\"mae\", \"mape\"])\n",
    "# otherwise, load the checkpoint from disk\n",
    "else:\n",
    "    print(\"[INFO] loading {}...\".format(model))\n",
    "    model = load_model(model)\n",
    "\n",
    "    # update the learning rate\n",
    "    print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
    "    K.set_value(model.optimizer.lr, 1e-5)\n",
    "    print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32614 samples, validate on 9250 samples\n",
      "Epoch 1/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0218 - mape: 9021.4141\n",
      "Epoch 00001: val_loss improved from inf to 0.03075, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0001-loss0.03.h5\n",
      "32614/32614 [==============================] - 7s 222us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9155.7246 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1276\n",
      "Epoch 2/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9392.5967\n",
      "Epoch 00002: val_loss improved from 0.03075 to 0.03067, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0002-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0218 - mape: 9142.1660 - val_loss: 0.0307 - val_mae: 0.0300 - val_mape: 5.1187\n",
      "Epoch 3/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0217 - mape: 8595.9766\n",
      "Epoch 00003: val_loss improved from 0.03067 to 0.03054, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0003-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0225 - mae: 0.0217 - mape: 9143.0762 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1073\n",
      "Epoch 4/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9385.6611\n",
      "Epoch 00004: val_loss improved from 0.03054 to 0.03050, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0004-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9135.4072 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1036\n",
      "Epoch 5/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9377.5986\n",
      "Epoch 00005: val_loss improved from 0.03050 to 0.03046, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0005-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9127.5752 - val_loss: 0.0305 - val_mae: 0.0297 - val_mape: 5.0995\n",
      "Epoch 6/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8606.6309\n",
      "Epoch 00006: val_loss improved from 0.03046 to 0.03042, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0006-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9137.0928 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0965\n",
      "Epoch 7/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9375.1191\n",
      "Epoch 00007: val_loss improved from 0.03042 to 0.03041, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0007-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9125.1416 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0951\n",
      "Epoch 8/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9374.5898\n",
      "Epoch 00008: val_loss improved from 0.03041 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0008-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9124.6318 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0924\n",
      "Epoch 9/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8210.0459\n",
      "Epoch 00009: val_loss improved from 0.03038 to 0.03038, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0009-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9126.5254 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0921\n",
      "Epoch 10/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8572.6328  ETA: 1s - loss: 0.0225 - mae: 0.0218 \n",
      "Epoch 00010: val_loss improved from 0.03038 to 0.03035, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0010-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9118.1846 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0894\n",
      "Epoch 11/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8207.2930\n",
      "Epoch 00011: val_loss did not improve from 0.03035\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9111.9365 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0895\n",
      "Epoch 12/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9357.2139\n",
      "Epoch 00012: val_loss improved from 0.03035 to 0.03034, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0012-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 109us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.7373 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0893\n",
      "Epoch 13/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8967.2090\n",
      "Epoch 00013: val_loss improved from 0.03034 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0013-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0859 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0887\n",
      "Epoch 14/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8584.7832\n",
      "Epoch 00014: val_loss improved from 0.03033 to 0.03033, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0014-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9115.9170 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 15/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9359.8037\n",
      "Epoch 00015: val_loss improved from 0.03033 to 0.03030, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0015-loss0.03.h5\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.2441 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0861\n",
      "Epoch 16/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8970.0029\n",
      "Epoch 00016: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9116.9443 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0868\n",
      "Epoch 17/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8182.7358\n",
      "Epoch 00017: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9107.9473 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0872\n",
      "Epoch 18/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9360.5244\n",
      "Epoch 00018: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.9551 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 19/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8964.7793\n",
      "Epoch 00019: val_loss did not improve from 0.03030\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9110.0879 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 20/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8972.5371\n",
      "Epoch 00020: val_loss improved from 0.03030 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0020-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9106.9902 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0856\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8572.1016\n",
      "Epoch 00021: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0217 - mape: 9105.2520 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0876\n",
      "Epoch 22/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9354.3223\n",
      "Epoch 00022: val_loss improved from 0.03028 to 0.03028, saving model to D:\\Checkpoints\\1_ieee_industrial info\\multistep\\hybrid\\4\\E3-0022-loss0.03.h5\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.9277 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0858\n",
      "Epoch 23/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9355.4004- ETA: 1s - loss: 0.0223 - mae: 0.0216 - mape: 9009 - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 869\n",
      "Epoch 00023: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9105.9697 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0881\n",
      "Epoch 24/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9358.2070\n",
      "Epoch 00024: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9108.6885 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0860\n",
      "Epoch 25/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 8967.9619\n",
      "Epoch 00025: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9104.4453 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 26/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 8956.2822\n",
      "Epoch 00026: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.1289 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0879\n",
      "Epoch 27/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0216 - mape: 9353.0088\n",
      "Epoch 00027: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0224 - mae: 0.0216 - mape: 9103.6240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0873\n",
      "Epoch 28/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.5430\n",
      "Epoch 00028: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1504 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0871\n",
      "Epoch 29/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0224 - mae: 0.0217 - mape: 9354.4570\n",
      "Epoch 00029: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.0547 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0886\n",
      "Epoch 30/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.4277\n",
      "Epoch 00030: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.0498 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0884\n",
      "Epoch 31/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2637\n",
      "Epoch 00031: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 111us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0391 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0890\n",
      "Epoch 32/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.9912\n",
      "Epoch 00032: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.1416 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0896\n",
      "Epoch 33/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8960.5801\n",
      "Epoch 00033: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7783 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0913\n",
      "Epoch 34/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9346.8809\n",
      "Epoch 00034: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9097.6689 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0885\n",
      "Epoch 35/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8965.5186\n",
      "Epoch 00035: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 36/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8569.2568\n",
      "Epoch 00036: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 112us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.1602 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0915\n",
      "Epoch 37/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.2012  ETA: 2s - loss: 0.02\n",
      "Epoch 00037: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 106us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.1436 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0920\n",
      "Epoch 38/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8574.7637\n",
      "Epoch 00038: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.4717 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0902\n",
      "Epoch 39/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.5205\n",
      "Epoch 00039: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.2197 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 40/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9350.8828\n",
      "Epoch 00040: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9101.5615 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0900\n",
      "Epoch 41/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.7637\n",
      "Epoch 00041: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.3633 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0908\n",
      "Epoch 42/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9359.6875\n",
      "Epoch 00042: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.1240 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0914\n",
      "Epoch 43/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.9414\n",
      "Epoch 00043: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9099.0850 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 44/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3818\n",
      "Epoch 00044: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9072 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0957\n",
      "Epoch 45/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9347.2998\n",
      "Epoch 00045: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9098.0811 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0930\n",
      "Epoch 46/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8428- ETA: 2s - loss: 0.0223 - mae: 0\n",
      "Epoch 00046: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 107us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.9570 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0959\n",
      "Epoch 47/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9353.5625\n",
      "Epoch 00047: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.1621 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0927\n",
      "Epoch 48/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8967.8184\n",
      "Epoch 00048: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.2930 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0955\n",
      "Epoch 49/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8958.4678- ETA: 2s - loss: 0.0224 - mae: 0.0217 - mape: 4847. - ETA: 1s - loss: 0.0222 - mae: 0.0215 \n",
      "Epoch 00049: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.8057 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0937\n",
      "Epoch 50/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8977.0430\n",
      "Epoch 00050: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.9639 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0950\n",
      "Epoch 51/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.6865\n",
      "Epoch 00051: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.2705 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0954\n",
      "Epoch 52/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8964.1064\n",
      "Epoch 00052: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.1123 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0919\n",
      "Epoch 53/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.2607\n",
      "Epoch 00053: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9102.9170 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0989\n",
      "Epoch 54/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8959.4570\n",
      "Epoch 00054: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.2852 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0969\n",
      "Epoch 55/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8970.4199- ETA: 2s - loss: 0.0225 - mae: 0\n",
      "Epoch 00055: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.2910 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0990\n",
      "Epoch 56/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6221\n",
      "Epoch 00056: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1025 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0925\n",
      "Epoch 57/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8961.5264\n",
      "Epoch 00057: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8096 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1009\n",
      "Epoch 58/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8963.5762\n",
      "Epoch 00058: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.3652 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1010\n",
      "Epoch 59/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8584.6045\n",
      "Epoch 00059: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9108.2227 - val_loss: 0.0303 - val_mae: 0.0296 - val_mape: 5.0963\n",
      "Epoch 60/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9354.8242\n",
      "Epoch 00060: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9105.3945 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0996\n",
      "Epoch 61/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9356.3730  ETA: 2s - loss: 0.0\n",
      "Epoch 00061: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.8975 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1003\n",
      "Epoch 62/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.2998\n",
      "Epoch 00062: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9113.6562 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.0987\n",
      "Epoch 63/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8572.1582\n",
      "Epoch 00063: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9106.7090 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1015\n",
      "Epoch 64/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9358.6914\n",
      "Epoch 00064: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9109.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1013\n",
      "Epoch 65/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9352.9131\n",
      "Epoch 00065: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.5430 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1008\n",
      "Epoch 66/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 9361.4287\n",
      "Epoch 00066: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9111.8457 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1032\n",
      "Epoch 67/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8968.2002\n",
      "Epoch 00067: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9110.6504 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1028\n",
      "Epoch 68/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0216 - mape: 8979.6250\n",
      "Epoch 00068: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9103.7305 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 8962.6729- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8456.346\n",
      "Epoch 00069: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9107.0010 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1072\n",
      "Epoch 70/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.2842\n",
      "Epoch 00070: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0223 - mae: 0.0216 - mape: 9104.8740 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1061\n",
      "Epoch 71/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.1514\n",
      "Epoch 00071: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9108.2100 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1067\n",
      "Epoch 72/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9350.1494\n",
      "Epoch 00072: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9100.8525 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1011\n",
      "Epoch 73/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9354.5088\n",
      "Epoch 00073: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0216 - mape: 9105.0918 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1070\n",
      "Epoch 74/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8561.8984\n",
      "Epoch 00074: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1914 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1073\n",
      "Epoch 75/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.5430\n",
      "Epoch 00075: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.3232 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1101\n",
      "Epoch 76/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.6846\n",
      "Epoch 00076: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.0889 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1018\n",
      "Epoch 77/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.6553\n",
      "Epoch 00077: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.3877 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1131\n",
      "Epoch 78/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.4307\n",
      "Epoch 00078: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.1025 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1066\n",
      "Epoch 79/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8972.5918\n",
      "Epoch 00079: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7490 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 80/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9351.0195\n",
      "Epoch 00080: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.7041 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1059\n",
      "Epoch 81/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.1738\n",
      "Epoch 00081: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 105us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9102.8281 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1086\n",
      "Epoch 82/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9359.2744\n",
      "Epoch 00082: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9109.7461 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1125\n",
      "Epoch 83/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9355.9951\n",
      "Epoch 00083: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 100us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9106.5498 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1091\n",
      "Epoch 84/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0216 - mape: 9361.0264\n",
      "Epoch 00084: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9111.4297 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 85/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8973.9590\n",
      "Epoch 00085: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.7012 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1117\n",
      "Epoch 86/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8980.7080\n",
      "Epoch 00086: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.9180 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1181\n",
      "Epoch 87/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9362.3105\n",
      "Epoch 00087: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9112.6982 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1146\n",
      "Epoch 88/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.5547\n",
      "Epoch 00088: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 104us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1924 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n",
      "Epoch 89/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.5771\n",
      "Epoch 00089: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.1777 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1158\n",
      "Epoch 90/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.7559\n",
      "Epoch 00090: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.4727 - val_loss: 0.0304 - val_mae: 0.0298 - val_mape: 5.1110\n",
      "Epoch 91/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9344.5049\n",
      "Epoch 00091: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9095.3486 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1201\n",
      "Epoch 92/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8966.3213\n",
      "Epoch 00092: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.6084 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1109\n",
      "Epoch 93/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.4619\n",
      "Epoch 00093: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.1074 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9357.2207\n",
      "Epoch 00094: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.7432 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1149\n",
      "Epoch 95/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9349.8730\n",
      "Epoch 00095: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9100.5840 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 96/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9366.0117\n",
      "Epoch 00096: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9116.2979 - val_loss: 0.0304 - val_mae: 0.0297 - val_mape: 5.1090\n",
      "Epoch 97/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 7807.1338- ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape:\n",
      "Epoch 00097: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9097.1768 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1134\n",
      "Epoch 98/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8969.1562\n",
      "Epoch 00098: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9108.1855 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1151\n",
      "Epoch 99/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9350.9482  ETA\n",
      "Epoch 00099: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9101.6289 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1170\n",
      "Epoch 100/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.7568\n",
      "Epoch 00100: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.3867 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1185\n",
      "Epoch 101/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9353.9570\n",
      "Epoch 00101: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9104.5547 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1194\n",
      "Epoch 102/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8963.7695- ETA: 1s - loss: 0.0222 - mae: 0.0215 - ma\n",
      "Epoch 00102: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.2959 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1222\n",
      "Epoch 103/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9360.5537\n",
      "Epoch 00103: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 101us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9110.9805 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1187\n",
      "Epoch 104/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8967.6729\n",
      "Epoch 00104: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9107.1299 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1163\n",
      "Epoch 105/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.5879\n",
      "Epoch 00105: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 110us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.3262 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1221\n",
      "Epoch 106/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9363.9297\n",
      "Epoch 00106: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 103us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9114.2559 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1136\n",
      "Epoch 107/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 8575.9648\n",
      "Epoch 00107: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 3s 102us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9099.5449 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1218\n",
      "Epoch 108/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9348.1768\n",
      "Epoch 00108: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 108us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9098.9189 - val_loss: 0.0306 - val_mae: 0.0299 - val_mape: 5.1236\n",
      "Epoch 109/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0222 - mae: 0.0215 - mape: 9352.6055\n",
      "Epoch 00109: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0222 - mae: 0.0215 - mape: 9103.2510 - val_loss: 0.0305 - val_mae: 0.0299 - val_mape: 5.1214\n",
      "Epoch 110/200\n",
      "31744/32614 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0214 - mape: 9352.8691\n",
      "Epoch 00110: val_loss did not improve from 0.03028\n",
      "32614/32614 [==============================] - 4s 113us/sample - loss: 0.0221 - mae: 0.0215 - mape: 9103.5098 - val_loss: 0.0305 - val_mae: 0.0298 - val_mape: 5.1184\n",
      "Epoch 111/200\n",
      "23552/32614 [====================>.........] - ETA: 0s - loss: 0.0220 - mae: 0.0213 - mape: 9332.0186"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-aa18ff9589c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                         verbose = verbose)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\irshad2\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "verbose = 1 #0\n",
    "batch_size = 1024\n",
    "History = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        batch_size=batch_size,   \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (validation_X,validation_y),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 411.01\n",
      "Median Absolute Error (MedAE): 326.15\n",
      "Mean Squared Error (MSE): 300114.5\n",
      "Root Mean Squared Error (RMSE): 547.83\n",
      "Mean Absolute Percentage Error (MAPE): 4.11 %\n",
      "Median Absolute Percentage Error (MDAPE): 3.17 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (4576, 24)\n",
      "y_pred.shape=  (4576, 24)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model = load_model('D:\\\\Checkpoints\\\\1_ieee_industrial info\\\\multistep\\\\hybrid\\\\4\\\\E3-0022-loss0.03.h5')\n",
    "scaler = pickle.load(open(\"C:\\\\Users\\\\arif\\\\OneDrive\\\\Journal Paper\\\\Code\\\\Irshad\\\\Dataset\\\\scaler.pkl\", 'rb'))\n",
    "y_pred_scaled   = model.predict(test_X) \n",
    "y_pred          = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler.inverse_transform(test_y)\n",
    "# Mean Absolute Error (MAE)\n",
    "MAE = np.mean(abs(y_pred - y_test_unscaled))\n",
    "print('Mean Absolute Error (MAE): ' + str(np.round(MAE, 2)))\n",
    "\n",
    "# Median Absolute Error (MedAE)\n",
    "MEDAE = np.median(abs(y_pred - y_test_unscaled))\n",
    "print('Median Absolute Error (MedAE): ' + str(np.round(MEDAE, 2)))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "MSE = np.square(np.subtract(y_pred, y_test_unscaled)).mean()\n",
    "print('Mean Squared Error (MSE): ' + str(np.round(MSE, 2)))\n",
    "\n",
    "# Root Mean Squarred Error (RMSE) \n",
    "RMSE = np.sqrt(np.mean(np.square(y_pred - y_test_unscaled)))\n",
    "print('Root Mean Squared Error (RMSE): ' + str(np.round(RMSE, 2)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print('Median Absolute Percentage Error (MDAPE): ' + str(np.round(MDAPE, 2)) + ' %')\n",
    "\n",
    "print('\\n\\ny_test_unscaled.shape= ',y_test_unscaled.shape)\n",
    "print('y_pred.shape= ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
